\chapter{Conclusion}
\label{sec:conclusion}
In this thesis we have presented a new key-value store, TropoDB, a key-value store built entirely on top of ZNS SSDs without file systems or any other layer in between the key-value store and storage. The goal of TropoDB is to implement a collaborative GC in which the GC patterns of storage are directly mapped to the GC patterns of the key-value store with minimal interference. The expected result is that by improving the collaboration of the GCs, their impact can be decreased, with results such as a more stable tail latency, lower write amplification and lower amount of resets needed. Our results have not shown that the approach taken by TropoDB can reduce these effects. However, this does not mean that collaborative GC can not have positive positive effects. It merely shows that the approach taken by TropoDB is not effective enough. The designs shown in \autoref{sec:design} and the implementation described in \autoref{sec:implementation}, can be used to look for further optimisations. Additionally, as it is the first of its kind, many of the ideas proposed, both positive and negative, can be used to get a better understanding of what works and does not work. This can in turn be used to come with better designs built on top of ZNS or how existing key-value stores can be modified to get ZNS benefits. It is thus a proper proof of concept and the first step to better collaboration between flash storage and key-value stores. Lastly, we have seen impressive differences in latency stability between ZenFS and F2FS for larger workloads. ZenFS is more optimised for ZNS and has less of a semantic gap between the key-value store and ZNS, provided that optimisations for ZNS can have significant advantages if done correctly.

\section{Research Questions}
\textbf{RQ1: What are the key-value store integration choices available on top of ZNS SSDs?}\\
There exist multiple approaches to integrate a key-value store and ZNS SSDs, as described in \autoref{sec:approach}. We identified the following approaches: creating a file system that supports ZNS in between the key-value store and ZNS, creating a key-value store from the ground up on ZNS and two approaches that integrate ZNS into a state-of-the-art key-value store known as RocksDB. The approaches that integrate into RocksDB are a top-down approach, where the benchmarking tool will be reused and RocksDB ecosystem will be available, and a bottom-up approach, where RocksDB will be modified to support ZNS. There is a trade-off to pick between the different integration choices. Removing layers of abstractions, adds more control over ZNS and tighter integration, but requires (re)implementing more logic and requires a hefty developer investment. In the end, the bottom-up approach was taken for TropoDB. The bottom-up approach gives the key-value store direct control over the storage, and allows reusing existing key-value store logic and tools, such as benchmarks and in-memory components. \\
\textbf{RQ2: What unique optimisation opportunities does a ZNS device offer for LSM-tree based key-value stores?}\\
ZNS devices offer various unique optimisation opportunities.  In \autoref{sec:design}, we describe in detail how TropoDB makes use of all of them. ZNS comes with a novel append operation, that allows increasing concurrency. This operation can be used to great effect for structures that require frequent I/O, such as \textit{write-ahead logs} (WALs) from LSM-trees. 

Additionally, ZNS comes with explicit control over the physical locations of data storage, which can reduce the number of overwrites and resets. LSM-tree inherently already limit the number of overwrites and resets as all data is stored sequentially, but without direct control over storage, data can still be reordered on storage (and require overwrites and resets). With ZNS, this is no longer an issue as it can be guaranteed that no data is overwritten and no additional resets are necessary, beyond what is specified by the LSM-tree. This can be done by implementing all data structures as append-only structures and disallow any overwrite. As we have explicit control over data locations, this also allows for optimisations such as not rewriting SSTables when they do not change, but simply only updating the metadata.

To continue, ZNS comes with excellent concurrency capabilities. The ZNS interace allows multiple operations to occur concurrently at entirely different locations on storage. This can be used to great effect for the task parallelism of LSM-trees. For example, by separately storing data of LSM-tree components such as WALs and SSTables and assigning these components to different threads, it allow them to function concurrently with minimal interference.

Lastly, ZNS gives explicit control over garbage collection. This allows control over not just the garbage collection of an LSM-tree, but of storage as well. This allows coordinating when to erase data on storage and when to issue background operations on the LSM-tree and can reduce latency effects. 
\\
\textbf{RQ3: How to implement and evaluate the design of an LSM-based key-value store on ZNS devices?}\\
The design of TropoDB as described in RQ2 and \autoref{sec:design} has been implemented on ZNS. There exist multiple valid methods to implement TropoDB, but we have only looked at one. An implementation that removes the kernel from the I/O path as well, and works entirely in user-space. When the key-value store is implemented in user-space, it adds more control over the latency effects of storage and can augment the performance targets of TropoDB. It has been implemented, following both \autoref{sec:implementation} and \autoref{sec:approach}. It is created entirely within SPDK and RocksDB with a bottom-up approach. In order to implement the design, it is also required to be able to evaluate the design. As there is key-value store that is entirely built on ZNS yet, this required using an existing benchmarking tool in the form of db\_bench and measuring the on-storage effects of the database. For example, garbage collection effects should be measurable by measuring write amplification, reset count and tail latency of the key-value store(s).\\
\textbf{RQ4: What is the impact of the optimisation process of TropoDB on the key-value store and ZNS device?}\\
The performance impact of TropoDB has been measured on the key-value store and ZNS device in \autoref{sec:experiments}. We have measured garbage collection effects of TropoDB and seen than TropoDB has not been able to reduce garbage collection effects such as write amplification, reset count and tail latency instability. TropoDB is able to come close to the state-of-the-art, RocksDB + ZenFS, and the state-of-the-practice, RocksDB + F2FS, but suffers from tail latency instability at 99.99\% and up, which can take multiple milliseconds to complete for each I/O operation. Write amplification and number of resets is slightly worse, and generally differs between 10 and 20\%.
However, we have shown that various optimisations made in TropoDB have effect. We have seen that \textit{unordered appends} of the TropoDB WAL component (\autoref{sec:wals}) are able to come close to the raw performance of the ZNS append operation in both latency and throughput and do not add significant overhead to read performance. 
\\
\textbf{Summary}\\
In short, we have looked at various integration options for building a key-value store on top of ZNS. We have taken a bottom-approach within the RocksDB ecosystem in which we directly interface the key-value store with ZNS. The resulting design, known as TropoDB, makes use of various ZNS novelties effectively by giving each individual ZNS component distinct logic that matches its characteristics. Each component uses ZNS-specific logic where possible. The resulting design was implemented in user-space with SPDK and comes with evaluations within the db\_bench benchmark and some custom tooling. The implementation has not been able to reduce garbage collection effects, but has shown that its WAL implementation can reach the performance of the ZNS append operation.

In this section we will highlight the contributions made in the thesis. The contribution list the same as listed in \autoref{sec:contrib}. We will also describe some new findings that were originally not the target of the thesis, but that we think are worth sharing.

\begin{enumerate}
    \item Design considerations for building a key-value store on top of ZNS SSDs. Specifically how LSM-trees fit in zones and how garbage collection of LSM-trees can look for ZNS SSDs.
    \item SimpleZNSDevice (SZD), a small API built on top of SPDK, meant to interface with ZNS. This API comes with a simple configuration and some data structures optimised for ZNS. It can be reused for any ZNS project.
    \item Benchmarks for ZNS SSDs that showcase the raw device characteristics of reads, appends, writes and resets when SPDK is used.
    \item TropoDB, a key-value store built directly on top of SZD in user-space with the help of SPDK. We explore design choices regarding the underlying ZNS storage with a focus on garbage collection. 
    \item An investigation of TropoDB, showcasing its tail latency, write amplification and zone reset characteristics compared to the state-of-the-art and the state-of-the-practice.
    \item Publicly available source code, both for SZD, SPDK and the benchmarks. The source code can be found respectively for TropoDB, SZD and the benchmarks at \url{https://github.com/Krien/TropoDB},  \url{https://github.com/Krien/SimpleZNSDevice} and \url{https://github.com/Krien/ZNS_SPDK_Benchmarks}. The sources are described in more detail in \autoref{sec:appendix}.
\end{enumerate}

The original research for TropoDB is not about raw ZNS performance or using multiple appends asynchronously for WALs. Nevertheless, we did investigate both of these concepts regardless. The results of those investigations are worthwhile sharing and do show potential. We have shown the raw append, write, read and reset performance that the used ZNS device can give with SPDK in the best-case scenario and that the WAL design can match this performance. We have seen that using concurrent appends to the WAL with multiple outstanding asynchronous appends for one zone matches the raw asynchronous append performance of ZNS with SPDK. The overhead for recovery, while not negligible, is less than the performance gains that can be achieved for appends in certain use cases. Therefore, it would be worthwhile to also investigate the WAL design for ZNS SSDs where append performance is able to scale better and achieves better than writes. If it is, it would be a good idea to make use of concurrent appends where possible, also for existing key-value stores. For example, making a separate design for WALs in RocksDB that can make use of multiple outstanding appends.

\section{Limitations}
There are a few limitations in the research that is conducted. It is only right that these are properly described, so that a correct conclusion can be formulated. Most of these limitations come from the novelty of TropoDB, the lack of proper tooling to verify correctness, measure performance and correctly measure garbage collection effects of TropoDB.\\
\textbf{Inaccuracies in measurements of garbage collection effects}:\\
Measuring the effects of garbage collection is not trivial. We were not able to measure the effects of garbage collection of TropoDB during the readwhilewriting measurements because of a bug, but there are also other issues. Garbage collection affects many components and is inherently tied into the entire storage stack. This makes it problematic to exclude interaction effects from the equation. Generally, in this research, an approach is taken that is only able to measure some effects by approximation. Inherently RocksDB and TropoDB are different key-value stores, even if they are similar in many regards. This can not be understated as the effects of the differences directly alter the characteristics of the key-value stores. Therefore, it is not possible to compare garbage collection of the two designs without also inevitably adding general performance characteristics of the two databases into account. For example, compactions might happen at less fortunate moments, be more frequent, involve fewer SSTables or be prioritised differently. This is worsened by the different background thread models. RocksDB can use a large number of compaction threads, TropoDB can not. The real garbage collection effects might (and probably are) be a bit different than shown in the conclusion. The results are still valuable as they are a close approximation and showcase if TropoDB is able to outperform the state-of-the-art. It is just necessary to state that the lesser results might be from different aspects of the key-value store than just the storage aspects themselves.

Additionally, the effects measured are noisy. Even if a large number of I/O is used, latency will fluctuate and so will the measured tail latency. There is a great deal of randomness involved when using multiple background threads. Further on, there might be inaccuracies in the write amplification measured or the number of resets issued. TropoDB has its own set of measurement tools for these measurements, which might work differently than the ones used for RocksDB. As they are less battle-tested than for example the S.M.A.R.T. log, they may even be incorrect in the worst case, even if the functions are properly tested (mistakes do happen). Further on, while TropoDB and ZenFS can only operate while the database is running, F2FS is not hindered by this constraint. This might result in F2FS resetting zones or writing data after the database is closed. Some of these operations might, therefore, not be measured.

Lastly, comparing against F2FS is not entirely fair. F2FS in the evaluation makes use of a second Optane disk for file system metadata operations. Such operations are assumed to be negligible, but are discarded from the results. The F2FS results, therefore, do not include all written bytes or resets (internal Optane deletes). \\
\textbf{TropoDB is the first of its design}:\\
TropoDB is the first key-value store that attempts to build a key-value store directly on ZNS (and in user-space at the same time). This makes it challenging to properly compare it to alternatives. The limitation is similar to the limitation of measuring garbage collection effects: not totally accurate comparisons. For example, there is no alternative ZNS WAL implementation or L0 ZNS implementation to compare against. It is only possible to test either the global effects changing a component has compared to another full implementation (which is probably differently designed) or by implementing multiple components in TropoDB and measuring their differences. The second approach leads to challenges itself. As all research is dependent on by one team of researchers (in this thesis at least), which can lead to bias for a data structure. Future research, if conducted, should also depend on different components being designed and properly tested against each other (and if possible by multiple researchers).\\
\textbf{Limitations of working in user-space}:\\
Working in user-space comes with benefits such as more control over storage and generally higher performance, but it also comes with major limitations. For example, many tools are missing. This includes many debugging and performance tools. If those tools are available, it requires fewer reimplementations of existing tools, but it also prevents mistakes. Mistakes in debugging tools or non-trivial debugging tools can lead to undetected errors and safety issues. Mistakes in performance tools can lead to incorrect results, which can lead to incorrect conclusions.\\
\textbf{Evaluating key-value stores and ZNS performance in a virtualised environment}:\\
All experiments were run within QEMU. It is assumed that the effects of such an approach are limited, but this has not been measured. We have only roughly measured the impact of NUMA (\autoref{sec:experimentpreparation}). Further on, the ZNS SSD is used within the VM with PCIe passthrough and the Optane disk with paravirtualisation. Some performance could have been lost in the configuration used, which is again not tested.\\
\textbf{Reliability and usability of TropoDB}:\\
TropoDB is not a complete database. It misses many functionalities that RocksDB has implemented and some database users may consider to be essential. For example, there are no scan operations, filters (bloom filter, cuckoo filters, ribbon filters to name a few) or column families implemented. In addition, crash tolerance is not guaranteed, tested or a goal of TropoDB. Already existing solutions, generally do provide better reliability. This raises issues for TropoDB. If those features would have been implemented, the key-value store would behave differently and it might perform worse in the aspects tested. Therefore, it is not completely fair to compare TropoDB to RocksDB. It must be stated that RocksDB is thus tested against a proof of concept and that it is an approximation of what a full key-value store on ZNS can achieve with TropoDBs design.

\section{Future Work}
Throughout this report, we have mentioned various designs, implementations and benchmarks that have not been implemented/conducted, but that would be valuable to investigate in future research. We will not restate all of these issues in this section, but we will give a short overview and come up with some broader ideas for future work, that we think are good next steps to conduct after TropoDB.\\
\textbf{Designing new components and evaluating individual components for TropoDB}:\\
Many components for TropoDB have not been properly benchmarked to investigate their effect. To mention a few: effect of SSTable size in L0 and LN, zone allocator designs, effects of the number of WAL regions assigned, effects of WAL sizes, performance of writes in WALs, asynchronous I/O for LN, performance of get operations, different thread models and their impact, different compaction strategies. We have also limited ourselves to a predefined set of benchmarks. Using different value sizes or workloads, for example delete heavy, or entirely different benchmarks such as YCSB, can also give insights.
Additionally, we have only designed and implemented a few unique storage data structures for each component. We have seen that a circular log in L0 comes with numerous challenges, different designs for L0 such as a fragmented logs would also be good research research options. Lastly, we have tested all I/O on one type of ZNS SSD. It would also be valuable to verify the results on a different ZNS SSD with other characteristics. For example, better append scalability, smaller zones or a different number of active zones.\\
\textbf{Adding multi-tenancy support to TropoDB}:\\
TropoDB does not support multi-tenancy~\cite{kabbedijk2015defining}. It is not possible for multiple connections to open to the database or for multiple databases to exist concurrently on the same ZNS device. This is done by design as it gives the key-value store the maximum that is achievable on the storage. However, this is not entirely realistic in reality. Many applications run in parallel with others on the same machines in practice~\cite{aljahdali2014multi, bezemer2010multi}. Storage, CPU and memory are shared in such cases. To make a ZNS approach more usable, it should support these scenarios as well. SZD already comes with simple \textit{containerisation} by limiting the zones that can be used by an application, but this should be taken further. For example limiting the number of open/active zones an application can use, prioritising applications and adding permissions for applications. Additionally, running multiple applications in parallel has profound implications on the performance of the applications, garbage collection included. This can be investigated further in future research.\\
\textbf{Implementing TropoDB in non kernel-space}:\\
Running in user-space comes with many disadvantages, but is also not always possible. Many key-value stores run in the cloud along with many other applications on similar machines (multi-tenancy). In general, permissions are limited and applications can not always be used in user-space~\cite{kourtis2019reaping}. TropoDB will thus not work in such situations. For such situations, it is important that there is also a solution that requires fewer permissions and does not need to run the storage in user-space, but can also run in kernel-space instead. Alternatively, it should also be possible to use a swappable storage backend that uses user-space if possible and not if it is not. This achieves better performance if user-space is available and still guarantees availability in the general case. A similar approach to uDepot can be taken to achieve this~\cite{kourtis2019reaping}.\\
\textbf{Creating an entirely different design for TropoDB}:\\
TropoDB is mainly built on top of LevelDB logic. This logic is not made for modern flash storage and is not able to scale as well as the storage can provide. Many of those challenges can be altered, but inherently it continues on a design that is not made for the storage used. This leads to locking issues and a workaround to make the model work properly with various threads. This might have also caused the concurrent design we added later on to TropoDB, to decrement in performance, rather than to improve in performance. It might, therefore, be a better approach to begin with a completely new design from the start that is made to handle the level of concurrency from the start. This prevents inefficient workarounds. To achieve such designs, approaches for faster storage with high parallelism such as persistent memory are a good start for investigations. Such designs can be reused if applicable.

Additionally, TropoDB takes a unique approach to pre-allocate the amount of space that each component might require. This comes with advantages, but also disadvantages. For example, not being able to alter most components after database creation. Future designs can look into alternative solutions that are able to modify the structure on the fly. Hot and cold separation could have also been done differently. For example, using an algorithm similar to CAZA~\cite{lee2022compaction}, instead of separating hot and cold SSTable data by separating levels.

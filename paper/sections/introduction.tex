\chapter{Introduction}
\label{sec:introduction}
We live a data-driven society. Each and every one of us generates data on the daily. This data is collected from all sorts of media, platforms and devices. For example, data is generated by social media~\cite{eberendu2016unstructured,jay20212facebook,vagata2014scaling}, IoT and edge computing~\cite{beierle2020data,sutherland2014forensic,bhatt2020next}, Large-Hadron Colliders~\cite{grange2022astronomical, avery2002data}, bio-informatics and cancer research~\cite{hinkson2017comprehensive,yang2015databases,greene2014big}, and ecommerce~\cite{akter2016big,ghandour2015big,patel2012addressing}. The amount of data this creates is enormous and ever-increasing. The amount of data is expected to reach 180 zettabytes in the year 2025~\cite{patrizio2018idc}. All of this data needs to be stored somewhere and loaded from somewhere. Storing and loading data is not free, it requires storage devices to maintain all of the data. Additionally, it needs to be accessible in a timely manner. No one wants to wait minutes for their data to be processed. Therefore, there is a large investment in data centers and other ICT infrastructure. According to a recently published CompSys Manifesto for the Netherlands, the ICT infrastructure of the Netherlands enables at least 3.3 million jobs and contributes to 60\% of its GDP~\cite{iosup2022future}. 

To be able to facilitate access to data and fast storage, researchers and industry alike have done many innovations in the past decade. Innovations have been made in both hardware and software technologies to deliver high performance. For example, we have seen interest in storage media such as flash-based Solid State Drives (SSD)~\cite{doekemeijer2022key}, Optane storage~\cite{wu2019towards} and even DNA storage~\cite{li2020can}. In the last decade, flash-based SSDs have become the norm for fast storage. State-of-the-art SSDs are typically connected to machines with a PCIe connection and a protocol known as NVMe~\cite{lu2022nvme,xu2015performance,lee2019asynchronous}. Such SSDs can deliver throughput of up to 3GB/s and sub-ten microseconds of I/O latency~\cite{lee2019asynchronous}. NVMe SSDs are not a technology that will become obsolete anytime soon. On the contrary, it is expected that the market share of NVMe SSDs will grow from 44.6 billion USD in 2022 to 163 billion USD in 2025~\cite{2022expectednvme}. However, it is not enough to just use a fast storage medium. In order to guarantee that data is accessible in a timely a manner, the software that is built on top of the hardware needs to be fast as well. Therefore, there have been innovations in software to store the data and communicate with the hardware as well. For example, there have been innovations in file systems~\cite{lee2015f2fs,bjorling2021zns}, object stores~\cite{hwang2014heapo,macko2017smore}, relational databases~\cite{matsunobu2020myrocks,kang2014durable,cao2020polardb} and key-value stores~\cite{doekemeijer2022key} alike. Every one of these data stores has its own advantages and disadvantages. When data stores are properly combined with the underlying hardware, they can scale along with the demands of the data-driven society.  In this research we also look at the challenges involved in properly combining hardware with software to scale along with the data-driven society. In particular, we will look at properly combining a database known as a \textit{key-value store} with a type of hardware known as a \textit{flash-based SSD}.

\section{Key-value Stores with Flash Storage}
Key-value stores are a type of database and a common way of storing data. One of the few requirements of a key-value store is to be able to store data as a collection of key-value pairs. Data can be stored by the client with simple \textit{put} operations and retrieved by client with simple \textit{get} operations. This abstraction can also be seen in \autoref{fig:kvstore}.

\begin{figure}[h]
\centering
\begin{minipage}{0.45\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/kvoverview.svg}
\end{minipage}%
\caption{ Key-value store: Put and get operation }
\label{fig:kvstore}
\end{figure}

Key-value stores are considered to be a part of the \textit{NoSQL} movement and have since their inception been used in all sorts of applications and have become ubiquitous~\cite{doekemeijer2022key}. Many large companies such as Google~\cite{chang2008bigtable,LevelDB}, Facebook~\cite{dong2021evolution}, Amazon~\cite{sivasubramanian2012amazon} and Microsoft~\cite{debnath2011skimpystash}, are known to use them.  Their usage has reached far and nowadays they can even be used as the backend for traditional SQL databases~\cite{matsunobu2020myrocks}. Further on, key-value stores are also used heavily for latency-sensitive/high-throughput use cases~\cite{lersch2020enabling,doekemeijer2022key}. Key-value stores are thus a prime optimisation target and a topic that attracts a lot of research efforts~\cite{doekemeijer2022key}. A part of this research is focused on the storage medium that the data is stored upon, such as the research that we have conducted for this thesis. 

Key-value stores are commonly stored on top of flash-based SSDs.
However, flash storage does not come without its own challenges and idiosyncrasies. For example, flash storage lacks support for overwriting data. Instead, data must first be erased, before it can be overwritten. It thus comes with a write and an erasure operation, instead of just a write. Internally the SSD stores its data in \textit{flash cells}. A property of flash storage that is challenging, is that these flash cells \textit{degrade}. Each write or reset to a flash cell degrades the durability of a flash cell, which is commonly known as \textit{wear levelling}. Similarly, reads also degrade the durability, which is known as \textit{read disturbance}~\cite{liu2015read}. Therefore, it is essential to reduce the amount of I/O an application needs when flash storage is used as it can increase monetary costs~\cite{agrawal2008design}.

A typical approach to deal with SSDs up until now has been to hide away the idiosyncrasies and expose the device to the user as if it is a traditional block device; a device, that many developers are already familiar with. This is done through an interface, which is typically implemented directly in flash devices themselves as firmware. This interface is known as the \textit{flash translation layer} (FTL)~\cite{chung2009survey}. The FTL does allow overwrites and does not require the earlier mentioned erasures. Clients can write and read anywhere. It is left up to the FTL to come up with an efficient solution for random writes. As data can only be erased at the erasure level, the FTL is as a result forced to move data around in order to reclaim old space. This is typically done through a process known as \textit{garbage collection} (GC). When an FTL is used on the SSD, this can lead to a storage stack such as is visible in \autoref{fig:ftlstoragestack}.

\begin{figure}[t!]
\centering
\begin{minipage}{0.45\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/linuxstoragestack.svg}
\end{minipage}%
\caption{ Linux kernel with a traditional SSD: Storage stack }
\label{fig:ftlstoragestack}
\end{figure}

This figure represents how applications can interface with flash storage within the GNU/Linux operating system. In this stack, applications need to go through various layers of kernel abstractions, before it can interface with the SSD. When the request has reached the SSD, the FTL processes the request and only after this has been completed, the request is sent to the flash storage.\\
\textbf{The Challenge of Garbage Collection}:\\
Garbage collection (GC) on flash storage is considered undesirable, because it causes interference in I/O latency and throughput. Interference in latency is causes because GC claims the resources that are available on the device. When the resources are claimed, they can not be used by client-issued operations. It is generally beyond the clients control to determine when GC should be done, which can lead to interference peaks. Such peaks increase latency of I/O operations. In other words, some operations will take longer to finish and clients have to wait. This is problematic as many companies make use of \textit{Service Level Agreements} (SLA) that specify the maximum allowed latency of an operation. Such an agreement can be set to guarantees such as: 99\% of all operations finish in less than 200 microseconds and 99.99\% of all operations finish in 10 seconds~\cite{lersch2020enabling}. GC makes such SLA's troublesome as it is hard to guarantee a low latency. Additionally, longer latencies can also have significant influence over the behaviour of customers. To give some examples: Linden et al. reported that for Amazon every 100ms of latency costs them 1\% in sales~\cite{hoff2009cost,linden2006}, Mayer et al. report that for Google an extra latency of .5 second causes traffic to drop by 20\%~\cite{hoff2009cost,mayer2006} and brokers can lose millions of revenue if their trading platform is 5 miliseconds behind the competition~\cite{hoff2009cost}. Therefore, there is a real incentive to keep tail latency to a minimum. Garbage collection hinders this incentive.

Garbage collection also causes \textit{write amplification}~\cite{yang2014garbage}, which means that the amount of physically written data is more than the logical amount of written data requested by the client. Garbage collection moves data around in the background to facilitate data overwrites. Each data movement caused by garbage collection adds extra write amplification. Earlier, we already established that flash storage has a problem known as wear levelling. Overwrites are, therefore, not only an issue for performance, but they can also degrade the life expectancy of flash storage significantly, which will cost money as new storage needs to be bought. Another problem that arises with the usage of GC is that it requires the SSD to maintain extra data in its DRAM (device memory). This extra data is needed because the FTL needs to maintain where all its data is stored to facilitate garbage collection.

To continue, the FTL and its GC suffer from a problem that is known as the \textit{semantic gap}~\cite{zhang2017flashkv}. The semantic gap means that the FTL has no knowledge of the application that is using its interface. This means that it can not accurately predict what data is either \textit{hot} or \textit{cold} or how data is even roughly related. Hot and cold data in this case refers to how frequently the data is updated. It is beneficial to separate frequently updated data from infrequently update data, as this allows minimal write amplification for cold data. Remember that the GC can move data around to support overwrites. If data is not frequently updated, and is thus cold, it should in general not move. The semantic gap causes the garbage collector to frequently put together data that is only written once (cold) with data that needs frequent overwrites(hot), which causes expensive overwrites, and essentially aggravates write amplification issues.

Lastly, as the FTL is generally left up to the SSD vendor, it means that there are various implementations, with each implementation behaving differently and requiring new optimisations from users of SSDs. GC is not something that can be avoided as it can happen at undesirable times, such as during peaks of database usage~\cite{balmau2019silk}. This complicates optimisation efforts significantly for developers.

\section{Emergence of Zoned-namespace Devices: an Opportunity}
There is a solution to these issues. This solution is a new interface for SSDs, known as \textit{Zoned Namespaces} (ZNS). ZNS is a thin interface that no longer resorts to a block interface and comes with no explicit garbage collector on the device itself~\cite{bjorling2021zns}. Instead, we use a common interface that naturally fits flash storage, a \textit{zone interface}. In this interface, the SSD is divided into zones, with each zone corresponding to an individual \textit{erasure unit}, which can be erased by the application itself when necessary, leaving optimisation and garbage collection up to the application. For GNU/Linux this leads to the stack visible in \autoref{fig:znsstoragestack} instead of \autoref{fig:ftlstoragestack}. The erasure operation can thus now be issued by the user of the SSD explicitly as the application is in charge of garbage collection. Further on, in ZNS data can only be appended to zones as overwrites are explicitly not allowed. Remember that data is not moved around in the background anymore by a garbage collector. However, reads can still be done everywhere on the device. 

\begin{figure}[t!]
\centering
\begin{minipage}{0.45\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/linuxznsstoragestack.svg}
\end{minipage}%
\caption{Linux kernel with a ZNS SSD: Storage stack}
\label{fig:znsstoragestack}
\end{figure}

An advantage that ZNS brings is that ZNS is a common interface on top of the NVMe host interface specification~\cite{NVMeSpec}, reducing the need to create a solution across different architectures and reducing the cost of optimisation efforts. This is different from traditional FTLs, where individual implementations can be very different. Further on, garbage collection is left entirely up to the host. Meaning that applications can remove the semantic gap and the entire storage logic can be optimised for one particular application, such as in our case key-value stores. It thus allows the key-value store designers to consider how the data should be laid out explicitly on the SSDs flash storage and how data should be garbage collected in the most efficient manner possible depending on the use-case. 

A key-value store itself also comes with garbage collection (GC) procedures. These typically come in the form of background operations. Similar to the GC of ZNS, the GC of key-value stores moves around data to different locations and deletes old data. With ZNS, a key-value store is able to optimise ZNS garbage collection procedures for its own use-case. This can lead to a new garbage collection design, in which the two garbage collection procedures are integrated. We refer to this phenomenon as \textit{co-optimising garbage collection}.

Our hypothesis is that if garbage collection is done correctly with the help of ZNS (and is properly co-optimised), it can reduce wear levelling, reduce read disturbance and increase write and read throughput of key-value stores. It can also aid in making latency of I/O more stable as it is more predictable what the SSD is currently doing. Lastly, as it uses the same hardware as ordinary SSDs, there is no additional monetary cost involved in using the interface.

As a co-optimisation it is also possible to reduce the number of abstractions on the host. For example, removing most of the operating system logic and giving full control over the SSD to an application. In this case, the SSD is essentially run in user-space. This can increase latency stability and improve throughput. When such a stack is used within GNU/Linux, it would look like \autoref{fig:znsuserstoragestack}.
Our hypothesis is that this can also increase latency stability and throughput for applications like key-value stores.

\begin{figure}[t!]
\centering
\begin{minipage}{0.45\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/znsuserspacestack.svg}
\end{minipage}%
\caption{ ZNS SSD when used in user-space: Storage stack }
\label{fig:znsuserstoragestack}
\end{figure}

\section{TropoDB: Optimising KV-Store for NVMe Zoned \mbox{Namespace} Devices}
In this thesis, we focus primarily on one of the main advantages of ZNS, co-optimising garbage collection of flash storage for key-value stores by reducing the semantic gap. To define this issue, we should first take a look at the current situation for key-value stores. Key-value store designs such as LSM-trees are already ``aware'' of the need of hot and cold separation and typically attempt to put together data that is related. For example, component ``A" of key-value store ``x'' is colder than component ``B'' and should, therefore, be stored elsewhere in a different location. Unfortunately, key-value stores that use LSM-trees typically have no control over the internal logic of SSDs and can not force the SSD to store data separately. Key-value stores can only guess how data is stored on the device or in the best case give hints to the SSD (e.g. with multistream SSDs~\cite{lakshman2010cassandra, dong2021evolution, yang2015optimizing}).

The ZNS interface removes the lack of control over the storage~\cite{purandareappend}. We should now be able to have full control over the layout of all key-value store data and be able to remove the semantic gap at the device level. As an extra co-optimisation it is also possible to remove the file system and most operating system logic as well when used with a key-value store. This is not ZNS-specific, but it removes yet another interface and semantic gap. This approach is taken for TropoDB. When all abstractions layers between the key-value store and storage removed, it gives the key-value store full access to the ZNS interface and full control over how the data is managed. If some layers remain between the key-value store and ZNS, the ZNS effects can no longer be properly be used to their fullest potential. Removing all layers is possible when the key-value store is used within user-space, with for instance the \textit{Storage Performance Development Kit}(SPDK) on GNU/Linux~\cite{spdk}.

If this lean stack is designed and implemented efficiently and correctly, such an approach can make the latency of key-value stores more stable; it will make the key-value store easier to predict and can stabilise latency. This is beneficial when the key-value store should respect SLAs set by clients. Further on, it is hypothesised that it can reduce wear levelling effects significantly and thus increase the life expectancy of flash storage in SSDs~\cite{purandareappend,bjorling2021zns}.

In this thesis, we have investigated one particular type of key-value stores, \textit{Log-structured Merge-tree} (LSM) key-value stores. The reason that we are looking at LSM-tree key-value store designs specifically is because LSM-trees are seen as a natural fit for ZNS and are considered to be a prime target for ZNS optimisations~\cite{purandareappend, stavrinos2021don}. It is considered to be a prime target because the data in LSM-trees is generally organised in large contiguous chunks that are treated as append-only logs. The logs could then directly correspond to ``N" zones and the concept of append-only fits nicely with the philosophy of ZNS focusing on appends instead of random writes.

\section{Research Questions}
\label{sec:researchquestions}
Due to the timely nature of this work on newly standardised NVMe ZNS devices, the research space is under-explored, thus opening opportunities for multiple research avenues around a closer storage-application integration. To reduce the design space complexity, this research focuses on a few major ideas that can potentially aid in similar ZNS-integration efforts, not just key-value stores. The overarching research question in this thesis is ``how to leverage the unique design properties of NVMe ZNS devices to optimise a key-value store‚Äù. Under this broader question, we tackle the following subquestions:
\begin{itemize}
    \item \textit{RQ1: What are the key-value store integration choices available on top of ZNS SSDs?}\\ 
    Due to the open and complex nature of key-value store design space (for a more comprehensive treatment of the topic see a key-value store survey by Doekemeijer et al. \cite{doekemeijer2022key}), we need to both synthesise and identify optimisation opportunities that can be practically tackled within the limited scope of this thesis. Specifically, we target a design space analysis around how ZNS devices have been integrated into an existing system and how key-value stores with the regards to the choice of software frameworks, data structures, garbage collection location, and effort makes use of such an integration. The design space exploration and the position of this thesis in comparison to the state-of-the-art and the state-of-the-practice is done in \autoref{sec:approach}.
    \item \textit{RQ2: What unique optimisation opportunities does a ZNS device offer for LSM-tree based key-value stores?}\\
    Both LSM-tree and NVMe storage are complex topics and there exists a large body of research into both of these topics (see \autoref{sec:novelty}). The aim of this question is to identify unique optimisation opportunities that our choice of integration offers (the integration identified in RQ1). In this research question we explore the LSM-tree data structure design space with its access, background operations, and concurrency properties to co-optimise them for the garbage collection on ZNS devices. This research question will be answered in \autoref{sec:design}.
    \item \textit{RQ3: How to implement and evaluate the design of an LSM-based key-value store on ZNS devices?}\\
    The design of a key-value store with a co-optimised data structures, processes, and policies is complex and needs verification. An implementation of such a complex idea must reconcile practical challenges regarding the choice of hardware available, operating system abstractions, storage APIs, I/O stacks, the use of programming language, debugging and development challenges regarding building the first-of-its-kind artifact. We will present our experience and analysis of implementation related challenges in section \autoref{sec:implementation}.
    \item \textit{RQ4: What is the impact of the optimisation process of TropoDB on the key-value store and ZNS device?}\\ There are limited publicly available guidelines and information available on the performance characteristics of ZNS devices and associated software ecosystems like key-value stores or file systems. Hence, to scientifically quantify and verify the claims that are made in the thesis, we must establish the best practices for experiment setup, design experiments (layered evaluation from the ZNS device to the key-value store itself), data collection, and analyse them to study the impact of various design decisions on the overall performance of the key-value store. We report on our findings in \autoref{sec:experiments}.
\end{itemize}

\section{Research Method}
This thesis project is a systems research in which we create a new key-value store.
First, we will do an integration exploration on how to integrate ZNS and a key-value store (\textbf{RQ1}). Then, after this exploration has been completed, we will start on the key-value store itself. We do this by designing (\textbf{RQ2}), implementing (\textbf{RQ3}) and evaluating (\textbf{RQ4}) the key-value store ourselves.

The following methodologies were used to carry out the research. Firstly, a literature survey was carried out (separate from this thesis) to investigate key-value stores for flash storage~\cite{doekemeijer2022key} (\textbf{M1} Quantitive research~\cite{kheir2018systems, levy2006systems}). This also helped to get an understanding of key-value stores for ZNS SSDs and forms the background for this research.

The design and implementation of the key-value store was carried out with a back-and-forth process. Ideas were abstracted, designed and then prototyped and when prototypes did not work as expected, we went back to the the design and abstraction phases. This is an iterative process (\textbf{M2} design, abstraction, prototyping~\cite{iosup2019atlarge, hamming1997art, peffers2007design}).

The entire research is conducted in the open. The design, implementation and benchmarks are carried out with methods of experimental research (\textbf{M3} Experimental research, designing appropriate micro and workload-level benchmarks, quantifying a running system prototype~\cite{jain2008art,gernor2202crimes, ousterhout2018always}) and are all open-source (see \autoref{sec:appendix} and \autoref{sec:contrib}) and are within the principles of open-science (\textbf{M4} Open-science, open-source software, community building, peer-reviewed scientific publications, reproducible experiments~\cite{bezjak2018openscience, wilkinson2016fair,berger2019crisis,uta2020big}). 

\section{Thesis Contributions}
\label{sec:contrib}
The primary result of this thesis is the design and implementation of the key-value store TropoDB. TropoDB \textbf{is the first key-value store that is fully designed and implemented on top of ZNS without a file system, kernel and block in layer in between the application and the ZNS device}. Additionally, it runs entirely in user-space. It is novel in its approaches to make use of unique data structures to store LSM-tree components. Furthermore, its exclusive use of \textit{append} I/O operations instead of \textit{write} I/O operations. The thesis addresses various issues caused by garbage collection and how those issues can be solved with ZNS. For example, how to separate hot and cold data and when and how to delete data. In addition, in the end product, we have explicitly separated the database code from the storage interfacing code, allowing the storage interfacing code to be reused for other (research) projects. The logic in this project can be of value for other ZNS projects. As TropoDB is the first of its kind, we also came across, and discussed, various design decisions for TropoDB that do not work properly or do not scale. These can be valuable lessons for future attempts at creating key-value stores or other applications built directly on top of (ZNS) SSD storage. Specifically, the contributions of the thesis are the following:
\begin{enumerate}
    \item Design and integration considerations for building a key-value store on top of ZNS SSDs. Specifically how LSM-trees fit in zones and how garbage collection of LSM-trees can look for ZNS SSDs.
    \item SimpleZNSDevice (SZD), a small API built on top of SPDK, meant to interface with ZNS. This API comes with a simple configuration and some data structures optimised for ZNS. It can be reused for any ZNS project.
    \item Benchmarks for ZNS SSDs that showcase the raw device characteristics of reads, appends, writes and resets when SPDK is used.
    \item TropoDB, a key-value store built directly on top of SZD in user-space with the help of SPDK. We explore design choices regarding the underlying ZNS storage with a focus on garbage collection. 
    \item An investigation of TropoDB, showcasing its tail latency, write amplification and zone reset characteristics compared to the state-of-the-art and the state-of-the-practice.
    \item Publicly available source code, both for SZD, SPDK and the benchmarks. The source code can be found respectively for TropoDB, SZD and the benchmarks at \url{https://github.com/Krien/TropoDB},  \url{https://github.com/Krien/SimpleZNSDevice} and \url{https://github.com/Krien/ZNS_SPDK_Benchmarks}. The sources are described in more detail in \autoref{sec:appendix}.
\end{enumerate}

\section{Research Relevance}
The research relevance of TropoDB lies in its novelty. TropoDB is the first key-value store built entirely on ZNS SSDs. This thesis has shown that such an approach is feasible. Additionally, by implementing the database in user-space, we have been able to investigate the viability for key-value stores in user-space with ZNS as well. During the design and implementation of TropoDB, we have come across various challenges that were not known before and are specific to ZNS. We have shown that a few designs are able to come close to what is achievable on ZNS, while others are not. The WAL design for example is able to closely match the raw performance of ZNS devices and is a design that has shown its merit for key-value stores. The other components have had less success, but by demonstrating the potential of dividing an SSD in various components, we have paved the way for future research. It should be possible to try out various data structures for different components and find designs that work better for their use cases. This thesis built a platform for future research in a direction of  improving average latency, tail latency, throughput, wear levelling and concurrency capabilities for all applications that make use of storage, not just key-value stores. Many ideas, such as design of WAL, are more broadly applicable beyond key-value stores, such as in file systems or relational databases. It also shows that is possible to investigate each component separately in a key-value store. However, considering that without a file system everything needs to be crafted by hand, piece by piece, it requires a significant investment from the research community. 

\section{Societal Relevance}
In this research we aim to design and implement a high performance key-value store. The goal is to tackle the influx of large amounts of data and the ever-increasing needs that go along with this data. In the end, the large amount of data has implications for monetary costs and and environmental costs\cite{iosup2022future}. We address this challenge by coming with a key-value store that is optimised for the underlying SSD storage medium. Such a co-optimisation can reduce latency issues and increase device lifetime. The proposed key-value store can lead to storage systems that are better provisioned, more efficient and scale better, all at the same time. It is common to scale systems by adding multiple units/nodes\cite{li2013distributed,escriva2012hyperdex,cao2020polardb}. Such an approach leads to both monetary costs and environmental costs. The solution proposed in this thesis, allows for increasing the performance of a single unit instead. The result is that less units are needed to guarantee performance and both monetary costs and energy costs can be decreased. Additionally, the approach taken can increase device lifetime significantly. Therefore, there is less investment needed to replace broken storage devices, which again reduces monetary and environmental costs. In short, this research aids society by allowing time-sensitive applications to work in low latency, and by reducing economical and environmental costs of data.

\section*{Plagiarism Declaration}
I confirm that all material present in this report, unless explicitly stated, is the result of my own efforts. No parts of this report are copied from other sources unless credited and properly cited. The work has also not been submitted elsewhere for assessment. I understand that plagiarism is a serious issue and should be dealt with if found. Note that the background section contains parts from my survey on flash-based key-value stores, which is my own work~\cite{doekemeijer2022key}.
\chapter{Implementation of TropoDB}
\label{sec:implementation}
In this section we answer RQ3: \textit{``How to implement and evaluate the design of an LSM-based key-value store on ZNS devices?"}.
TropoDB is an implementation of the key-value store design given in \autoref{sec:design} for the storage engine SPDK on GNU/Linux and closely matches the proposed design. Nevertheless, implementations are always different than designs. In the case of TropoDB, it contains some deviations and optimisations that are made possible because of SPDK. Additionally, we did not discuss the components of the database that are completely in-memory. For example, memtables or the SSTable cache. In this section we will describe these decisions/implementations as well, why they were chosen and if they have any disadvantages that are worth mentioning. 

We will explain our implementation in a number of steps. We begin by setting requirements for the implementation. We will then explain our lower storage API, SZD. This will be followed by looking at how we intent to evaluate the implementation. To continue, we will look at our implementations for data structures for both storage and memory. We will finish by looking at the implementations for background operations and the key-value store deletion operation.

\section{Requirements}
There exist various possible implementations of the proposed design. It is important that we set up a number of requirements to ensure that we come with an implementation that does the design justice and addresses the main research question. We define the following \textbf{Implementation Requirements (IR)}:
\begin{itemize}
    \item \textbf{IR1}: \textit{The implementation should be build bottom-up in RocksDB}:\\
    In \autoref{sec:approach} we have stated we will built TropoDB in RocksDB with bottom-up approach. This should be adhered.
    \item \textbf{IR2}: \textit{Storage should be managed in user-space}:\\
    In \autoref{sec:approach} we have stated we will built TropoDB in user-space with SPDK. This should be adhered as well.
    \item \textbf{IR3}: \textit{TropoDB should rely on one ZNS SSD only}:\\
    TropoDB is an implementation for ZNS SSDs only. It is not allowed to make use of other devices to store extra data. For example, storing metadata on a different disk that does allow for random I/O is not allowed, even for debug storage. Remember, we are building for ZNS here and should not force an implementation that requires extra non-related storage devices. Additionally, TropoDB should not require the use of multiple SSDs as well. In order to evaluate the system, concurrency should be testable. When multiple devices are used, such an evaluation is hindered.
    \item \textbf{IR4}: \textit{The implementation should support db\_bench with minimal alterations}:\\
    The implementation will be evaluated using db\_bench. This requires the database to support db\_bench. It is not allowed to modify db\_bench beyond adding support for interfacing with TropoDB. For example, some minor fluctuations are needed to allow db\_bench to interface with TropoDB, but no workloads should be altered. The system should be evaluated \textit{fairly}.
\end{itemize}

\section{Simple ZNS Device (SZD)}
The implementation of TropoDB is split into two parts: the database itself and a library to interface with SPDK. This allows decoupling the database logic from the storage logic. The interface with SPDK is named \textit{Simple ZNS Device} (SZD) and handles all generic I/O logic. We consider it necessary to shortly explain SZD as SPDK differs from other storage APIs and SZD is not raw SPDK itself. SZD itself is split into two parts of code. A low-level C API and a high-level C++ API. The resulting layered design looks like \autoref{fig:tropolayers}.

\begin{figure}[h]
\centering
\begin{minipage}{0.35\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/TropoDB_layers.svg}
\end{minipage}%
\caption{The software layers in TropoDB}
\label{fig:tropolayers}
\end{figure}

The SZD C API directly calls SPDK functions. It is in charge of discovering, opening, closing and reopening ZNS devices. This is entirely name-based on PCIe addresses, making it trivial to open devices based on their PCIe addresses. It comes with functions to allocate DMA-backed memory through huge pages and release it. Additionally, it comes with synchronous and asynchronous I/O functions. It supports commands for appends, reads, retrieving zone heads, resets and zone finishing. It does not expose writes to users by design, it is append-only. All I/O is done with SPDKs \textit{QPair} implementation, which is an abstraction around software/hardware queue pairs. There can be more software QPairs than there are hardware queue pairs for example. Qpairs are not thread-safe and require external synchronisation. In addition, all buffers require the memory to be present in \textit{huge pages} to properly make use \textit{Direct Memory Access} (DMA), which in turn allows the storage to read and write to memory. This is also left up to the host to guarantee. As we will discuss later on, the C++ API hides some of this logic away. Synchronous I/O makes use of busy polling and only returns once the polling it conducts, has either completed or errored. Asynchronous I/O returns an object which can be polled at any convenience. The operation supports cleaning data on completion, but it needs to be polled at least once in this state. SZD does not contain any interrupt logic. SZD does come with various forms of error handling and automation. Error handling prevents I/O from being sent that would be erroneous and prevents out-of-bounds I/O for, for example, appends to incorrect read addresses. Automatisation is done to prevent having to deal with all ZNS intrinsics. Appends larger than ZASL are split into smaller parts and reads larger than MDTS are split into smaller reads. Further on, when I/O crosses the border of a zone, it automatically moves on to the next zone. A final feature that is unique to SZD is simple containerisation. SZD allows specifying the minimum and maximum zone that an application can use. When an application is initialised this way, no I/O may cross their respective border. This should make it possible to store data from multiple applications or databases on the same device. 

The C++ API adds many conveniences on top. For example, it allows creating a DMA-backed buffer. This buffer can then be reused when necessary. This prevents repetitive patterns of allocation, delete, allocation and delete. The C++ API also does not directly expose QPairs, but comes with a simple abstraction on top, which we refer to as \textit{channels}, a reference to OCSSDs~\cite{bjorling2017lightnvm} but unrelated in practice. Channels come with convenience functions for I/O. A major feature of channels is that they require setting a specific range to which they may conduct I/O. This prevents out-of-bounds issues that could occur. For example, WAL channels should not have access to L0 zones. It is possible to send I/O requests without DMA-backed memory in which case the channels themselves create some DMA-backed memory, copies the data to this memory and then does the deletion of this \textit{new} memory automatically in the background after completion. Because DMA in SPDK is based on hugepages, the number of DMA memory that can be allocated is limited and I/O is limited to ZASL and MDTS, it is problematic to deal with large I/O. Therefore, large I/O is split in smaller parts of ZASL and MDTS in this case. Only a buffer of up to ZASL is allocated for appends and MDTS for reads. The I/O is then sent in parts and the buffer is reused.  Additionally, it allows writing and reading to any address in the earlier mentioned DMA-backed buffer, allowing for simple address virtualisation. Asynchronous I/O comes with polling abstractions on top and automatic memory management. Lastly, it comes with some diagnostic tools on top of the channels. Every reset, write or read can be tracked and requested at any moment. These variables are implemented as atomics to guarantee that there will be no concurrency errors. Channels themselves are, however, still not thread-safe.

SZD also builds some basic data structures on top of the channels. For example, the \textit{once log} used in WALs, the \textit{fragmented log} and its allocators used in LN and the \textit{circular log} used for metadata and L0. TropoDB only adds a little bit of data on top of each of these structures to fit its needs. Locking and thread synchronisation is still kept to a minimum. This is mainly done by allocating multiple channels for a structure. For example, using a channel solely for reads and a channel solely for appends. Solving the problem of concurrent access to QPairs. How the implementation of these structures functions will be described component by component in their respective sections.

A major problem and design flaw with the current implementation of the SZD C++ API is that the logic of zone sizes and zone capacities is hidden away. The entire ZNS device is exposed to the user as a set of zones with a zone size equal to the zone capacity. Internally SZD translates all I/O back to the correct locations. To do this SZD only support one homogeneous zone capacity that must be the same for all zones. So zone capacity may be different than the zone size, but must be the same everywhere. This was implemented by mistake as SZD was originally tested with the incorrect assumption that zone capacity would be constant. This is an implementation flaw of SZD, not a design flaw.  

\section{Benchmark Tooling}
\label{sec:benchtool}
TropoDB can reuse most benchmark tooling from db\_bench~\cite{cao2020characterizing}. This is enough for retrieving information such as put and get operation tail latency, but it provides no metrics that concern I/O itself. Therefore, custom designs are made for TropoDB and SZD. Every append operation logs the number of appends needed for the operation (updates that are bigger than ZASL require more than one), the bytes written by the append and the zones affected by the append. The same is done for all reads and resets. Retrieving this information is made thread-safe with atomics and allows getting insight into how much I/O has happened and where it has exactly happened. This adds a bit of extra overhead to each call. The information is lost on database shutdown as it is not stored. Latencies for WAL operations are measured around their respective calls in nanoseconds. For recovery of WALs this means measuring around the recovery procedure of one WAL and for WAL appends this means measuring around the call to the once log of WALs (only concerning the I/O functionality, not the database functionality).

\section{Implementations of On Data Structures for Storage}
Various storage structures have been designed for TropoDB. In this section we will describe how these structures are implemented. All of these structures make use of general structures that are implemented within SZD. Therefore, we will describe the implementation of the storage structure, by describing the used SZD components and what TropoDB components make use of these components. In particular, we will look at SZD once logs and how WALs make use of them, SZD circular logs and how both metadata and L0 SSTables make use of them, and we will look at SZD fragmented logs and how LN SSTables make use of them.

\subsection{Once Log and WALs}
The WAL implementation is built on top of the concept of once logs as described in \autoref{sec:wals}. The WAL itself only adds a bit of logic on top of the once log implemented in SZD. That is to say, while once logs are not restricted to the use of WALs, we will only describe how the once log functions in regard to the WAL.

Internally the once logs only maintain 1 SZD channel. This channel is in charge of all reads and resets. This is safe because these operations never happen concurrently. Once logs borrow an append channel from the WAL manager, this is done because only one WAL can be active for appends at the same time. It would be a waste of a QPair to allocate a QPair for appends for each possible WAL. Especially since the number of QPairs is limited. The once log internally has support for a queue up to the queue depth of the device. This queue is bound to the append channel. For each append, it determines if doing an append is safe (less than ZASL, not crossing borders). If it is, it verifies the number of outstanding requests. If this is more than or equal to what is allowed, it needs to wait. It repeatedly polls till one of its requests completes. The data from the finished requests is freed and the new request is scheduled. In the case an append is not saved, the suboptimal approach is taken of clearing the entire append queue. In this case, the implementation keeps polling till all requests are finished and then sends one request (the incompatible append) and sets the queue depth temporarily to 1. To ensure that I/O will always happen to the wp, the once log itself maintains a wp. After each (async) append it increments its wp, regardless of if it already happened. The associated zone of the virtual wp can then be sent to SPDK for the append. Internally it also maintains how much space is still left to append, which can be used by for example the WAL on top of the once log to determine if it is full and it is time to request a new WAL.

In case the WAL needs to be refreshed or the database closes, it is very important that all I/O requests finish. Therefore, we need an explicit sync call that polls till all requests are done. This delays database shutdown a little, but guarantees that all I/O to the WAL has been completed and is not just queued. Additionally, when a WAL is done, we finish the last zone of the WAL as we do not want to leak an active zone.

All WAL appends are added to a buffer that is a bit bigger than the original append. That is because an extra header needs to be prepended to the WAL. This header contains the following data in order: a sequence number (8 unsigned bytes) and the total size of the append in bytes (4 unsigned bytes). The total size is used to remove the padding that is added to fit WAL appends in the size constraints of pages on appends/reads. The key-value store does not need to know about the padding that flash internally requires. The sequence number is used for the recovery of unordered appends.

Recovering a once log itself is done by looping through all zones allocated to the WAL and verifying their zone heads. This is done from end to beginning to ensure that we always pick the last possible head (in case an external force resets a zone in the middle). The last zone that contains any data or the first zone, is set to be the head zone. The WAL recovery waits for the once log recovery to complete and then reads the data in steps of MDTS. It then does exactly the procedure as described in \autoref{sec:wals}. It reads all data in order and splits the data in sequence numbers with a pointer and data to which the pointer points. The sequence numbers are then sorted. Then the sorted sequence numbers are looped through and their associated data is applied to the memtable.

\subsection{Circular Logs}
Two components in TropoDB make use of circular logs: metadata and L0. Both of these circular logs can share a common lower-level structure and add some minor logic on top of it. Therefore, a generic circular log design is used from SZD. This circular log structure comes with a few implementation details that we will shortly describe and then we will look at some logic that we added on top of the TropoDB structures. \\
\textbf{Generic circular log}:\\
The circular log design makes use of multiple SZD channels. One channel is used for appends, one for resets and multiple for reads. There is a separate channel for appends and resets because those operations can be issued by different components/threads. For example, for L0 the flush thread appends and the L0 compaction thread resets. It does not make sense to support more than one append and reset channel as the circular log design is bound by one tail and one head inherently. Multiple reader threads are supported because reads are unrestricted by the circular log design; any read between tail and head is valid, allowing multiple threads to complete concurrently. Each reader has a unique \textit{number} that can only be used by one thread concurrently. This still guarantees that the channels are only used by one thread. It is up to the callee to ensure this rule is respected. Not putting explicit concurrency-safe mechanics in reads prevents unnecessary complications with locking when it is not required to do so.

Appends can be done either with ordinary heap/stack-allocated arrays or DMA-backed memory buffers (SZD buffers). The client never specifies an address, this is left entirely up to the log. It only appends. The client only specifies the data to write and the number of bytes to write. There is only support for synchronous I/O, which results that each append will only complete once the data is written in its entirety to storage. On a successful return, the LBA address of the first page is returned along with the number of pages that are appended. If the append crosses the last zone and needs to wrap around to the beginning of the log, the append is split in two, but the client will still only get the first LBA and the number of pages appended. Additionally, if the write is not properly aligned (not a round number of pages), a bigger buffer is made that is aligned and the data is copied to this buffer.  The client does not need to know about the internals of the log. After each append, the number of available pages is decremented and if upon an append the space available is less than the append size, it is immediately aborted beforehand. On a restart, the available space can easily be inferred from the zone heads in the log, just like the head and tail can. As a safeguard, the tail and head may never touch or else we can no longer distinguish the head from the tail (one zone is always lost because of this workaround, so a log requires at least three zones to function).

On resets, the client must specify the location and the number of pages. The location must always be equal to the LBA of the tail. This is a safeguard to prevent misuse of the circular log and is used for proper error handling. That is because if no address was given, the client would be unaware that incorrect data was deleted. The tail is incremented by the number of pages requested and deletes all zones it passes, again with wraparound. We refer to this operation as \textit{consume tail}. One important detail and inconvenience is that if the number of pages to reset is not a round number of zone capacities, the delete is not persistent. That is because, on recovery, the tail is reset to the zone head. Resets of this kind only prevent reads occurring on these pages, but should not be used for persistent deletes, unless somewhere else the ``unpersistent delete'' is maintained for recovery. Alternatively, this could have been stored in some metadata that can be given to the circular log on recovery, but this is not tried.   

Reads proceed in a few steps. They first check if the read is in between head and tail and fail (with wraparound) if it is not, preventing reads of unwritten data. If the check passed, the read is executed. The log uses the specified reader (from the reader number) to reads all pages either into either a specified heap/stack array or an SZD DMA-backed buffer (if future appends are expected this is more optimal). In case the read transcends the last zone, the read will wrap around to the beginning. Similarly, if the number of bytes requested is not a round number of pages, the last page will only be copied partially. The partial copy requires some further explanation. Ordinarily, the read can simply read to the client-provided buffer, but in case it is not aligned, a bigger buffer is needed or else we will have an overflow in the read operation (SPDK will read more bytes than fit in the client-provided buffer). TropoDB solves this by providing a small \textit{spill buffer} for each reader channel of the circular log. This buffer is exactly the size of one page. In case the read is unaligned, the last page to read will be written to the spill buffer. Then only the bytes requested by the read operation will be copied from the spill buffer back to the main buffer. Using unaligned reads will thus always require exactly 1 read to the spill buffer and 1 additional copy to the main buffer.

On a recovery, we retrieve the zone heads of all zones present in the circular log. From this, we can infer the head and tail, provided there is at least one zone free between the two or all zones are empty. If all zones are empty, the head and the tail are equal and set to the beginning of the log. Otherwise, the head is equal to the last zone that is not completely full or the zone next to the last full zone. The tail is in this case set to the first zone that contains any data. \textit{First} and \textit{last} is in this explanation a bit misleading. As first can be later than last. It is based on a series of sequential zones that contain data, potentially with a wraparound. Before the tail, there should be an empty zone and after the head, there should be an empty zone.  \\
\textbf{Circular log of metadata}:\\
The circular log of metadata hides away the circular log logic. It comes with a \textit{new manifest} operation, \textit{set current} and \textit{recover} operation. The operations can also be seen in  \autoref{fig:implmeta}, albeit in a simplified form. Note that the figure does not show error handling operations such and should only be used to get a general idea of the layers of abstraction. The new manifest operation does a number of things. First, it breaks the serialised index structure metadata into blocks in the size of pages. For each page it adds a small header, containing how much of the page is used, what fragment of the serialised data it is (header, center, trailer) and a CRC32 code. This is done entirely in memory. Then it is verified if there is enough space left in the log and if there is, the data is written. Internally in memory, it is maintained, where the last manifest begins (head before the append occurs) and how much data has up until that point in time been written to the log. Additionally, the metadata log keeps track of what data is old and needs to be deleted at some point, which is defined as a \textit{deleted range} containing the circular log tail and the number of pages to delete. The \textit{set current} operation deletes old data and tries to write new \textit{current} data. First, it is checked what data can be safely deleted. This is done by rounding the deleted range down (floor) to a round number of zones, remember that only deleting entire zones in the circular log is persistent. This rounded number of zones is then reset. The deleted range is then set to a new pair. The start of this range is set to an LBA specifying the new tail of the log. The number of pages in the deleted range is set to the remainder of pages after deletion (the pages that could not be deleted safely) and the \textit{last} number of manifest pages. The last number of manifest pages is added because we will change the last manifest to the new manifest. In the next iteration, we should be able to delete the last manifest. It is critical that we do not immediately consider the last manifest for deletion as this will lead to issues when the process quits during the creation of a new current. This will lead to the situation where the old current is gone, new current has not been written yet and, therefore, all data is lost. Then after altering the deleted range, we write the actual current. The current data is always exactly one page containing the magic bytes \textit{``CURRENT:''}, followed by some metadata. This metadata contains 8 bytes for the beginning of the new manifest data, 8 bytes for the number of pages of the new manifest data, 8 bytes for the beginning of the new deleted range and 8 bytes for the number of pages in the deleted range.

\begin{figure}[!ht]
    \hspace*{-0.075\textwidth} % < This is a hack?
    \raggedleft
    \subfloat[New manifest operation (no error handling)]{
        \begin{minipage}[c]{0.55\textwidth}
          \raggedleft
          \includesvg[width=1\linewidth]{graphs/newmanifest.svg}
        \end{minipage}}
    \subfloat[Set current operation (no error handling)]{
        \begin{minipage}[c]{0.55\textwidth}
          \raggedleft
          \includesvg[width=1\linewidth]{graphs/setcurrent.svg}
        \end{minipage}}
        \\
        \subfloat[Recover operation (no error handling)]{
        \begin{minipage}[c]{0.55\textwidth}
          \centering
          \hspace*{-0.75\textwidth} % < This is a hack?
          \includesvg[width=1\linewidth]{graphs/recoverlog.svg}
        \end{minipage}}
    \caption{ TropoDB implementation: Metadata log abstraction  }
    \label{fig:implmeta}
\end{figure}


On a recovery, we first recover the circular log itself. If this succeeds, we try to recover the current manifest by starting from head to tail. It is assumed that the current closest to the head contains the metadata of the most recent manifest. This is done, by moving page by page from head to tail. The current data is at most one page, so this is safe. For each of those pages, it verifies if it contains the magic bytes of the current and if the manifest pointers are in bounds of the log and valid. If they are, we can use this current. If not, we continue till the beginning. If no valid page is found, the database can not be recovered and we need to create a new one. However, if the data was valid, we use the current data to read the manifest. The manifest is read in one go from beginning to end. The CRC32 of each page in the manifest data is verified and the database is only recovered with success if they are all valid. At this point, the data is deserialised and set to the current index structure and the database is opened. \\
\textbf{Circular log of L0}:\\
The circular log of L0 adds minimal logic on top of the generic circular log. Most logic is related to the metadata, concurrent reads and resets. On an append, it first creates some metadata. It sets the LBA of the SSTable to the head of the circular log and requests the append to the circular log. The circular log then sets the number of pages written itself. This is the only change required. L0 adds support for concurrent reads with a \textit{read queue}. This read queue requires one lock. This queue is as long as there are readers in the fragmented log. After locking the queue it picks the first reader that is not claimed and sets it to the claimed state. Then it unlocks, reads with the claimed reader, locks again, sets its reader back to unclaimed and sets a signal that a reader has become available (to avoid spinlocks). If no reader is available, it waits till one is available. This is done with a signal and not with a spinlock. The read data is verified by checking the header, which contains the number of bytes it contains and the number of entries. If this is valid, an iterator is created, which either returns a value from the iterator or itself (a merge needs the iterator for example).

Resets are more involved because there are no partial resets of zones. TropoDB only allows full resets, not partial resets. This is by design as deletes that are not persistent, lead to leaks and all sorts of problems on restart. On a reset, all deleted L0 SSTables are sorted on recency. Then only if the LBA of the first SSTable aligns with the tail address of the circular log, do we consider deletes. First, we create a \textit{shadow tail}, which is used to get an idea of where the tail will end up. Then we walk through the rest of the deleted SSTables. In each step, we increment the shadow tail with the number of pages present in the deleted SSTable. We stop for the first entry where the shadow tail does not align with the LBA of the deleted SSTable (we can no longer safely delete). Then we divide the number of pages to delete by the zone capacity (floor) and only delete these zones. Then we create a \textit{mock sstable} as described in the design. This SSTable has the number of the last deleted SSTable (guaranteeing recency order is respected), an LBA equal to the tail and a page count equal to the difference between the shadow tail and the tail. All non-deleted SSTables are readded to the deleted SSTable list. Now provided that we save the new metadata properly to the metadata log, we have persistent deletes.

\subsection{Fragmented Logs of LN}
Fragmented logs are implemented in SZD and are used in TropoDB by SSTables from L1 up to LN. Fragmented logs internally support multiple writers and readers. This is done by making use of multiple SZD append channels and SZD reader channels. Each channel can only be used by one thread concurrently, but this is left up to the client. Each append or read requires specifying a \textit{reader number} or \textit{writer number}. All operations are done synchronously to the fragmented log; they are only completed once the data is persisted to storage. 

Appends can similarly to the earlier data structures (once logs and circular logs) be done with either stack/heap allocated memory or SZD-buffers. Then I/O proceeds in a number of operations. First, we check if the fragmented log is created with more than 1 writer. If it is, we need to lock, because we need to reserve zones from the zone allocator and zone allocators do not have support for concurrency. Zone allocators in turn do not support concurrency because this would cause issues with space reservation and internal schemes that might be used, say reserving zones sequentially. After locking, it is first verified if there even is space. This prevents the zone allocator from partially allocating and leaving a persistent leak (data is reserved, but there is an error halfway). Then zone regions are reserved and the lock is relieved. This is followed by appending to each zone in the reserved zone regions sequentially. Once a zone region is full, it continues onto the next zone region, splitting the data. After the appends are completed, it is checked if the last written zone is not full. If it is not, the zone is finished, preventing leaks of active zones. The zone regions along with the number of pages written are returned to the client.

Writers can also issue resets. Unlike circular logs, we do not create additional channels for resets. It is left up to the client to synchronise between writes and resets. Such an approach is possible because appends and resets are disjoint for fragmented logs. Appends do not rely on tails like for circular logs and resets do not care if data is written to zone regions or not. The fragmented log itself is unknowing of its own layout, apart from the free list, requiring minimal coordination.  This is used to great effect by LN. The L0 compaction thread always uses the first writer channel and the LN compaction thread the second. Only the LN compaction thread resets and it uses the same channel for both its writes and resets.

When we implemented the fragmented logs, we also tried out concurrent appends. In this design, each writer has multiple channels. It only has an effect if an append needs more than one zone, which can happen for large SSTables for example. It uses a form of striping in this case and requires no changes to reads or resets at all. This is possible because we already know what data needs to be written to each of the zones as the sizes are known beforehand. Data for zone 1 in a zone region is still always written to zone 1 of the zone region and data for zone 2 in a zone region is still always written to zone 2. The operation then works in a number of steps. The number of needed zones is divided by the number of writers. Each writer then gets this amount of zones to write. Then I/O proceeds in a number of rounds. In a round, each writer writes up to ZASL asynchronously. Then after all writers have issued their I/O, the writers are polled. Once this is done, the next round starts. If a writer no longer has I/O to write, it simply stops. This allows doing multiple appends in parallel for large appends in LN. Nevertheless, it uses a large number of channels and QPairs of which there are limited amounts (SPDK sets a limit on the number of QPairs). The tests for performance showed a minimal gain and concurrency efforts were shifted to L0 and WAL later on. Future work could still look into this approach for fragmented logs and look into more optimal designs. It is suspected that more could have been gained by not waiting for all I/O operations to succeed after each append round.

Reads are more simplistic. Reads do not check if the zones it reads are full or empty, they just read the zone regions requested up to the specified number of bytes and do some minor bound checking. Similar to circular logs a \textit{spill buffer} is used for unaligned reads. It issues one read to its read channel for each zone region specified. They rely on SZD channels supporting reads of arbitrary sizes.  

Fragmented logs are the only structure in SZD that require metadata to function properly. This serialisation is left entirely up to the zone allocator used. The zone allocator itself should return data that is enough to guarantee persistence. The only zone allocator implemented, the merging allocator, needs a bit of metadata. For each entry in the free list it encodes the zone number (fixed 8 bytes), the number of zones (fixed 8 bytes) and 1 byte to indicate if it is in use or not (free or claimed). Additionally, it stores the last allocated zone, this allows continuing zone allocation from the exact position before database shutdown after recovery. Lastly, it maintains the number of bytes it has serialised to prevent reading null bytes on recovery. 

LN SSTables add a little functionality on top of fragmented logs. Notably, SSTables only support a limited number of zone regions and save zone regions in a different format. In this format, zone regions are represented by the total number of used zone regions and for each individual zone region a number representing the starting LBA of the region and the number of pages used for the region (not in zones like fragmented logs expect). This is done because it allows using data similarly to L0. Reads and resets then translate it back to the original fragmented log format. Reads also add logic to support safe concurrent reads. Internally it contains a \textit{read queue}. This is the same read queue as used for circular L0 logs. In case an iterator is requested for an SSTable, the entire table is still read in its entirety and the resulting buffer in memory is sent to the iterator. 

\section{Implementations of Data Structures for Memory}
This thesis does not focus on in-memory structures. Nevertheless, an LSM-tree does require a few. In this section we will describe the in-memory structures that were used. In particular, we will look at the memtable implementation and a cache for SSTables. 

\subsection{Memtable}
\label{sec:memtable}
The memtable is a part of the key-value store that is not integral to this research. That is because it generally does not depend on the storage used. Therefore, we have decided to reuse an existing implementation with some alterations. TropoDB uses the default memtable implementation supported in RocksDB, an implementation that makes use of \textit{skiplists} (Jellyfish by Yeon et al. has a good explanation of how skiplists are used in RocksDB~\cite{yeon2020jellyfish}). Each change to a key-value pair is added to this skiplist along with a version number. This version number increments after each change and can guarantee that only the most recent change is used (in for example a merge later on).  

\subsection{SSTable Cache}
As a performance optimisation, LSM-trees can implement a SSTable cache. An SSTable cache supports caching a few SSTables in memory, such as in 4GB of DRAM. This prevents needing to read SSTables from storage for each individual read. Instead on a read, it is first checked if the SSTable is already present in cache. If it is, it is possible to search for the requested key within the cache, reducing the need to do expensive I/O. If the SSTable was not present, the SSTable is first read from storage. Then the SSTable is added to the cache and only then is the requested key searched in the SSTable. This can amortise a large number of gets, provided at least a few reads are issued to the same SSTables, but can lead to a few more in-memory operations for each read. LevelDB and RocksDB take such an approach, but do not allow disabling the cache explicitly. This is not a ZNS-specific optimisation and, therefore, not a goal of this research. Nevertheless, a minimal implementation is used in TropoDB, using most of the implementation of LevelDB. 


\section{Background Operations}
Most of the background operations are already described in the design and most of the other logic is not relevant for the storage research. We will, therefore, only shortly discuss the parts that are. Those are the policies to pick for flushes and compactions and the locking mechanics as these affect the underlying storage components.

Flushes are scheduled only when a memtable is big enough. However, it might be that there is not enough space left in L0. In this case, a compaction in L0 is scheduled (if there is not already) and the flush waits for the compaction to finish by waiting for a signal from the L0 compaction thread. This is also true for the concurrent design of WALs and L0.  Once the circular log in L0 reaches a certain number of SSTables, client-issued puts are also slowed down with explicit delay operations to prevent flooding L0. Compactions in L0 are scheduled if the number of SSTables in L0 reaches a certain amount or the size left in any L0 log is less than a configurable threshold. The second constraint is unique to TropoDB as L0 can not physically become bigger than its assigned zone region and must be set to a safe value below 100\% of the L0 log or else there might be out-of-space issues during a flush. Flush operations can be scheduled both by clients or flushes themselves (recursive scheduling). Compaction from L0 always picks the L0 SSTable present on the tail and picks a number of overlapping tables in L0. The maximum number of overlapping tables is limited to the maximum size of compactions, this prevents loading more SSTables in memory than fit. In case multiple L0 logs are used, the tail to pick is switched in round-robin fashion, but overlapping tables can be picked from both logs. All overlapping SSTables in L1 are picked and the size of L1 is not taken into account for the merge.

Compactions from L1 and up, make use of a score function. This score function divides for each level the number of bytes present in the level by the maximum allowed number of bytes in the level and multiplies it with a level modifier. The level with the highest score is considered for compaction, if the score is high enough. Each higher level has a bigger modifier, which allows prioritising compactions at lower levels. The SSTables considered for compaction in the level are done in round-robin fashion, which is made persistent by storing a \textit{compaction pointer} along with the rest of the metadata similar to LevelDB~\cite{LevelDB}. Compaction in these levels can also occur if the space left in the fragmented logs is too little. If this is the case and no level is considered for compaction, we have a lot of SSTables to delete, not enough space (or a persistent leak...). In this case, the compaction thread only tries to delete old data. Compactions on this level are scheduled by the L0 compaction thread or on startup of the database.

There still is one problem with compaction. Compaction from L0 to L1 and compaction from L1 to L2 might refer to the same SSTables. This is not safe as L0 to L1 invalidates data in L1 and L1 to L2 wants to move this invalidated data to the next level. Therefore, some locking is needed. Compactions from L0 to L1 create a global list of SSTables in L1 that are considered for compactions. Compactions from L1 to L2 do the same, but to a different global list. Before compacting, both of these threads first lock on a \textit{L1 compaction mutex}. Then they verify if there are overlaps in their L1 list with the other list. If there is, they can not continue. In that case, they wait till the other operation is finished, then reconsider their compaction and repeat the procedure. However, this is naive and leads to \textit{starvation} issues. It can lead to situations where either L0 to L1 gets a lot more compactions done than L1 to L2 or the other way around. Therefore, we need to add some \textit{fairness}. This is done with a very simple algorithm. Whenever one of the compaction threads has to wait for their compaction to finish, they set a personal counter, which is a token that they should be preferred later on. Then when they are to create their L1 compaction list, they also verify the list if the counterpart \textit{needs} a compaction and what SSTables they will use if they need a compaction (precaution). If this overlaps and their counterpart holds a token, they wait. Their counterpart has already waited once and will again if we continue. If not the operation in question can proceed and reset its token to 0, this operation does not need to be preferred next round. It already had its turn.

We also need to set some locks for client operations and background operations. Most locking can be prevented by separating the responsibility of tasks for each background thread.  However, it can not be avoided with the current design. On a get, we need to do a global lock to ensure that we get a snapshot of the current database state and that it remains alive during the entire get. It is not acceptable that in a snapshot the tables in L1 are from a different version than L0. Therefore, we lock on a get and pin the current version of the WAls, memtables and the entire index structure. All are referenced, which in combination with using ref counting for memory management, guarantees that a snapshot is read. For a put we need to globally lock as well, to reference the current WAL, find out if there is enough space left in the memtable and the WAL and schedule background I/O if necessary. This is all similar to LevelDB. However, background operations require a very different approach. 

There can be concurrent flushes, L0 compactions and LN compactions, but only one version head. Therefore, they work on different parts of the index structure, to guarantee consistent changes. At the end of their operation, they always lock globally, load the latest index structure, apply their changes to this index structure, write the new version to storage and only then unlock and complete. Flushes also lock globally temporarily to reset old WALs and dereference the current immutable memtable (so that it can be deleted when 0 refs are reached). L0 compaction threads do some more locking at the end for reset operations. The L0 compaction thread is the only thread that is allowed to add deleted SSTables for L0 and to remove deleted SSTables for L0. This change must always be applied to the most recent version, requiring locking. After finishing a compaction and adding the deleted SSTables during the compaction, it might be safe to also physically remove some SSTables. Therefore, after updating the current, it unlocks, removes all dereferenced deleted SSTables that it can, locks again globally and sets the deleted SSTables to the newly deleted SSTables for L0. This is safe as long as the process does not crash during the physical deletion.  LN compaction is the only component in charge of physical deletion on L1 to LN, but L0 compaction can still add deletes to LN. This is still safe as long as the LN compaction thread locks around adding or removing dead SSTables from LN or else there will be persistent leaks (if LN deletes dead SSTables by accident by shadowing), which is also done in TropoDB.

\section{Key-value Store Delete Operation}
Some key-value stores support and explicit delete operation apart from put and get operations. Both LevelDB and RocksDB support this. Since TropoDB builds on top of the interface of RocksDB, it was decided to support deletes as well. This operation can be done in the form of \textit{delete(keyname)}. There exist multiple approaches to implement this method. One valid strategy is to store the key along with an empty value and treat it like an ordinary key-value pair. When on a get, the value retrieved is empty, it is assumed that the key-value pair has been deleted. An issue with this approach is that for some use cases an empty value can also be a valid value. For example, think about a simple use case where all middle names of clients are stored. Some clients might not have a middle name, and for clients of the key-value store it might make sense (not every client is the same) to store the middle name as an empty value in such cases. When empty values are treated as deletes, this will lead to unexpected behaviour. 

Another approach is to make use of \textit{tags}. This is done in LevelDB, RocksDB and TropoDB. We will explain the approach taken in TropoDB. In TropoDB each key-value pair update can be stored with the\textit{insert} tag or the \textit{delete} tag. Inserts are persisted to storage as the triple \textit{(insert tag, key, value)} and deletes are stored as the tuple \textit{(delete tag, key)}. On a get, the tag is first verified. If the tag is an insert, the data is assumed to be valid and the value is retrieved. If the tag is a delete, the data is assumed to be deleted and the client is notified that the key is not available.

An optimisation that is done in both LevelDB, RocksDB and TropoDB is to prevent writing deletes to higher LSM-tree levels needlessly. When a memtable is the only datastructure containing the deleted key, there is no need to write it to storage. We already know that all reads that request this key, will return that the key is not present. Similarly, if L0 is the highest LSM-tree level that contains the deleted key, there is no need to move the deleted key to L1. This is true for all levels. However, if a later level does contain the key, it is absolutely necessary to push the deleted key upwards and the delete can not be removed under any circumstances (yet). Reads always return on the first valid occurence of the key. Therefore, if a key is stored as a deleted pair in the memtable, it does not matter if the data is stored as a live key in, for example, L0. As long as there is always a dead entry of the key in between the top level and the live entry of the key. The dead key must slowly move down till it reaches the level that contains the key as a live entry. At this point the two will be merged and the key will be dropped. It is no longer necessary and can safely be deleted.  

This optimisation can be implemented as follows. On each flush or compaction, all delete keys are fist checked before they are added to their respective SSTable. If there is no SSTable in any higher level that contains this key, the key is simply dropped and not added to the SSTable. If it is, it is added to the SSTable as a deleted entry. Additionally, if the next key returned during the merge returns the same key and this duplicate key is older, this key is dropped. The duplicate key refers to the key before it was deleted. This can occur since during a merge, multiple SSTables can contain the same key. Normally, we would simply pick the most recent key-value pair, but we need to treat deletes differently. Note that if the workload is delete-heavy and a SSTable only contains deletes, this can lead to flushes/compactions that write no SSTables at all, as all key-value pairs can be dropped.


\section{Summary}
In this chapter we have examined the implementation and evaluation tooling of TropoDB, answering RQ3. We have looked at the software stack used in TropoDB. This software stack exists out of SPDK, a C-interface on top of SPDK known as SZD and TropoDB itself built within the RocksDB ecosystem. SZD adds abstractions for ZNS storage and only support appends, not writes. It also comes with its own benchmarking tooling, which can aid in understanding the system. For example, by measuring the distribution of zone resets.

Then, we looked at implementations for the storage components designed in \autoref{sec:design}. We explained how WALs are implemented on top of SZD's once logs. In particular, we explained how once logs make effective use of queue pairs, perform asynchronous I/O, buffer appends, how padding is used, what metadata is stored in once logs and how to minimise I/O on recovery. We also looked at SZD circular log implementation and how both the circular log of L0 and the metadata log of the index structure are built on top of this abstraction. For the generic log, we describe how queue pairs are used for the circular log, how buffering is used and how resets are performed. For the metadata log we describe the abstraction we built on top of the log and how the design of the metadata log described earlier is realised. For the circular log in L0, we described concurrent reads and a more advanced reset procedure that can also do partial deletes (instead of deleting entire zones). We also take a look at the fragmented log implementation and how queue pairs and buffering are used in them. Additionally, we take a look at how to implement fragmented logs to allow for concurrent writers, readers and resetters. 

We continued by looking at the implementations of in-memory components in TropoDB. The memtable of TropoDB is implemented with RocksDB's skiplist. We have also described how an SSTable cache is used to cache SSTables in memory to reduce the cost of client-issued reads.

We did not limit ourselves to just structures. We also took a look at the implementations for background operations. We described the flush and compaction process. We also showcased a contention issue in the communication between the L0 compaction thread and the LN compaction thread. To solve this issue, we came with a communication procedure, that reduces both starvation and fairness issues when both background operations contest for the same resources. We also described the locking implementation for both background operations and client operations.

Lastly, we took a look at the delete operation in TropoDB, which is implemented by tagging an update as a delete. Deletes are only flushed or compacted to the next level if necessary, else they are dropped.

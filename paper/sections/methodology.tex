\chapter{Analysis/Exploration of the Key-value Design Space}
\label{sec:approach}
In this section we answer RQ1: \textit{``What are the key-value store integration choices available on top of ZNS SSDs?"}. We will answer this question in a couple of steps. First, we will state the requirements needed by the thesis concerning the integration choice. This should limit the set of integration choices to the set of key-value stores that are able to achieve our goal. We will continue by investigating in-depth how ZNS devices and LSM-tree key-value stores can coordinate and what integration choices can achieve such a feat. This should also help in defining the problem space. Then we will investigate and look at the current solutions for key-value stores that already exist. The existing solutions will also function as the alternatives to evaluate against in \autoref{sec:experiments}. We will finish by stating all of the integration choices that we have considered for this thesis and what integration choice we have picked in the end. After having picked an integration choice, we will describe the design on top of this choice in \autoref{sec:design} and the implementation of the design in \autoref{sec:implementation}. 

\section{Requirements of the Key-Value Store Integration Choice}
The goal of this project is to measure the effects that the unique design properties of NVMe ZNS devices can give for the performance of LSM-tree-based key-value stores. For this the ZNS device and LSM-tree need to be able to collaborate. To achieve our goal we are only interested in the storage aspects of the LSM-tree, not in other parts of the key-value store. Though important for any practical key-value store implementation, in this thesis we do not focus on optimisations for memtable implementations, compression support or any other in-memory LSM-tree component, due to time constraints. An approach were we only focus on the storage aspects is also necessary, to accurately measure the effects of our approach. It should be possible to compare our ZNS approach with a default implementation not using this approach; to test the storage effect ceteris paribus. To be able to compare two approaches, it is required to support two databases (configurations) and a tool to measure the effect. To accurately match the effects of co-optimising GC, it is also required to take an approach that allows integrating the GC of ZNS with the GC of the key-value store. We thus define the following set of \textbf{requirements (REQ)}:
\begin{itemize}
    \item \textbf{REQ1}: \textit{Support for a common benchmarking tool}:\\
    We need a benchmarking tool that is able to benchmark both our solutions and the key-value stores that we intend to evaluate against. This requires a tool that is usable by both.
    \item \textbf{REQ2}: \textit{Support co-optimising GC}:\\
    A goal in this thesis is to implement co-optimsising GC. Therefore, it is imperative to pick an integration choice that supports maximum control over garbage collection and allows integrating garbage collection of both the key-value store and ZNS.
    \item \textbf{REQ3 (soft requirement)}: \textit{Limit the amount of work needed for in-memory components}:\\
    This is not a hard requirement, but something to prefer. We should not focus on in-memory components and limit our design as much as possible to storage solutions only. Therefore, we should pick an integration choice that adheres to REQ1 and REQ2 and is favorable for REQ3. 
\end{itemize}


\section{Key-value Design Problem Space}
 The goal in this experiment is to make a key-value store where the garbage collector of the key-value store and the SSD properly coordinate. This requires making a design that allows such coordination (REQ2). A design should either allow the GC of the key-value store and SSD to properly communicate or the two should integrate with each other as one garbage collector. This communication is generally hindered by one concept, the \textit{semantic gap}. A concept we have explained in \autoref{sec:backgroundsemantic}.  In short, the semantic gap means that two different systems are not properly able to communicate as they do not fully understand this other. Additionally, we will now identify another problem that can occur because of the semantic gap. The semantic gap denies making specialised designs for storage components in applications such as key-value stores. A few components require certain performance characteristics, for example, WALs need excellent write performance, but read performance less so.
 
 We now know what the problem in the problem space is, but there are multiple approach to solve the issue.
 There are two general approaches to improve the communication: add support for possible communication or remove layers. The first problem is problematic as it requires complex layers that have support for all applications that make use of it and is generally unfeasible. The second approach, requires a vertical integration of the layer into the application itself. This can improve garbage collection at the cost of development time and maintainability. It is a trade-off.  
 
 With key-value store designs we identify a number of layers: the FTL and in some cases file systems. Using ZNS removes the FTL as a layer, but it is not always the only the one between the key-value store and the SSD. Key-value stores typically also make use of a file system. Using a file system in between the store and storage, makes communicating garbage collection problematic. File systems do not come with explicit ZNS commands by default or specialised data structures for WALs. File systems can define for themselves when to erase data and support generic files. It is possible to create a file system specific for a certain domain. ZenFS, for example, is built as a plugin for the key-value store RocksDB~\cite{bjorling2021zns}. However, ideally for this research we want to have a more informed communication between the store and storage. For TropoDB we want to remove parts of the file system as well where applicable to get a real collaborative-GC in which the key-value store can direct control the GC of the SSD. This should address REQ2 in an optimal manner. \textit{We thus propose a design without any semantic gaps}. 

\section{Current Solutions in the Problem Space}
It is important to test against a key-value store that is already up to standard to make a fair comparison. It is not possible to test the effect of a collaborative-GC without also testing a solution without such an approach. Ideally, we should also be able to test various solutions with more semantic gaps. This would allow, measuring the difference that each gap can bring, rather than just looking at one gap. Therefore, we decided to test against the state-of-the-art that is still able to work on top of ZNS SSDs. This is possible with \textit{RocksDB} (see \autoref{sec:rocks}). RocksDB is optimised for ordinary SSDs, but has no direct knowledge of ZNS. Nevertheless, it does not need to as it makes use of file systems to do the actual I/O~\cite{bjorling2021zns}. RocksDB can thus be used with ZNS-aware file systems.

File systems that are, as of May 2022, able to support ZNS include \textit{ZoneFS}~\cite{le2020zonefs} and \textit{F2FS}~\cite{tehrany2022understanding}. F2FS is a file system that is optimised for flash~\cite{lee2015f2fs} and already used frequently for conventional SSDs and can, therefore, be seen as the \textbf{state-of-the-practice} for RocksDB. It requires no further action for RocksDB to make use of this file system configuration, but this configuration is not optimised for ZNS devices and has a semantic gap in the file system level. This solution can thus suffice as the baseline performance for this project: a key-value store using a generic file system on top of ZNS. 

However, RocksDB can also make use of \textit{plugins} to circumvent relying on generic file systems. A plugin allows setting up a custom file system API and can thus be used to create a domain-specific file systems. This allows optimising a file system for key-value stores and reducing the semantic gap. One of such approaches is \textit{ZenFS}~\cite{bjorling2021zns}. This is also a file system optimised specifically for ZNS storage. This can thus be used as the \textbf{state-of-the-art} for benchmarks. As both file systems make use of the same key-value store, it also allows for a more proper evaluation. The last advantage of RocksDB is that it already comes with a benchmarking tool, \textit{db\_bench}~\cite{cao2020characterizing}. This tool ensures that there already is a well-defined benchmarking tool that can be used for measuring I/O characteristics. 

\section{Key-value Store Design Integration Choices}
The next decision to make is how to built a key-value store on top of ZNS with SPDK and design/implement TropoDB. We wanted to make use of the existing benchmarking tooling to allow for proper comparison against the alternatives. This requires a tool that can benchmark both the RocksDB solutions and the TropoDB solution. Therefore, the following four solutions were considered, respectively in order of effect and required work:
\begin{enumerate}
    \item[\textbf{1.}] Create another domain-specific file system for RocksDB with a plugin. This allows leveraging all RocksDB functionalities, but it does mean that there is limit to the effect GC can have. It is not possible to properly differentiate between different LSM-tree data structures as they are all stored in generic files. It only allows specifying file systems hints indicating their \textit{temperature}. It would also not be possible to truly combine the GC of the key-value store and the file system. In the end with this solution, there will always be a generic file and a semantic gap will remain.  
    \item[\textbf{2.}] Alter RocksDB to make use of specialised storage data structures instead of files. This would allow making custom and optimised structures for each individual database component. This would essentially nullify the semantic gap. At the same time, RocksDB was made to make use of a file system API. RocksDB comes with many options, meaning these all need to be implemented or their support needs to be dropped. This approach requires an intrinsic knowledge of each RocksDB operation and requires reworking many parts of the database from the ground up, even if they are not relevant for the research.
    \item[\textbf{3.}] Built a new key-value store on top of the RocksDB interface. RocksDB comes with a basic interface (defined in the C++ header file \textit{db.h}). This allows leveraging the entire RocksDB ecosystem. Whenever, a function from RocksDB needs to be used, it can then be included. At the same time it gives full control over the SSD removing the gap. This is similar to altering RocksDB, apart from its approach. This approach is bottom-up instead of top-down. This allows beginning with storage aspects first and only adding what is necessary. As RocksDB is a continuation of LevelDB, it also allows using LevelDB components instead of RocksDB components in some cases. LevelDB components are generally simpler in design and come with less options. This makes them easier to add and modify for ZNS. 
    \item[\textbf{4.}] Built an entirely new key-value store and modify \textit{db\_bench} or another benchmarking tool to support the key-value store. This fundamentally gives the most control. However, it does require reworking each part from scratch. This also includes the non-storage parts of the key-value store and the benchmark tooling. Such an approach would make it non-trivial to compare the solution against the alternatives and makes it easy to make an \textit{apples to pear comparison} instead, which is generally not what we want.
\end{enumerate}
In the end, approach (\textbf{3}) is taken for TropoDB, creating a new key-value store on top of the RocksDB interface with a bottom-up approach. This gives a lot of control and reduces the semantic gap (REQ2), while still retaining access to the RocksDB ecosystem(REQ1,REQ3), which is already researched by a large selection of academia and industry alike. Creating a file system, approach (\textbf{1}), is rejected as this would be very similar to ZenFS and not bring anything novel to the table. It would also complicate collaborative-GC as a semantic gap would remain (Fails REQ2). Approach (\textbf{2}), modifying RocksDB top-down, is rejected as well as it would require investigating all RocksDB I/O paths for each step. This would make it harder to test individual I/O structures and test the effect they would have on the key-value store. We still think that it would be a worthwhile approach to test out in future work. Approach (\textbf{4}), creating an entirely new key-value store, is rejected as it would make it hard to compare the key-value stores against alternatives (Fails REQ1). There is no other key-value store made purely for ZNS; so there would not be a clear baseline anymore. This in combination with the need to recreate everything from scratch, including the memory parts of the key-value store, would make it non-trivial to distinguish storage from other parts of the store and would in all likelihood lead to measuring interaction effects (Fails REQ3).  

The resulting approach for TropoDB will lead to a minimal stack. As we also make use of SPDK, the kernel is also removed from the stack of layers, providing maximum control over the storage used. The amount of layers TropoDB has compared to the alternatives in the problem space can be seen in \autoref{fig:layers}. RocksDB with NVMe SSD has 3 storage layers and an OS layer, F2FS with ZNS removes one storage layer and ZenFS removes one layer and integrates another (domain-specific file system). TropoDB on the other hand removes all layers.

\begin{figure}[h]
\centering
\begin{minipage}{1.0\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/layered_world.svg}
\end{minipage}%
\caption{ The semantic gap between key-value stores and flash storage }
\label{fig:layers}
\end{figure}

\subsection{RocksDB for ZNS Design Choices}
Following last section it has been decided to  built TropoDB bottom-up on top of the RocksDB API This automatically gives full access to \textit{db\_bench} as well. We will now describe what else we need to do to implement the chosen integration choice and how we intend to achieve this.

As RocksDB is written in and has support for C++17~\cite{iso2017iso}, it has been decided to make use C++17 as well. A part of the lower API stack of TropoDB still makes use of C code. TropoDB is thus a mix of C++ and C. By default RocksDB has no access to SPDK and especially not to ZNS. To allow calling raw ZNS SPDK functions, the application is linked to SPDK. To built the key-value store, logic has been added bottom-up. When a feature needs to be added, it is first investigated if this logic can be reused from RocksDB and LevelDB. Then if this feature makes use of storage, it is investigated how this structure can be optimised for ZNS storage and what needs to be altered to the already existing LevelDB/RocksDB logic. In many cases this requires creating new I/O structures as no solutions exist yet for ZNS. 
Not all LevelDB/RocksDB functionalities have been implemented, as they are not all necessary for this research. Instead, we only implement the minimum needed to get a functional key-value store. This means that there is logic for \textit{put}, \textit{get} and \textit{delete} operations, but no support for \textit{scan} operations or \textit{column families}. Similarly, there is no support for non-ZNS specific optimisations. Such as database filters or memtable-only approaches (in-memory key-value store). The key-value store was in the end built in three steps: implementing the basic key-value store, allowing the data base to be used with benchmark tooling such as db\_bench and optimising the individual key-value store components based on large I/O benchmarks. The resulting key-value store is known as \textit{TropoDB}. In later sections, we will take a look at the individual components, how they are designed and how they are implemented, and then we will take a look at how the resulting key-value store behaves as a whole and how it can be configured.

\subsection{Splitting Key-value Store Logic from Generic ZNS Structures}
During development it was noticed that not all structures and storage logic that was created up until that point in time were ZNS specific. In fact, many parts could be reused by other ZNS projects. Therefore, it was decided to split the project. This resulted in a separate library, apart from TropoDB, that could be linked to statically and dynamically by the database, \textit{SimpleZNSDevice} (SZD). This contains generic logic that can also be reused for other projects such as file systems. It comes with the additional benefit, that it allows changing the backend of the database. This would allow optimising the storage aspects at least partially independent of the key-value store at the cost of introducing a new layer.

\subsection{How to Benchmark the Solution and the Effects of GC?}
\label{sec:methodbenchgc}
Since, we are benchmarking key-value stores on SSDs specifically and want to also verify write amplification among others, we have to make sure that we benchmark correctly (REQ1). Just executing db\_bench, will not be enough because, db\_bench, only measures the top-level effects of the key-value store. It measures latency of I/O operations, compaction cost in latency, write amplification of the LSM-tree itself and throughput. However, it does not test raw effects of I/O. This is necessary, as we are not testing just one \textit{layer}, we are testing the entire stack; from key-value store to storage. Effects on storage need to be measured as well. Therefore, in order to properly benchmark, some additional custom tooling needs to be added on top of db\_bench to measure the storage effects. All possible effects GC need to be measured. This includes latency of puts, tail latency of puts, number of writes to storage, number of reads from storage and number of erasures issued to storage. To benchmark storage correctly, guidelines in \textit{the unwritten contract of SSDs}~\cite{he2017unwritten} and \textit{Toward a better understanding and evaluation of tree structures on flash ssds}~\cite{didona2020toward} are followed where applicable. However, guidelines concerning garbage collection and a steady state are harder to follow. That is mainly because the solutions used have inherently different designs that are not completely compatible with each other. For example, TropoDB will claim the entire SSD and clean it before it creates a database. Therefore, it is not possible to bring the SSD to a steady state with other operations than database operations. Instead, it is only possible to bring the database to a steady state and reuse this database for later tests. To make such comparisons fair with designs using file systems, the file systems should test with a clean slate as well and only reach the steady state by using database operations. In general, all candidates for the benchmarks should start out in the same state.  In the end we identify the following \textbf{measurable GC effects} that need to be measured by all solutions:
\begin{itemize}
    \item \textbf{Measuring write amplification (WA) of the entire key-value store}:\\ 
    We define total write amplification as the total amount data written to storage during a benchmark divided by the size of all key-value pairs written to storage (number of put operations and raw size of each pair). Thus all bytes written on storage divided by all data put by clients. We use this definition as the solutions live in a different design space. Both the garbage collection process of the key-value store (application-level) and the device (device-gc) can cause write amplification. TropoDB will not cause write amplification on the device level (or an amplification that approximates 1) as the key-value store has full control over the storage. All amplification is thus issued by the application and can not be logically separated.  Similarly, the RocksDB solutions only record write amplification at the application level and using this metric would thus be wrong. To properly compare the two solutions, it is necessary to incorporate all write amplification. 
    \item \textbf{Measuring normalised reset count}:\\
    We define the normalised reset count as the total amount of zones reset during a benchmark divided by the size of all key-value pairs written to storage (number of put operations and raw size of each pair. Resets are an explicit garbage collection operation. There purpose revolves around it. The garbage collection process that has control over the storage, resets the zones. This should be measured.
    \item \textbf{Tail latency of client-issued key-value store operations}:\\
    Garbage collection processes do not just write and move data, they also claim available hardware resources. For example, they use part of the available bandwidth (and available parallelism) of the device. This reduces the amount of resources that are available for other resources. Additionally, some garbage collection operations are time-sensitive in the sense that clients have to wait for them to finish. For example, if there is no more space left, the client has to wait for space to become available. It is hypothesised that during expensive garbage collection processes, this will lead to fluctuations in latency for client-issued operations as clients might have fewer resources or are stalled. This metric will also contain various other latencies (latencies of in-memory operations and more) and means little on its own. However, when we compare comparable key-value store solutions, it should possible to recognise garbage collection effects (hypothesised).  
\end{itemize}

\subsection{Summary}
In short, we have answered RQ1 by looking at various key-value store integration choices that are available on top of ZNS SSDs. In the end we picked an integration choice where we built TropoDB on top of RocksDB with a bottom-up approach. In the bottom-up approach we start with the database interface from RocksDB, but do not start with any additional RocksDB logic by default. Instead, we add functionalities on top as needed. Functionalities are taken from RocksDB, LevelDB or created ourselves. In particular, all storage components will be crafted by hand. Storage will be interfaced with the help of SPDK. The solution will be benchmarked with RocksDB's benchmarking tool, db\_bench. TropoDB will be compared against the state-of-the-art, RocksDB + ZenFS, and the state-of-the-practice, RocksDB + F2FS.

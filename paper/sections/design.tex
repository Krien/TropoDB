\chapter{Design of TropoDB}
\label{sec:design}
In this section we will answer RQ2: \textit{``What unique optimisation opportunities does a ZNS device offer for LSM-tree based key-value stores?"}.
We are thus mainly interested in the unique design properties that ZNS can give for LSM-tree key-value stores. For example, co-optimising garbage collection for key-value stores by reducing the semantic gap. Most design decisions that are thus discussed in this paper are only related to these issues and other optimisation efforts are left to future work or only discussed shortly. In this section, the major design decisions of TropoDB are discussed. We will first give an overview of what we need to design, along with their requirements. Then we will describe how each individual LSM-tree component is designed in TropoDB, followed by how all background operations are designed and we will finish by describing how the individual components interact with each other. Components that make use of storage will be described in more detail.

\section{Overview and Requirements}
Before we come up with a design, we need to get an understanding of what we need to design. To achieve this we look back at the LSM-tree as defined in \autoref{sec:lsmtree}. An LSM-tree consists of a couple of structures that need to be physically stored. Those structures are \textit{Write-ahead logs} (WALs), \textit{Sorted String Tables} (SSTables) for L0 to LN and \textit{metadata for the index structure}. We need to create a different data structure for each different component. 

We also define a number of background operations that we need to take care of. These background operations are vital as they are in charge of the garbage collection. For key-value stores these include \textit{flushes} and \textit{compactions}. Additionally, we define \textit{deletion} operations. Operations that are in charge of removing old data. This is necessary because the key-value is in charge of physical deletion according to the design approach we have taken for TropoDB in \autoref{sec:approach}.

We setup a few requirements that the designs of each the components and background operations should adhere to. These are based on features that make ZNS stand out. ZNS stands out because of its support for append operations, explicit parallelism with active zones and appends and the ability to create a custom garbage collector. To ensure that all of these properties are tested, so that we can answer the research question (RQ2), we want to force the design to use these unique properties. Therefore, we define the following \textbf{Design Requirements (DR)}:
\begin{itemize}
    \item \textbf{DR1}: \textit{The design should be append-only}:\\
    The design is not allowed to make use of the traditional \textit{write} command to perform writes. It is only allowed to make use of the ZNS I/O operation \textit{appends}. 
    \item \textbf{DR2}: \textit{Reduce the number of overwrites and erasures}:\\
    ZNS gives full control over the physical locations of data. Every write and reset has to be issued by the client. This is also true for garbage collection as this is in the hands of the client. This should allow for designs that minimise overwrites and resets of data (in-place I/O). For example, by using logs and hot and cold separation. Every design that uses overwrites in any way, needs justifications.
    \item \textbf{DR3}: \textit{Make effective use of the available parallelism}:\\
    ZNS devices allow doing I/O operations in parallel. For example, multiple appends to the same zone and using multiple zones concurrently. Not using the available performance would waste what ZNS can give. Where possible, we should try to use what is given to us by the device.
    \item \textbf{DR4}: \textit{Limit interference of garbage collection to aid latency stability}:\\
    With the co-optimised garbage collection approach taken in this research it is possible to decide when to do garbage collection operations for both the LSM-tree and storage. This can lead to better control over latency. Where possible, the design should take this into account and stabilise latency effects. For example, by not postponing deletes to the last moment.
\end{itemize}

For each of these components and background operations we will also come with a structured analysis and description. These details should help to get a better understanding of what each structure exactly is, how it is currently stored and how we can improve this with ZNS. For each component we will at least describe:
\begin{itemize}
    \item how the component functions and is used (access patterns)
    \item how the component is traditionally stored in a file system
    \item how the component is stored in TropoDB
    \item how the component is cleaned (garbage collection)
\end{itemize}
Additionally, we will explicitly mention how each component tries to adhere to the design requirements and come with some disadvantages for each proposed design as well. We will also properly identify what the semantic gap for each LSM-tree component is, this is necessary to state as this is the main problem that TropoDB's approach can solve. We should design our components around that principle. For example, if we want to reduce the effects of the semantic gap, we need to make components that are aware of the temperature of their data and if we want to reduce the effects of garbage collection, we should use proper scheduling for cleaning operations. 

Throughout all of our explanations, we will refer to RocksDB and its precursor LevelDB as the traditional approaches. We will also refer to the state-of-the-art file system ZenFS for some components, to explain how using ZenFS as a file system can alleviate the effects of a stated problem. Justifications for many design decisions are supported by experiments/evaluations described in \autoref{sec:experiments}. This includes both advantages and disadvantages of the designs chosen.


\section{TropoDB as a Whole}
TropoDB is a collection of individual components that together can be used to create an LSM-tree. We will described each of the individual components independently in later sections. In this section, we will describe how the components together make up the LSM-tree and what configuration designs should be considered. It thus gives an overview of the design philosophy of TropoDB as a whole. Each individual component in TropoDB reserves a contiguous number of zones. This results in a design where the entire SSD is split between components and there is no additional space left. Such a result can be seen in \autoref{fig:tropodesign}. In such an approach, it must be approximated or known beforehand how much data each component is expected to use. As this can differ on the workload used for an application or the storage device used, it is to be made configurable, but only on initialisation. After initialisation, it should be trivial to find the location of each component on storage.  

Clients of such a database must make a trade-off between the components. Reserving many zones for metadata might be wasteful, but too few zones will create problems for large key sizes or devices with small zones. Similarly, assigning to few zones for WALs might result in faster wear levelling on a few zones, but will leave more space for SSTables. Further on, each component requires I/O and the I/O each component uses reduces the amount available for others. In the case of active zones, it can even cause too little zones to be available. Another trade-off is, therefore, to prioritise what components should get the highest level of concurrency (addressing DR3). In short, TropoDB removes most abstractions and physically stores the key-value store components to exact locations. Users of TropoDB are thus left to make choices based on the available hardware, not on higher abstractions. 

 \begin{figure}[h]
\centering
\begin{minipage}{0.85\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/TropoDB_design.svg}
\end{minipage}%
\caption{ TropoDB design: Dividing an SSD among LSM-tree components }
\label{fig:tropodesign}
\end{figure}


\section{Write-ahead Logs (WALs)}
\textit{Write-ahead logs} are structures necessary to guarantee persistence of the database. In particular, they are the persistent part of memtables. They do this by functioning as append-only logs and recording all of the changes applied to the memtable. Examples of such changes are: insertions, modifications and deletions of key-value pairs. A put operations is only considered complete when the put data is written to both the memtable and the WAL. The result is that WALs are very hot and one of the most I/O heavy data structures of the key-value store. This makes them an important target for optimisations (it is on the common path). This is also true for garbage collection and latency stability issues. When looking at how WALs are used in \autoref{sec:background_lsm_writes}, an important detail to notice is that for LSM-tree WALs, only the client appends to the structure. Further on, during the WALs lifetime it is generally never read. It would thus be accurate to say that the WAL is write-only, except on startup when it is only read. Therefore, when designing a WAL, we only have to take care of two situations: a situation with only writers and a situation with only readers. In addition, writes generally need to be fast as otherwise clients have to wait longer before their put requests are finished, while reads may be slower as they are only done once at the start or during a recovery (common and uncommon I/O path). 

\subsection{File System Approach for Designing WALs}
To design WALs for ZNS, we first take a look at how WALs are currently implemented with a file system approach. We refer to the WAL implementation as used by LevelDB and RocksDB. In both implementations, there can be numerous WALs on storage. The maximum amount of WALs is configurable and each WAL has its own file, that can easily be identified with a filename like ``.LOG''~\cite{RocksDB}. However, only one WAL is active for writes at each point in time. Old WALs can be removed by a background thread when they are no longer needed, which happens when the data is also present in an SSTable and is, therefore, no longer necessary. We can at this point state that the data is backed to storage twice. Data only needs to be present on one layer of the LSM-tree.

By giving WALs unique files, they are already clearly separated from the rest of the data. In fact, individual WALs are also separated as they are stored in different files. Both key-value stores identify these files as \textit{append-only files}. The problem with this approach is that the underlying file is still just a general file and not optimised for its use case, regardless of how it is used. Similarly, even though WALs are identified as unique files, there is no control over the physical location of the files. Lastly, when a WAL file is removed, it might be removed at a later point in time, providing little control over the actual removal process.

Using a domain-specific file system such as ZenFS can alleviate the issue. ZenFS tries to put different files in different zones~\cite{bjorling2021zns}, which should allow for better hot and cold separation. Yet, they still have to resort to using only one type of file, which is optimised for the general use case. ZenFS can not come with unique removal schemes for WALs or unique implementations for WALs because it needs to generalise. When it receives a file, it does not know whether it is a WAL or another file. ZenFS is a domain-specific file system, so this can be solved by adding semantics between the file system and the key-value store to indicate the file type (e.g. extended attributes).

Another property that LevelDB and RocksDB support are various levels of persistency. For many applications, persistency guarantees are less strict than performance guarantees. Trading persistency for performance can be a good trade-off in such situations. By default, persistency guarantees of both databases are not that strict.  The resulting persistency model makes use of buffering and asynchronous I/O. Writes to the WAL are first buffered to a small application buffer. Only once the buffer is filled or the application is shutdown, will it be written to the file system. To get more reliability, this buffer must be disabled manually by the client. However, this might not be enough. By default, writes are only issued to the underlying file system. However, the file system can have a buffer of itself and is not forced to flush the contents of its buffers to storage immediately. Further on, depending on the storage engine used, it is also not guaranteed that all I/O sent to the SSD will be immediately written to flash storage. This can also lead to data loss on power loss of the machine, but data should generally still be written to storage on a graceful shutdown. More reliability can be ensured by forcing the databases to \textit{sync}, which forces the underlying file systems to flush their contents and use direct I/O (e.g. fsync on GNU/Linux~\cite{bornholt2016specifying}).

\subsection{TropoDB Approach for Designing WALs}
\label{sec:wals}
In TropoDB we take an entirely different approach. We reserve a part of the zones to be used for WALs exclusively and each individual WAL gets its own distinct contiguous subset of zones. This separates hot and cold data by creating one cluster that spreads the temperature of WALs (addressing DR2). The WAL implementation is optimised for its use case only and uses a highly specialised data structure that we call a \textit{once log}. Thus we specialise, instead of generalise. It is called a once log because its use case is to be used only once and then thrown away. The log is internally made up of a contiguous set of zones, where there are three possible types of operations: appends (DR1), random reads and complete erasures (throwing the log away). As there are only appends, the log can only grow in one direction until it is full. At that point in time, we have to claim a fresh WAL, or else we will lose data. \\
\textbf{Consistency models}\\
Similar to how RocksDB supports multiple consistency levels, we support multiple consistency levels. In TropoDBs design it is possible to configure a small application buffer to stall issuing writes to ZNS. When the buffer is made larger, more data can be lost, but I/O speed can increase. We also support the earlier mentioned \textit{sync} option. When the option sync is disabled it is not necessary to immediately flush the content of a put to storage (direct I/O), as long as it is queued to be flushed at some point in time. In TropoDB, we do this by only queuing the contents of the put, but we do not wait for the write to finish. The request is only added to a submission queue, but no polling is done yet. RocksDB also guarantees that in case of a crash of RocksDB, the write will still persist, as long as the underlying file system does not crash. We make no such guarantees for TropoDB as we will need proof to make such claims, which makes our design a bit less reliable when sync is disabled. Future work can look into how to make sure that writes still complete on a crash. \\
\textbf{Effectively using the append command}\\
The once log also uses a unique concurrency model that is designed to match the performance that ZNS appends could give. The structure does not allow concurrent reads or writes, which reduces the need for unnecessary and expensive locking operations. Yet, it does allow for multiple concurrent appends (which aids DR3). This is made possible by using the append operation because, as stated in \autoref{sec:znsworkings}, it is possible to issue multiple appends in parallel to the same zone, at the risk of the individual appends being reordered. The WAL is allowed to be reordered as long as the order is known at the time of recovery. The ordering can be recovered by storing some additional metadata that specifies the ordering. After reading the WAL, the data can then be reordered/sorted and reapplied to the memory table in order. There are multiple types of metadata that can be used to infer the ordering. We will explain a few of them shortly. We will begin with the methods that we did not pick and explain why we did not pick them as well.

When using multiple appends asynchronously it is not possible to know where the data is stored, this information is only made available once the append has completed, which makes it impossible to store information about where other current appends are stored to WAL entries. However, it is possible to know where completed appends are stored. This allows storing a small metadata table containing the physical location of all previous appends in the order that they were issued (to allow sorting). Naively, this would be stored along with the appends at the beginning of each WAL append. This would not work. A major problem with this design is that it does not work for all current outstanding appends. For example, if there are 4 outstanding append requests, those 4 append requests will not have each other's location in their metadata tables. They might not even have knowledge of each other. This is always a problem for the last `n` appends and it would not be possible to find out the order of the last `n` appends. Instead, such metadata must be stored separately \textit{after} each append completed to guarantee correctness. In this new scenario, the last completed append will have accurate information about its ancestors. It requires an extra metadata append after each successful data append to the WAL. So each logical append require two device appends. This comes with the additional benefit, that only fully completed WAL appends are accepted (only WAL appends followed by a metadata update are accepted). However, this does requires an extra append operation (Not good for DR2). Further on, the metadata appends themselves can also be reordered. Therefore, it is not enough to pick the most \textit{recent} metadata table. Instead, the one with the largest number of entries must be picked or we need to store additional metadata to guarantee the most recent change is picked. Such metadata can for example be a sequence number or timestamp. In short, during a recovery multiple pages need to be read to find out the most recent metadata table update. Then this table will be used to read all WAL changes in order and apply them to the memtable in order.

TropoDB uses an alternative approach. The approach of appends as done in TropoDB for WALs is visible in \autoref{fig:walappend}. It does not bother with maintaining where physical data is stored. In fact, each WAL append has no knowledge at all about any other append. Instead, at the beginning of each WAL append a small sequence number is stored. This sequence number starts at 0 for an empty WAL. After each WAL append operation, no matter if they are synchronous or asynchronous, the in-memory sequence number is globally incremented. On recovery, the in-memory sequence number is set to the largest sequence number found in the WAL log. With this approach, the order of all WAL appends can be inferred from their sequence numbers. An alternative to sequence numbers would be to make use of timestamps. In this approach each WAL update is stored along with the epoch time the WAL append was issued at. This also guarantees ordering of append operations as long as the clock is properly synchronised. Using sequence numbers or timestamps does make recovery a bit more complicated. The recovery procedure is visible in \autoref{fig:walappendrec}. On recovery, all appends and their sequence number are first read. The appends are then sorted based on their sequence number and then applied one by one to the memory table in order. This design thus requires 2 reads for each append. However, it is not required to do 2 reads to storage. WALs are generally small and can be stored completely in-memory. Therefore, the WAL is first read in steps of MDTS (see \autoref{sec:znsworkings}) and then the WAL ordering operations are done completely in fast memory. Further on, the sorting is done on sequence numbers pointing to pointers which itself point to the actual locations in the memory buffer (the WAL is now in-memory), making the sorting cost negligible. It does not sort on the WAL data itself. In the evaluation as done in \autoref{sec:walrec}, we will see that the overhead of reordering an asynchronous WAL versus recovery for a synchronous WAL is not significant. 

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/unordered_wal_append.svg}
\end{minipage}%
\caption{ WAL design: Unordered appends to a WAL}
\label{fig:walappend}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/unordered_wal_recovery.svg}
\end{minipage}%
\caption{ WAL design: Recovering an unordered WAL}
\label{fig:walappendrec}
\end{figure}

Using versioning numbers to find out the ordering of ``multiple'' WALs is not new. This idea is based on ideas of earlier key-value stores that make use of multiple concurrent WALs. For example, SplinterDB~\cite{conway2020splinterdb}. SplinterDB uses multiple concurrent per-thread WALs and increments and uses \textit{cross-referenced logs} to achieve this feat. Each WAL operation increments a globally maintained generation number that is logged along with the data in the individual WALs, enabling a recoverable order. SpanDB takes a similar approach by using multiple parallel WAL write streams~\cite{chen2021spandb}. Notice the similarity to how WALs in TropoDB store a generation number for each update as well to ensure ordering.

 The proposed design makes using multiple appends a valid optimisation technique. It makes the hot path: appends, faster at the cost of making the cold path: recovery, slower. However, in some cases we do have to do some synchronisation (this is of course always the case when sync is enabled). This happens when the end of a zone is reached or a put requires more than one append. A put requires more than one append if its size is more than ZASL or the border of a zone is reached, which requires breaking up the append into multiple smaller appends. Synchronisation is needed here because the WAL does require individual WAL appends to be appended in order; all data of one unordered append is required be adjacent to each other. When I/O is split into multiple asynchronous appends, this can not be guaranteed. Alternatively, such big appends could have been further split into fragments with their own sequence numbers. Then apart from sorting writes, the fragments of each append have to be sorted as well. This approach has not been tried and left to future work (will aid for DR3). Instead in TropoDBs case, all outstanding appends in this case are synchronised until the write pointer is known and then only one append is requested for such I/O (forcing order for this append). Otherwise, we can safely enqueue multiple WAL updates asynchronously. Asynchronous appends are thus only possible at granularities less than ZASL. An outstanding asynchronous append request is reserved with the help of a \textit{concurrent waiting list}. The concurrent waiting lists checks if the maximum number of appends is not reached before an append is issued and reserves a slot for this append, requiring minimal locking. When the maximum number of appends is reached, some minor synchronisation is needed as well. In this case the outstanding appends are polled until one append finishes and a slot becomes available. The newly available slot is then claimed and is used for a new asynchronous append.    


Appends with an unordered order can be beneficial, but it might be better to use ordered solutions in some cases. For example, when reliability is required of the key-value store all WAL appends must complete before a put request is considered to be in a completed state. In this case, there is no asynchronicity to speak of and no queue depth. Therefore, unordered appends should be disabled in such a scenario. TropoDB disables unordered writes by default if the option sync is enabled. The option sync is used in RocksDB and TropoDB when I/O needs to be synchronised after each put operation. Disabling unordered appends is also used to test the performance gain that can be achieved with unordered appends compared to ordered appends. When unordered appends are disabled, it is not necessary to log sequence numbers anymore and a default implementation can be used. In this case, appends in the WAL will use a queue depth of 1 and look like the design seen in \autoref{fig:wallappendordered}. Notice that all I/O will happen in sequence. During recovery, all entries can be read in order and no sorting is needed. This recovery is more simplistic and visible in \autoref{fig:wallappendorderedrec}.
\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/wal_append.svg}
\end{minipage}%
\caption{ WAL design: Ordered appends to a WAL}
\label{fig:wallappendordered}
\end{figure}
\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/wal_recovery.svg}
\end{minipage}%
\caption{ WAL design: Recovering an ordered WAL}
\label{fig:wallappendorderedrec}
\end{figure} \\
\textbf{WAL zone circulation}\\
Another property that is interesting in TropoDBs WAL design is that it can lead to a few zones of the ZNS device getting most of the I/O. As mentioned earlier, WALs are the hottest I/O data structure, but are relatively small, only necessitating WALs to claim just a few zones. Nevertheless, this will lead to a few zones wearing out faster than other zones. Therefore, it might be better to rotate zones used for WALs to spread the load and prevent burning a few zones. Therefore, TropoDB allows assigning a larger region of WALs than just one WAL requires. The amount of zones reserved must always be a multiple of the amount of zones one WAL needs. Internally, WALs will then be circulated in this region of zones to spread the load. This is done with a \textit{circular log} approach and is visible in \autoref{fig:wallog}. In this example a WAL needs 2 zones and 4 zones are assigned to all WALs. The first WAL will use zone 1-2 and the next WAL uses zone 3-4. As all WALs have the same temperature, the temperature can be evenly spread. At the same time, it requires reserving more space for WALs and is thus a trade-off. This is, therefore, a good parameter to make configurable. 
\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/log_of_wals.svg}
\end{minipage}%
\caption{ WAL manager design: Contiguous circular log of WALs}
\label{fig:wallog}
\end{figure} \\
\textbf{WAL resets}\\
The last property that is unique in TropoDB is that the key-value store is directly responsible for cleaning its own WALs (Control for DR4). Theoretically, multiple erasure policies can be used, depending on the use case. This allows the key-value store to time its resets and make the latency more predictable. Removing WALs in TropoDBs design is not a suggestion like it would frequently be in file systems.  At the moment, TropoDB only supports an eager policy. Whenever WALs are removed and no longer necessary, the space is immediately reset for later reuse. This is done by background threads and never by clients. This ensures that clients should in most cases not have to wait for an erasure to complete, unless no more WALs are available (aids DR4). With the approach taken in TropoDB it is not possible to come in a situation where no more WALs are available, as all WALs are erased eagerly and there need to be at least 3 WAls (there is always one free).  The reason to pick an eager policy is to prevent needing large erasures later on for WALs or needing to make the client wait for new WALs to become available. WALs are hot, which makes it likely that WAL zones will need to be reused, especially when the amount of WAL zones reserved is little. A goal of TropoDB is to get latency stability, therefore, it might be better to spread out the cost of erasing the zones, instead of issuing a few expensive erasures once in a while. Further on, postponing erasures can cause all WAL zones to become filled. This will cause issues when a new WAL needs to be reserved, in this case the client has to wait for a new WAL to become available, creating latency issues. Ideally, this should be confirmed by an evaluation. Therefore, future research could look into multiple erasures policies apart from eager, such as a lazy strategy or erasing WALs every once in a while periodically (addressing DR4 optimisations).\\\\
\textbf{Addressing the design requirements}\\
The WAL design addresses the design requirements (and RQ2) in the following way:
\begin{itemize}
    \item DR1: The WAL implementation makes exclusive use of append operations.
    \item DR2: The WAL design of TropoDB can minimise overwrites and erasures because there are no in-place updates, either implicit or explicit. This is possible because only appends are used and data is never rewritten in the WAL by the LSM-tree itself. Further on, as WALs get a predefined set of zones, there is hot and cold separation, not requiring implicit rewrites for cold data by garbage collection processes. All WAL updates are in one big append-only log. While not necessarily an overwrite issue, some minor write amplification does occur because of the page size of the device. That is because WAL updates always need to be a multiple of the page size (by adding padding), but this can not be circumvented and happens regardless of ZNS or FTLs. Note that there is also research into \textit{group appends}~\cite{purandareappend} and \textit{rocks}~\cite{maheshwari2021blocks}, solutions that come with ZNS interface modifications that allow for smaller page granularities. The need for padding might, therefore, change in the future. Erasures are minimised as WALs are only erased when they are no longer needed and they contain stale data. This is data what should be removed in all cases. Further reduction of erasures can only be reduced by changing the LSM-tree design itself on top (compression techniques for example). 
    \item DR3: The WAL design makes use of the concurrency capabilities of the device by effective use of append operations. WALs make use of a novel operation known as unordered appends, that enqueue multiple appends to the same zone. This increase concurrency capabilities of the WAL and the key-value store as well. As an extra benefit, using multiple concurrent appends does not make use of extra active zones to achieve parallelism.
    \item DR4: The WAL design of TropoDB can aid in performance and latency stability with the help of appends and its erasure policy. Erasures are done eagerly, which should cause most clients to experience the same latency fluctuations because they are spread out. Similarly, clients should never have to wait for WALs to become available, which can be done by ensuring that more than one WAL region is available. Using a proper ratio of writes and appends can lead to more predictable and lower latency. 
\end{itemize}
\textbf{Disadvantages of the proposed design}\\
The WAL design comes with a few disadvantages. The first issue with the WAL design is that the amount and maximum size of WALs are predefined and reserved from the start. This is not the most space-efficient design. It means that WALs always make use of the maximum amount of space that they are allowed to use. Future work can look into an allocator that assigns zones to WALs only as needed, preventing the need to pre-allocate. This can also aid with spreading temperature on the ZNS device. WALs are very hot, even when a rotating scheme is used as is done in TropoDB. It can, therefore, be beneficial to also append to a few other regions of the SSD once in a while. Another challenge of the WAL design is that the size of an individual WAL is fixed and that WALs are inherently tied to a memtable. This makes it non-trivial to configure the size of a WAL to match the buffer size of memtables. Once a WAL is full, it is also truly full. It is not possible to write a little bit more data to the WAL in this case. This forces the key-value store to do a flush. Similarly, if the WAL is larger than what a memtable can hold, a part of the WAL will never be used. This can lead to resets on partially filled zones, which makes these resets less efficient (more data could have been written to these zones). This is also not solved by the WAL rotation scheme and will lead to holes in the WAL that will never be written or reset (Some WALs will never become completely filled), Currently, this is mitigated by making the WAL just big enough to match the memory table, but it might be a better idea to allow a memory table to use more than one WAL if necessary as this is more flexible. Additionally, the size of WALs should be altered to be more flexible.

\subsection{Group Logging}
\label{sec:desgrouplog}
Both LevelDB and RocksDB support \textit{group logging}~\cite{chen2021spandb}. Group logging allows batching multiple write requests for one log data write. For example, if a put is issued to the database and another put operation is also issued and waiting, the two can be merged into a bigger put request. This is done even before the write is issued to the memtable and the WAL. While it is not a focus for TropoDB to optimise group logging, it does support group logging and uses a modified implementation of LevelDBs group logging. Therefore, multiple appends can be batched as one update to the WAL and the memtable. Updates are, therefore, not necessarily send one by one. It is known that for faster NVMe SSDs group logging can result in writer threads idling and an increased software overhead~\cite{chen2021spandb}. It is not known if this is also true for ZNS and this has not been investigated in TropoDB. It is a good idea to further investigate this in future ZNS investigations.

\subsection{Handling Data Corruption in WALs}
TropoDB does not focus on how to become crash resistant or how to deal with corruption. However, it does come with a few features to make it a bit more tolerant of these issues. Appends can be done up to steps of ZASL, but there is no guarantee that all pages of the append are physically written. That is because updates up to ZASL do not have to be atomic, which can lead to issues on a system crash. However, appends at the page level do have to be atomic. Therefore, an append should come with some metadata to ensure that no corruption has occurred for each individual page. Each page written to the WAL (one append can have multiple) comes with a little bit of extra metadata. This includes the segment of the append (head, center, trailer), the amount of bytes used in this page and a CRC32 header to check for corruption of data. This was taken from the block implementation of LevelDB~\cite{LevelDB}. Future work, could test the effectiveness of this approach for ZNS designs and come up with designs that are more optimised and made more reliable for ZNS.

\subsection{Summary}
In short, WALs in TropoDB are designed around the concept of once logs. Once logs are structures made uniquely for WALs. They only support appends, resets and reads and only type of operation can be conducted at the same time. WALs are for most of their lifetime append-only, only requiring reads on startup and recovery. Therefore, WALs trade read performance for more append performance (DR1). This is done with the help of unordered appends. Internally multiple asynchronous appends can be enqueued to the WAL in any order (DR4). On database recovery, the entries can then be sorted to get a consistent ordering. WALs support a novel circulation scheme to spread heat across multiple zones. WALs are reset eagerly in background processes, which resets in at most one active WAL and one dead WAL at most. Lastly, WALs have support for group appends and come with basic corruption checking.

\section{Index Structure}
\label{sec:index}
LSM-trees make use of an index structure to maintain their metadata. This structure is necessary to maintain indexes for the tree components. That is because the LSM-tree contains various components, but there needs to be at least one structure that knows what each of these components is and where their data is stored. In particular, it needs to at least keep track of how many SSTables each level has and it needs some data to retrieve the data of each SSTable from storage. It can also be used to store additional data, such as metadata from deleted SSTables. The design of this index structure can differ between LSM-tree implementations, but generally resides in memory. Nevertheless, this structure needs to be stored on storage as well. That is because, when the key-value store is restarted, the index structure should still \textit{remember} the state of all of its components. Otherwise, proper recovery will not be possible. In general, it holds all state of the key-value store that needs to be persisted apart from the raw data of the components themselves.

Similar to memtables, this research is not about optimal in-memory structures, so an existing index structure is reused. Nevertheless, this structure does, unlike the memtable, need to be altered significantly. Normally it would contain file system information, such as filenames of live- and dead files, but this is not possible when not using any file system. Instead, for TropoDB it needs to contain component-specific information for each individual component. Therefore, it was decided to pick the LevelDB implementation. This implementation does not contain a significant amount of file system or I/O-specific information, which makes such a transition achievable. 

\subsection{In-memory Design of the Index Structure}
LevelDBs version structure is not very different from RocksDBs implementation, but contains less information and logic. In its most basic essence, it is a collection of metadata attributes and a \textit{version number}. In later sections, we will see what data needs to be maintained/stored for each structure. Notably, it does not need to maintain any metadata for WALs, it only needs data for L0 and up. The implementation of LevelDB makes use of a concurrency control method known as \textit{multiversion concurrency control} (MVCC). In MVCC multiple versions of the index structure can be alive at the same time. This method is used as an optimisation because the index structure can not be used safely while it is modified. That is because, if the index structure is modified during use, it will cause correctness issues for the reads. For example, an operation might read a part of the index structure in a different state than it reads the rest of the structure. On a read, all metadata must always be in the same state. Updates must, therefore, be atomic. A naive implementation would be to lock around all metadata updates to guarantee all changes are only done in one go. However, this requires waits for client-issued gets and is not ideal for operations that wait on I/O to complete (Will inevitable cause issues for DR4). The MVCC pattern can solve the correctness problem and still allows the client and updates to be done at the same time. We will explain how MVCC achieves this feat.

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/MVCC.svg}
\end{minipage}%
\caption{ Metadata design: MVCC pattern, the light gray version is the preferred version for clients }
\label{fig:mvcc}
\end{figure}

 We will explain MVCC with the help of the explanation in \autoref{fig:mvcc}. \textbf{(1)} In MVCC, when the index needs to be updated, a new version of the index structure is created in the background.\textbf{(2)} This new index structure will be modified, not the original index structure. This allows both the update operation and the get operation to proceed as they use a different structure. After the modification is completed, the new version will then be serialised and written to storage. Only once the modifications to the new version are finished (atomic), serialisation to storage included, is it safe to read for clients. \textbf{(3)} It is then added as a \textit{live} version to a \textit{version list}. On the background a \textit{versioning number} will be incremented globally and the new version will get this number. All get operations will read the version from the version list with the highest number. This effectively replaces the old version with the current version for all future reads. Such an approach, allows the old version to still be usable by gets of the client and should give minimal latency hindrance. The client always tries to claim the most recent available version and once it is finished, \textit{releases} the version.\textbf{(4)} Once an older version is no longer referenced by any client, it can be safely deleted from memory and storage (reference counting). This means that at any point in time, it is possible for multiple versions of the index structure to be \textit{active}. 
 
 The index structure is used to retrieve information about the LSM-tree. For example, on a get it is necessary to find an SSTable that contains the key. In order to get a consistent state (not information from multiple versions as data can move between levels as well) and ensure MVCC is obeyed, one version is referenced and pinned by the get operation (snapshotting an MVCC version). It then proceeds to iterate through L0 to LN until it finds the key-value pair that matches. The pinning operation is also done by both flushes and compactions to determine what SSTables they to merge and where to find those SSTables. Again a version needs to be referenced and pinned for these procedures to be safe. Versions are only released by an operation, once they are no longer used. 
 
 MVCC makes deleting data complicated. To illustrate this issue, assume an SSTable is no longer needed in a new version and can thus be deleted (at least according to this version). Therefore, a new version is made that no longer references this SSTable and marks the SSTable ready to delete. However, at the same time a client-issued get is still reading this old SSTable. In this case, we are essentially left in a state where some data is dead nor alive. To circumvent such issues, deletes of any type \textbf{must} be treated differently. This can be done by adding metadata for deleted data as well and only truly deleting it when no version contains this data in a \textit{non-deleted} state. In other words, metadata of deleted objects needs to be maintained as well. The metadata of the index can in the end be divided into three types of data: static data that is never modified after database creation, live data that differs between versions and dead data that differs between versions.

On a shutdown of the key-value store, no versions are referenced anymore. As stated earlier, the newest version is already present on storage. Therefore, they can all be safely removed from memory. On a restart, the most recent version is loaded from storage and reapplied to memory. This returns the database to a consistent state on recovery. 

\subsection{File System Approach for Storing Metadata of the Index Structure}
RocksDB and LevelDB make use of files to store their metadata. In particular, they make use of 2 different types of files, which we will both explain. These are known as \textit{manifest}~\cite{dong2017optimizing} files and \textit{current}~\cite{RocksDB} files. A manifest file is a file that contains all metadata of the index structure. There can be multiple manifest files with each file holding (a) different version(s) (again a form of MVCC). The current file's sole purpose is to keep track of what manifest file is the most recent manifest file. The current file only contains the name of the most recent manifest file. The file system approach for storing data can be seen in \autoref{fig:manifestfile}. In this figure, the current file references the current manifest file and will reference the new manifest file once the new manifest file is ready. Thus, the purpose of the current file is to determine what manifest file holds the most recent data. The current file only needs to be updated each time a new manifest file is created. Each manifest file that is not the \textit{current} can be deleted once it is no longer referenced. When and how the manifest updates or a new manifest is created, differs between LevelBD and RocksDB and what options are picked for these databases. 

\begin{figure}[h]
\centering
\begin{minipage}{0.35\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/manifestfile.svg}
\end{minipage}%
\caption{ Metadata design: RocksDBs metadata file approach }
\label{fig:manifestfile}
\end{figure}

For LevelDB, a new manifest file is created each time the database is restarted~\cite{LevelDB}. On database startup, the index structure is serialised and written to this file as a snapshot. From this point onward, the manifest file becomes and append-only transactional log. Each update to the index structure, is serialised and appended as a transaction to the manifest file. Updates can range from adding new SSTables, adding new deleted SSTables, to updates to compaction metadata (we will tell more about this in \autoref{sec:compact}). Important to note is that because the manifest file is used as a transactional log, the file does not hold just one version or just one state. Instead, it contains a snapshot of the index structure as it was on startup and all changes that were applied to the index structure after database startup. On a restart the database can be recovered in two steps. First, the snapshot is read and the resulting index structure is set to be the current index structure. Then all transactional changes are read from the transaction log and applied in order to the index structure. After all changes are applied and a new snapshot is created, LevelDB creates a new manifest file that contains the current \textit{snapshot} of the current state of the index structure. The old manifest file (that were used to get to the snapshot) can then be deleted. In this design, static data will only be written once for each manifest file at the beginning. Added and deleted files are not differentiated and are added to the log on a change (files are added or removed). As each file has a unique name, it becomes trivial to determine what files are still present when the log is read in order.

In RocksDB it is also implemented as a transactional log. RocksDB by default creates a new manifest file once the size of the transactional log passes a certain size. When a new manifest file is opened, the current state is added to the manifest file as a snapshot, similar to LevelDB. RocksDB policy for creating new manifest files prevents creating a large file of metadata (space-efficiency) and reading large logs on a restart.

An advantage of the manifest file design is that it fits flash storage and its properties. It does not do any in-place updates and is append-only instead. As manifest data is stored in different files, it also separates manifest data from the rest of the key-value store data, such as WAL and SSTable data. This leads to hot and cold separation. An advantage of storing the name of the current manifest in a current file, is that it prevents needing to read multiple manifest files to determine the current version. Instead, only 2 files need to be read: the current file and one manifest file. It also results in reasonably low write amplification at the application level. Only changes applied to the structure need to be written to storage, instead of the whole index structure on each change. It does make reads a bit slower as all changes need to be read instead of just a snapshot. However, since the manifest is only read once on startup, this is justifiable.  

The problem with using files for such a design is that it relies on the file system to treat the manifest as an append-only file. Manifest files can increase quite a bit in size throughout their lifetime, especially with LevelDBs design, and no space is reserved for manifest files beforehand by the database. Further on, manifest files are reasonably hot. They need to be updated on each flush or compaction, regardless of the SSTable in question. This results in the need to allocate more space for manifests on each background update. This in combination with the inability to physically force the file system to store manifest files in different zones than other files, can lead to issues. It prevents communicating to the underlying storage that it is better to store the manifest in a few separate zones and pre-allocate some space. For example allocating to the maximum size that a manifest can become in the case of RocksDB. Nevertheless, updates to metadata are generally small and are just a small part of the I/O issued by the key-value store. Therefore, the gain that can be achieved by reducing the semantic gap of manifests is less than for other parts of the database. 

\subsection{TropoDB Approach for Storing Metadata of the Index Structure}
Index data is stored a bit differently for TropoDB. In TropoDB a few zones are reserved at the start to hold all metadata. Essentially, the maximum size that metadata might need, needs to be reserved beforehand. This allows hot and cold separation of metadata from the rest of the key-value store's data (Addressing DR2). All of the metadata zones will then have the same temperature. A big difference in TropoDBs design is that unlike how LevelDB and RocksDB try to separate \textit{manifest} data and \textit{current} data, TropoDB makes no such attempt.  In TropoDB they are always stored physically together in the same zones. That is done, because there is no concept of files in TropoDB, so it is not possible to differentiate between the two types of data on that level. The only method to truly separate manifest and current data physically would be to store them to separate zones. As the current only needs to hold a few bytes that references the current manifest, it is wasteful to reserve entire zones just to hold a reference to the current. In order to guarantee persistence of the current data, at least 2 zones would always be needed (because of erasures and no support for overwrites). To illustrate this issue in more detail: the ZNS SSD used for this experiment has a zone size of $>$ 1 GB. This would mean reserving a few GB for just a few bytes, as only one current entry is active at the same time. Instead, it was argued that current is inherently tied to the manifest and the caused write amplification is negligible. As ZNS does not allow random writes, this means that manifest data and current data needed to be added to the same write pointer. Essentially making it impossible to figure out in what page the current is stored exactly.

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/metacollapsed.svg}
\end{minipage}%
\caption{Metadata design: Metadata operations on zones }
\label{fig:metadataworking}
\end{figure}

Therefore, it was decided to use the current in TropoDB as a \textit{barrier} for updates. This makes it easy to spot the current data, as it \textit{should} always be the last page written. After each update to the database, the current data is appended (DR1) in a separate page. This looks similar to \autoref{fig:metadataworking}. Data can be added to the same zones or different zones, but the current always follows the metadata that it references. If on a restart no such page is found, it is assumed that the last update did not complete or is corrupt. Instead, the last change is picked that is followed by an explicit \textit{current page}. A current page is a page that holds the current data, it thus replaces the concept of a current file used in LevelDB and RocksDB. Since the current data is never more than a few bytes, it should always fit in just one page. On a replay, the metadata zones are thus replayed from end to beginning until a valid current data page is found. This is also visible in the graph, where in order: the new, the current and the old current are read and verified. The first current that is valid is picked. This design also guarantees that updates to the index structure are atomic, which is an additional benefit that comes with this design. It is not possible to write a current page without writing all metadata first and the other way around.

Further on, the manifest in TropoDB is not implemented as an append-only log like LevelDB and RocksDB. It instead uses a \textit{Copy-on-Write} (CoW) design, or perhaps more accurately Copy-on-Append, implementation and uses explicit snapshots. Snapshots mean that after each update to the index structure, which happens after a flush or a compaction, the entire index structure is serialised and appended to storage (DR2 no overwrites). Snapshots are not assigned to different zones than other snapshots. So, multiple snapshots can be stored together in the same zones, achieving higher space-efficiency. This leads to situations such as the one in \autoref{fig:metadataworking}. Storing metadata as snapshots is also beneficial for the design of the earlier mentioned current data. When, a design different from snapshotting is used, we will have to deal with manifest data and current data being intertwined. Within the first page of a manifest and the last page of a manifest, there can be multiple pages holding current data. Therefore, without snapshotting a current should know where a manifest's changes begin, end and what pages in between the beginning and ending contain other current pages. These other current pages need to be ignored. This would require adding significantly more data about all other current data pages to the data of current page. When the amount of data passes a certain size, it will not even fit in one page anymore.

The CoW design used in the metadata design does require the \textit{current page} data to hold a bit more data, because it does need to know where the latest snapshot begins and ends. This is done by adding a few bytes to the current, defining the first page of the latest snapshot and the number of pages of the version it references. Since old snapshots are not stored in files, it does make deletion (and GC) tricky. It is not possible to delete old manifest files based on their name as would be possible with file systems. We can also not delete all zones that hold old snapshots, as there can be a live snapshot in a zone along with a dead snapshot. This issue is in TropoDB solved by deleting all metadata zones that do contain data, but that do not contain the most recent snapshot and current. The resulting GC process is visible in \autoref{fig:metadataworking}. In this graph, the GC moves from the first zone to the last zone that it is allowed to reset. Once the GC reaches a zone that it can no longer reset, it stops. Eventually, after enough updates to the index are completed, the last reserved metadata zone will be reached and the write pointer will have reached the end of the log. At this point in time, many earlier metadata zones should be safe to delete and reuse. Therefore, it should be possible to move back to the beginning of the metadata log. This requires a circular log design as visible in \autoref{fig:metadatacircle}. Since metadata is not restricted to one zone, in such a design it is possible and in fact likely for a snapshot to wrap around from the last zone to the first zone if it contains more than 1 page. This is also why the current data holds a reference to the first page and the number of pages of a snapshot. This should make it possible to also read the snapshots that wrap around from the end of the log to the beginning of the log (when first page + number of pages $>$ last page, do a modulo from the start). The metadata zones are thus used as a \textit{circular log} in TropoDB. 

\begin{figure}[h]
\centering
\begin{minipage}{0.35\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/Metadatacircle.svg}
\end{minipage}%
\caption{Metadata design: Metadata as a circular log}
\label{fig:metadatacircle}
\end{figure}

Deletions are done eagerly. After each update, all old data is removed (if any). This prevents doing one large removal operation once the circular log is filled (addresses DR4 issues). Instead, the cost of resets is spread amongst all updates. Alternative removal strategies have not been tried and are left for future work. For example, it would also be a possible design to do GC when no other background action is running, as removal of metadata zones is generally low-priority until all zones are filled.  

On a restart, it must first be determined where the circular log starts and begins. The location and the number of metadata zones are fixed, making it trivial to find out where all metadata is stored. Therefore, the zone heads are retrieved for all of the metadata zones. A zone that is followed by an empty zone (with wrap around as it is a circular log), is considered to be the \textit{head zone}. If all zones are empty, the first zone is the head. The tail is the first zone that is followed by a non-empty zone or the head. This recovery would break when all zones are filled, therefore, there must always be at least one empty zone between head and tail. This is forced in TropoDB.\\\\\
\textbf{Addressing the design requirements}\\
The metadata design addresses the design requirements (and RQ2) in the following way:
\begin{itemize}
    \item DR1: The metadata log design only makes use of appends.
    \item DR2: While the design does not make use of any overwrites, conforming DR2, it does make use of many writes. The metadata log design is not designed specifically to reduce write amplification. The design is CoW, which means that the entire index structure needs to be serialised on each update. The design by RocksDB only logs the changes in most cases, which can lead to less written bytes in the end. However, unlike RocksDB, TropoDB's metadata log design never has to rewrite data because hot and cold data is stored together. It is not possible to store metadata in just any zone. Live metadata will only be stored in a few contiguous zones at most. Metadata is considered to be relatively little for a LSM-tree, which means that the write amplification gains that can be achieved for this component are not big.
    
    The amount of resets can be kept low with the metadata logs design. During no test on real hardware was their a necessity to reset zones in the metadata log. Nevertheless, the circular log design is already prepared for situations in which it would (and evaluated on small emulated ZNS SSDs). The metadata explicitly uses CoW, requiring no in-place updates. This prevents needing to move live data out of dead zones in case there is not enough space left in the log. The circular log combined with an eager reset policy that resets whenever possible, further ensures that it will only be necessary to delete what is already dead. If a zone contains both dead- and live data, it will be ignored, at the cost of a little space overhead (because of the circular design, a zone can only contain 1 live entry at most). 
    \item DR3: The metadata log does not specifically try to increase parallelism. The metadata log is considered lower priority in TropoDBs design than other components. By not using extra I/O resources for the metadata log, more resources will be available for the other components.  
    \item DR4: The circular log uses an eager reset policy that resets whenever possible, By eagerly resetting, clients never have to wait for space to become available in the metadata log. The space is either already there or there is too much data to fit in the metadata log in the first place. However, as resets can be issued after each metadata update, metadata updates can take a bit longer as they wait for the reset to finish. Using a CoW design can lead to more I/O being required for each change as all metadata needs to be serialised on each update, this has not been a problem in practice, but it is something to consider. The amount of data that is needed to be written on each change depends on the size of the index structure. For a bigger key-value store (more SSTables), this resulted in writing a few megabytes of data on each update instead of a few pages. A low-latency system should also reduce the cost of this operation.
\end{itemize}

\textbf{Disadvantages of the proposed design}\\
The proposed design works, but also has a bunch of disadvantages. Since it uses a CoW design, it leads to WA (DR2 issues). All metadata is rewritten on each change, even if it is not altered. This can be expensive and lead to wear levelling. Preferably, a log of changes should be used as well, similar to how RocksDB stores metadata or how the WAL stores its data. Nevertheless, during testing it was shown that the amount of data written to the metadata log was relatively small, needing less than 1GB (1 zone) after more than a TB of key-value pairs was written. The achievable gains might thus not be as high. Future work can still look into how to use a log with changes instead of snapshotting, while still appending the ``current'' data after each update. For example, by maintaining positions of all live \textit{changes} in the current data instead of just a snapshot and merging all changes together into one big snapshot once in a while. Another problem with the design is the circular log. If an external force manages to erase one of the zones in the middle of the log, the implementation would no longer function. This can be mitigated, by storing a versioning number at the beginning of each zone. This should allow determining the ordering by something else than zone heads. Another persistency issue is that there is no error recovery in the case data is not followed by a current. Instead it tries to read an older version, if any, and discards all data in the corrupted version. This older version might reference data that is already deleted. All the data that was stored in the new version is lost and can not be recovered. This can lead to persistent storage leaks when new data was added to say L0 and is dangerous. To guarantee crash resistance, this needs to be improved.

\subsection{Handling Data Corruption in Metadata of the Index Structure}
RocksDB and LevelDB come with some checks for data corruption for writes to storage and reads from storage. Most writes done to files are split into the smallest denominator that a file system (and the underlying storage) can guarantee to be atomic to storage, such as a page. This is beneficial as the NVMe spec also specifies writes on the \textit{sector size} (which is another word for the page size) to be atomic. Each of these pages will then get a small header, containing information about this page. It contains how much data this page contains, the segment part of the append (trailer, center, or end) and a small CRC32 (see ``A painless guide to CRC error detection algorithms''~\cite{williams1993painless}) header check to verify data corruption. On a read, these headers will be read and verified. This corruption handling is also done for manifest and current files in RocksDB and LevelDB. TropoDB does not deviate here and does exactly the same. Metadata updates are split into pages and each page starts with a small header section. How well this corruption checking works, was not tested, but it has caught quite a few errors throughout development. Future research could look more into how to recover from such corruptions and what other forms of corruption are not caught yet.

\subsection{Summary}
TropoDB uses a similar index structure to LevelDB and RocksDB. This index structure makes use of the MVCC pattern to circumvent locking issues. To ensure that deletes are done correctly, both in memory and on storage, the index structure maintains pointers to both alive and dead data. After each change applied to the index structure, the index structure is serialised and persisted on storage. This is done with a circular log implemented on top of ZNS that holds snapshots of the index structure. Multiple snapshots can be written to the same zones. Internally the circular log also maintains pointers to valid snapshots. These pointers are stored along with the data they point towards. On recovery, the most recent valid pointers along with their associated snapshot are read. This snapshot is then deserialised and becomes the current the index structure. In the background old zones are reset eagerly (addressing DR4). The zones reset procedure is called after each update to the circular log and it resets all zones that do not contain the currently-used index data. The circular log also makes use of basic corruption checks by using CRC checksums. 

\section{SSTables}
\label{sec:sstable}
\textit{Sorted string tables} (SSTables) are designed to be stored on storage. They are immutable memtables converted to a storage-friendly format. They are always a collection of key-value pairs sorted by their keys. This allows iterating through them at a reasonable speed with for example \textit{binary search}. It also allows making a minimal summary of the SSTable to be used in the index structure, containing information such as its smallest and largest key. When a key does not fall in between the smallest and largest key of the summary, the SSTable will not contain said key, reducing the need for many read operations to storage. Additionally, SSTables store a \textit{versioning number}. This versioning number increments after each SSTable is generated and can be used to retrieve only the most recent change. For example, tables can overlap on L0. In that case only the most recent change needs to be picked. A common compression technique is to make use of \textit{run-length encoding} (RLE~\cite{robinson1967results}) to store the keys in SSTables. Run-length encoding is a lossless form of data compression, where consecutive overlapping items are compressed to one pair. This pair contains the overlapping data and the number of consecutive items that contain the overlap. For example if a part of a key is repeated multiple times in sequence, this part can be summarised as the overlapping part followed by the number of keys that have the overlap.  This is an effective strategy for SSTables as the keys are stored sequentially. This allows reducing the space needed for each SSTable and can increase space-efficiency. As tables are also smaller it reduces the number of pages that need to be written and read for writes and reads of SSTables. RocskDB, LevelDB and TropoDB all do this. The effect with and without such encoding is not tested for TropoDB in detail, as it has already been shown to have had a great effect in LevelDB.

\subsection{File System Approach for Storing SSTables}
To design SSTables for ZNS, it is a good idea to first look at how SSTables are stored in conventional designs. After transforming the memtable to the RLE-encoded SSTable, it is stored in a file in LevelDB and RocksDB. Each SSTable is stored in a separate file with a unique name. Each SSTable is thus treated as a distinct file system object. On a successful write, the given filename can be used to retrieve the SSTable. The name of each SSTable along with its size can, therefore, also be stored in the index structure. However, SSTables of different levels are treated the same and are indistinguishable for the file system. This can cause problems with hot and cold separation. That is because L0 is hotter than LN and it is, therefore, not advisable to put them in the same zones. Similarly, as both SSTables are the same type of file and they have the exact same internal logic, which misses optimisation opportunities.   

Using ZenFS as a file system alleviates the problem of hot and cold separation by ensuring that SSTables are most of the time persisted into their own zones~\cite{bjorling2021zns}. Nevertheless, there is still no clear distinction between SSTables and other file types or between SSTables of different LSM-tree levels.

\subsection{TropoDB Approach for Storing SSTables}
TropoDB differentiates between the logic for L0 and LN. That is because L0 is used differently than higher LSM-tree levels and TropoDB makes effective use of these differences. SSTables in LSM-trees are generally not rewritten to L0 (exceptions such as TRIAD~\cite{balmau2017triad} do exist) and L0 contains overlapping tables. All SSTables are eventually moved to higher levels than L0. Further on, SSTables in L0 are typically smaller than SSTables at higher levels. L0 can, therefore, be seen and implemented as a waiting queue. Whereas, LN SSTables need overwrites to support merges (merges can lead to DR2 issues), preventing such usage. We will describe both implementations (L0 and LN) independently in section \autoref{sec:l0} and \autoref{sec:ln} respectively. The result of this design decision is that SSTables of both levels are also stored a bit differently, as they are used by different data structures. The data is first RLE encoded. Apart from the RLE-encoding, no additional data is added to the SSTables themselves. For the next piece of information, we recommend first taking a look at the L0 and LN implementation itself, as the information is intertwined.

L0 supports storing data only at the level of pages. On L0 each SSTable is thus aligned on the level of pages. This requires setting a padding of at most $pagesize-1$ bytes for each SSTable. On a successful write, L0 returns the location of the first page of the SSTable and the number of pages used to store the table. This metadata is what needs to be stored to retrieve SSTables on later reads. This is used instead of a filename. No additional metadata needs to be stored in the index structure that is different from the file system implementation.

LN only allows claiming entire zones. An SSTable, therefore, claims a number of zones and not a number of pages. However, similar to L0, the actual data itself is aligned to pages. So this still only requires setting padding of at most $pagesize-1$ bytes for each SSTable. The only difference is that there can be multiple pages that are not written to at all, up to $zonecap-1$ pages or $(zonecap-1)*pagesize$ bytes in total. This also requires a bit more metadata to maintain than L0. On a successful write to LN, LN returns a number of \textit{zone ranges} and the total number of pages written. Zone ranges are contiguous sets of zones and are defined as the starting number of a zone and the number of adjacent zones claimed (1 for only this zone, 2 to also claim the next zone, ...). Up to 6 zone regions are allowed to be claimed by TropoDB SSTables by default. However, this is not necessarily the best number to pick and made configurable. The larger the number of regions, the larger SSTables can be, but the more metadata needs to be stored for each SSTable as well. In general, for each SSTable in LN, ``n" zone ranges and a number representing the number of used pages needs to be maintained. In short, LN SSTables in this design are less space efficient and need more metadata. 

\subsection{TropoDB Storage Design for L0}
\label{sec:l0}
L0 is different from the other levels of the LSM-tree. Memtables are flushed to L0 as they are and are appended to the rest of the SSTables on L0. SSTables on L0, therefore, can not be bigger than the size of memtables. The size of memtables is by default relatively small (a few gigabytes at most), which results in small SSTables on L0 as well. Further on, as the SSTables are simply appended, there can be SSTables with overlapping key ranges on L0. SSTables on L0 are never merged and not rewritten back to L0 by any standard operation. Instead at a certain point in time, L0 SSTables are moved/merged to the next level. In other words, data from L0 is relatively small, only written once and never changed in any way. Therefore, there is no need to implement overwrites for L0 (DR2) and it is accurate to say that L0 is essentially a \textit{waiting queue}. Lastly, L0 SSTables have a unique temperature, that is different from the other structures. L0 is hotter than L1 or LN, but colder than WALs.\\
\textbf{L0 hot and cold separation}\\
Similar to how WALs get their own set of zones, so does L0. L0 gets a predefined contiguous number of zones as L0 has its own unique temperature. This not only helps with hot and cold separation (aids DR2), but also makes it easier to create a structure that can manage the data in this region of zones to deal with the temperature as we will see later.\\
\textbf{Fitting L0 data in zones}\\
It was decided that since SSTables on L0 can be small, it is wasteful to reserve entire zones just for these SSTables. This will lead to large space-inefficiency. Instead, it is better to allow zones to contain multiple SSTables. Such a design would normally lead to problems when data needs to be overwritten because zones will contain both alive and dead SSTables, necessitating garbage collection operations (DR2 issue), which would in turn create write amplification. A zone can contain both alive and dead SSTables because of the MVCC pattern used in the index, see \autoref{sec:index}. This is not a problem for L0 as data is only written once and never altered.

On each flush, the SSTable in question is appended to the rest of the L0 SSTables (conforming DR1). SSTables have a maximum size. If the data of a flush exceeds this size, the flushed data is split into multiple SSTables. This is done to ensure that the size of L0 SSTables does not become too big, which can result in expensive merges. How appends work in this design can be seen in \autoref{fig:l0append} and will be described in further detail. In this example, a memtable is too big to fit in one SSTable. Therefore, it is split into two smaller SSTables. The version number is incremented and both tables get a unique number. These tables are then appended in order to the head of the log of L0. The resulting data layout will look similar to \autoref{fig:l0reset} (ignore the dead and live attributes for now). This metadata will then be stored in the index structure. In this example, all zones of the log of L0, are filled up. When the log of SSTables starts to fill up, a few of the SSTables need to be moved to the next level and deleted in this level (ideally this should be done before, which we will see later on for compactions in \autoref{sec:compact}). However, this deletion of SSTables poses one challenge. It is only possible to delete entire zones and we have just stated that zones can contain multiple SSTables in L0. The method that TropoDB uses to tackle this challenge is to maintain metadata of dead SSTables as well. These dead SSTables are also persisted along with the rest of the metadata in the index structure. They are virtually identical to ordinary SSTables, except that they are treated differently. Only when a zone only contains dead SSTables, is the zone considered for deletion. Then, when the zone is deleted, the dead SSTables it contains can and must be removed as well. Keeping the metadata would be disastrous, as that might lead to double deletions (double free) later on, which can delete newly stored data in these zones (remember after a delete, they are considered free again). In case a dead SSTable contains multiple zones, we can not remove the entire dead SSTable just yet. In this case the dead SSTable in question is split into a truly deleted part and a part that still needs to be deleted. The part that is truly deleted is removed and the rest is maintained in a \textit{mock} dead SSTable. This is also visible in \autoref{fig:l0reset}. In this example, all L0 tables except for SSTable 4 need to be removed. The rest of the SSTables are all deleted by an external process. However, only SSTable 1 up to 3 can be removed as the tail can not move past live files. SSTable 3 can also not be fully deleted as it partially lives in zone 2 which contains the live SSTable 4. In this example, the garbage collector will reset zone 1 and add a mock SSTable for the part of SSTable 3 that is present in zone 2. The resulting state will end up in a situation similar to \autoref{fig:l0circularappend}. 

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/L0_append.svg}
\end{minipage}%
\caption{ L0 design: Flushing SSTables to zones }
\label{fig:l0append}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/L0_reset.svg}
\end{minipage}%
\caption{ L0 design: Deleting SSTables from storage }
\label{fig:l0reset}
\end{figure}

There is still one big challenge. This challenge is that as described in \autoref{sec:index}, older versions of the index structure may still consider some SSTables to be alive, while they are considered dead in a more recent version. The dead SSTables contain in the worst case all pages between the head and tail, which dependent on the number of reserved L0 zones, can be multiple gigabytes. It is not investigated, how common such an occurrence is. Deleting dead SSTables that are still alive in a few versions, can lead to fatal errors when resetting the zones of these SSTables. Therefore, dead SSTables are only considered for deletion when they are not used in any live version at all. In other words, only the disjoint set of dead SSTables and all live SSTables for all live versions is used for deletions. All tables that could not be deleted are then added to the new index structure again as dead tables, as they still need to be deleted later on. This design mitigates most concurrency issues related to deletes and prevents leaking storage with \textit{persistent storage leaks}. Persistent storage leaks are leaks where deletes did not complete, but the system no longer knows the deletion needs to take place.\\
\textbf{L0 as a circular log}\\
There are multiple strategies that can be taken to determine where to append new SSTables and what SSTables to consider for compaction to the next level. In the end, it was decided to treat L0 as a circular log. This design was taken for a couple of reasons. Firstly, ZNS is limited in the number of open zones (see \autoref{sec:znsworkings}, addresses DR3 issues). This means that only a few zones can be neither empty nor full. Therefore, L0 can not write to many different zones at the same time or is required to finish zones regularly as a counter-measure, which is space-inefficient (finishing a zone wastes a couple of pages left in a zone). Secondly, data is always appended to L0, which makes using one write head convenient and natural. Lastly, data is only removed during compaction from L0 to L1, so by one task. This makes it convenient as well to use just one tail. 

L0 is thus implemented as a circular log that exists out of the N zones assigned to L0.  When an SSTable is flushed, it is appended directly to the head of the log (DR1), provided that there is space left. SSTables are in this implementation not limited to the size of a zone, they can be less or more than a zone and cross borders of multiple zones. As the log is circular they can even start at the last zone and loop back to the first zone. In \autoref{fig:l0circularappend} L0 can be seen as a circular log. The tail is still present at the beginning of zone 2, but zone 1 is already free. Therefore, the head of the circular log is free to write any data to zone 1. Compaction in the circular log design always needs to happen at the tail of the log as it is a circular log. Technically it would be allowed to use a different SSTable for compaction, but when a different table is picked, no space can be reclaimed as the tail can not move. Essentially, L0 would then be as filled as it is before, making no progress and stalling the client longer than needed (causing DR4 issues). Therefore, compaction is forced to start at the tail of L0 in TropoDB. This is an issue that we will explain in more detail later on and turns out to be a bigger problem than was initially thought.

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/L0_append_circular.svg}
\end{minipage}%
\caption{ L0 design: L0 as a circular log }
\label{fig:l0circularappend}
\end{figure}

However, this does make compaction a bit more complicated. That is because as a performance optimisation, it is common to pick all overlapping L0 SSTables on a compaction. When this is not enabled and there are many overlapping tables, we might need to do a lot of merges when only one merge would have sufficed; creating a lot of write amplification in the end (indirectly leading to problems with DR2). Further on, it might require a lot more compactions, hindering the latency stability (do note that using one big compaction has its own latency problems). Therefore, on a compaction all SSTables that overlap with the SSTable on the tail are picked, even in TropoDB. In TropoDB these SSTables are then marked as \textit{deleted SSTables} just like before, but they can not be deleted yet. Remember that SSTables that are not on the tail of the circular log are not even considered for deletion. We essentially end up with a circular log with \textit{holes}. Holes that will be reclaimed only when the tail of the log passes over them. Deletes are thus delayed. An advantage is that only full zones are considered for resets and resets are, therefore, used to their fullest potential.

The process of reclaiming needs some further explanation. This is done with a separate background operation, that we refer to as \textit{reclaiming invalid L0 zones}. It verifies what zones can be deleted, following the definition of dead SSTables and then eagerly deletes all of these zones at once.  This operation is in TropoDB called right after a compaction is finished (compaction itself we will describe in more detail in \autoref{sec:compact}), on startup of the database and once L0 is filled and can no longer be used. Deletion is thus always done eagerly; it is done after each moment that more data might be ready for deletion and it always deletes all data possible. Generally, this should always lead to minimal latency fluctuations because of deletions (addressing DR4). There are no big deletions once in a while or long waits because there is no more space left in L0 because of stale deletes. Instead, there will be constant small latency fluctuations. Alternative strategies have not been tested and are something future work could look into. For example, it might be problematic to schedule deletions when it is important to do a flush at the same time. For example, when a client needs to wait for a new memtable. In such a case it might be better to postpone deletes just a little while as generally a client should wait for their operation to finish as short as possible (alternatives to address DR4).\\
\textbf{L0 SSTable metadata}\\
In order to make the changes persistent, we need to store metadata about where the data of L0 is stored. Since the key-value store keeps track of both live and dead SSTables, we need a list of both. For each SSTable, dead or alive, we keep track of the starting page, the number of pages it contains and the number of bytes that are used by the SSTable. The number of bytes that the SSTable uses is necessary because SSTables by themselves do not have to be aligned to the page size and SSTables on storage, therefore, contain some additional padding. The circular log itself can take care of the wrapping of the address space and zone boundaries.\\\\\
\textbf{Addressing the design requirements}\\
The circular log design addresses the design requirements (and RQ2) in the following way:
\begin{itemize}
    \item DR1: The circular log makes exlcusive use of append operations
    \item DR2: Data in L0 is written once and never written again to L0, regardless of the storage design used. L0 functions as a waiting hub and all SSTables have an equal temperature. The circular log design ensures that this still holds true. Zones are only ever reset if they only contain dead data. Deletes of SSTables thus never rewrite data to L0 and cause additional write amplification or extra resets. All write amplification caused in L0 will thus be because of flushes and compactions, not because of overwrites or storage itself. The circular log design also ensures that zones that are reset are completely filled and only contain dead SSTables. No data needs to be moved and the resets are used to maximum efficiency (no partial resets). This would not have been possible if each SSTable got its own zone. Dead SSTables in L0 can not be reused, which means that the invalidated SSTables need to be erased at some point in time. Fundamentally it does not matter if the reset is postponed or not. The zone does not contain usable data and can not be reused until it is reset. The circular log design, therefore, does not issue more resets than necessary.
    \item DR3: The circular log does not try to increase concurrency capabilities explicitely. It does try to keep its number of active zones to a minimum, to leave more concurrency for other structures (even other circular L0 logs as we will see later in \autoref{sec:desconc}).
    \item DR4: Similar to most other data structures implemented in TropoDB, an eager reset policy is used. Whenever a compaction in L0 completes, all valid resets are issued. This should ensure that clients should generally not have to wait for long resets. However, the circular log can contain various holes, which can not immediately be deleted because of the circular log design. This can cause a large number of resets to still occur at later times. Further on, it can cause the circular log to be ``full'', even if it only contains a handful of live SSTables. This pseudo-full state, in turn, will require the client to wait for compactions. The circular log design might thus not be the best design to guarantee latency stability. 
\end{itemize}

\textbf{Disadvantages of the proposed design}\\
The L0 design comes with a couple of disadvantages. Some disadvantages also add more challenges for the research question itself rather than solving them, such as latency stability (causes DR4 problems). Many of these disadvantages were only found during the implementation and long benchmarks running the implementation. We, therefore, recommend also trying out other designs for L0 in future implementations or designs for LSM-trees. The biggest disadvantage is the explicit head and tail of the circular log. Compactions from L0 might always pick one table from the tail, but they can also pick additional tables from anywhere between the head and tail of the log. These tables are not present on the tail and can not be deleted. Instead, they will remain stored until all SSTables in their zone can be deleted and the tail is present in the zone. This means that there are no partial resets, so resets are only used to their fullest potential (generally good for RF2), but it also creates \textit{holes}. These holes are problematic as they can lead to highly fragmented situations; sometimes with more dead SSTables than live SSTables. The result is that the log might fill up, but not because it contains a lot of usable data and the client will have to wait even if there is ample space. Further on, it can take multiple compactions before the zone on the tail can be reset. If there is no space, the client has to wait for all of them to finish. This creates large latency instability and issues for the tail latency.

The next problem is adjacent to the earlier stated disadvantage. If immutable memtables are too big to fit in one L0 SSTable, they are split into multiple smaller SSTables. All of these SSTables are guaranteed to be disjoint, but they are added to the same circular log in order. Compactions from L0 to L1 (which we will discuss later) try to pick overlapping tables for compaction. This amplifies the problem of holes as it is unlikely to pick the SSTable next to the SSTable on the tail as the ranges do not overlap. It is in fact more likely to use SSTables that came from other immutable memtables. This could have been solved by using a few circular logs instead of one and adding the data to the circular log with the most overlap with the SSTable on the head (the adjacent SSTable).

Another problem is the explicit number of zones that are allocated for L0. This is not trivial to configure, but has profound effects on the latency of the key-value store. If L0 is full, no flushes can continue and if the memtable is full, clients will have to wait (DR4). If L0 is made large, there will be less space for LN, but the stall can also be postponed and eventually become a bigger problem (again DR4). In \autoref{sec:evalL0} we have investigated the effect of increasing and decreasing L0 and shown that it has a big impact on latency, write amplification and the number of resets. Using a large L0 can decrement performance, but a small L0 as well. Therefore, the size of L0 log must be properly chosen by clients of the database, but it is not yet known how an appropriate size for L0 can be picked.  LevelDB and RocksDB make use of a variable that contains the maximum numbers of SSTables in L0. The size of L0 approximates this variable, but the problem is that in the case of L0 it also contains dead SSTables. This is not easy to configure for. It is not possible to set L0 equal to the size of this constant (max SStable size * max number of SSTables in L0), it must be set higher to include deleted SSTables.

The last problem is recovery and crash tolerance. This is not a focus of TropoDB, but the current design will need significant alterations to support these functionalities. The circular log may not have physical holes, only logical holes. Any hardware error in a zone between the head and tail zone will lead to the circular log being unable to recover. Solving this would require extra metadata to verify L0 correctness and patch the log if necessary (filling mock data in broken zones). It also does not log what changes will be applied to the circular log before it applies the changes, it only returns once it is finished with what it was ordered to do. For example, if during an append to L0 the process crashes, no component has a reference to the bytes written by the append. This data will be neither dead nor alive and is not caught by log or the key-value store. No one knows that a write to the log occurred and that it did not finish. The result is that once the tail of the log reaches this garbage data, it can never reset and the log is broken. Similarly if during a cleanup operation, the process crashes, no component knows that some data might already be deleted. The key-value store will reattempt deleting its SSTables, which might have already been deleted. The tail has already passed them and again the key-value store is broken. These issues can be solved by first logging what change is going to be applied, then applying it, and only after applying it, removing the log or marking that it is finished. For example, with the help of a small WAL. Then on recovery, this log can be scanned and checked for partial operations and if necessary undo or reapply them. This will add more write amplification and needs another active zone, but is necessary to guarantee better crash tolerance.  

\subsection{TropoDB Storage Design for LN}
\label{sec:ln}
LN tables are the colder on-storage structures of the LSM-tree. They exist out of multiple levels and typically contain larger SSTables than L0. Unlike L0 they also contain data that can be overwritten. That is because each compaction can lead to a merge in the next level. Therefore, updates to LN need support for these merges. A circular log design like L0 would not work as well in this case because data can be modified anywhere within the log. Further on, storing multiple SSTables in a zone can be problematic. As updates can be anywhere, this can lead to situations where multiple zones contain dead SSTables, but no zone can be deleted. If no sane compaction policy is picked in this case, eventually there might be no space left because no zone can be cleared, requiring garbage collection operations to defragment the data (which cause DR2 issues).\\
\textbf{LN hot and cold separation}\\
LN gets a contiguous region of zones similar to the other data structures. This allows separation of hot and cold data. However, L1, L2, ... and LN all store their data in the same region of zones. This is done because as we will discuss later, SSTables in LN only contain entire zones. Data will thus by design never be stored in the same zone, from L1 onward. At this point, hot and cold separation techniques such as assigning unique zones make little sense. Similarly, this can lead to space issues. Each additional level is bigger, but stores the same data. L1 is already a lot bigger than L0 and the WALs. When all data moves to the next level, there always needs to be enough space to store all of this data. If L1, L2, ..., LN all have their size to a predefined constant that is their maximum we have an issue. All levels all reserve the maximum size that they can, because in TropoDBs design each component is assigned a number of zones from the start. This number of zones can not be changed after database creation. The results is that LN can not be bigger than $SSD_{cap} - metadata_{reserved} - WAL_{reserved} - L0_{reserved} - \sum_{i=1}^{N-1}{Li_{reserved}}$.  This in turn can result in LN not having a lot of space left. In this case, even if a multiple terabyte SSD is used, less than a few hundred gigabytes might be available to use for LN at maximum. Instead, since it is already determined that hot and cold separation from L1 onward is less of an issue, it is saner to store them all in the same region. The levels can then still have unique logic, for example, by specifying how big their SSTables are allowed to become, but LN will not be limited in size by the lower levels. Space that was claimed by those levels can then be used by LN instead, leading to better space utilisation.\\
\textbf{Fitting LN data in zones}\\
TropoDB claims entire zones for SSTables from levels L1 up to LN. This is easier to justify as SSTables are generally bigger at higher levels. It is not uncommon to see SSTables spanning multiple zones, making the space-inefficiency issues that would otherwise occur on L0 less. This design also simplifies deletion. When an SSTable needs to be deleted, it is not dependent on other SSTables in a zone. Instead, all zones of an SSTable can be safely reset (no DR2 issues ass well). After an SSTable needs to be deleted, a deleted SSTable is still generated, similar to in L0. Then when this SSTable needs to be deleted it is first verified if it is still live in any version. If it is not, it is deleted and its metadata as well.\\
\textbf{LN as a fragmented log}\\
LN is also not implemented as a circular log to prevent issues that can occur with the tail of a circular log. LN is implemented as a \textit{fragmented log} of zones instead. In this implementation, each SSTable has a round number of zones and zones are not shared between SSTables. What zones an SSTable receives are determined by a \textit{zone allocator}. A zone allocator assigns zones to callees and is similar to how a memory allocator assigns memory to its callees. This zone allocator can be designed in any way that fits the need of the key-value store. This allows creating an allocation pattern that fits the usage of the key-value store. Theoretically it could, therefore, also be implemented similarly to a circular log. However, a few implementation requirements are set for the allocators, to guarantee that they are in fact usable. Internally it should maintain a structure that is able to keep track of all live and free \textit{zone regions}, which is the structure that the zone allocator uses. A zone region is a contiguous set of zones and consists of a number identifying the first zone in the region and the number of additional adjacent zones present in the region (a number of contiguous zones). Client operations on the zone allocator should also make use of zone regions. Reads, appends and resets all need to use zone regions. On a successful put, it returns a set of zone regions acquired during the append and a number identifying the number of pages appended. If not enough space was present, no zone regions are claimed and an error is returned. The zone regions returned by a put can later be used to retrieve data. A get operation needs a set of zone regions and a number of pages to read to retrieve the data. It reads the zone regions specified in order and reads up to the maximum number of pages specified. Using more pages than fits in the zone regions is undefined behaviour. In \autoref{fig:lnstorage} a possible state for the fragmented log can be witnessed. In this situation, two SSTables are stored in a small LN log. SSTable 1 is stored in multiple zone regions and needs a little bit more than 2 zones to store all of its data. SSTable 2 needs less than one zone and only requires one zone region as well. An issue that is visible in this example and that is a problem with the fragmented log design in general is that if an SSTable takes less space than a zone region, some space is wasted. This causes gaps to occur in zones and causes minor space-inefficiency issues. This can be solved with approaches like \textit{tail packing} as done in file systems such as ReiserFS~\cite{galli2001journal}, but this is not attempted as forcing unrelated data together in similar zones can cause garbage collection issues (DR2 issues). The design sacrifices space efficiency to gain lower garbage collection costs.

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/LN_design.svg}
\end{minipage}%
\caption{ LN design: SSTables stored in zone regions }
\label{fig:lnstorage}
\end{figure}

A zone allocator is not allowed to do any garbage collection of its own volition. This must be explicitly issued with an erasure operation. This erasure operation again needs a set of zone regions. All zones in the requested zone regions will be erased and their zones will be added to the free list. If we are to delete SSTable 1 of \autoref{fig:lnstorage}, it would look like \autoref{fig:lndeletion}. In the example it can be seen as the zone regions of SSTable 1 only contain data of SSTable 1, they can be safely reset. The garbage collector thus resets all zone regions with their assigned zones of the SSTable in order. It ignores the rest of the fragmented log. Zone allocators also come with serialisation and deserialisation functionalities. This allows making the changes persistent, also on shutdown. This data should allow returning the zone allocator to the state of the zone allocator as it was before a shutdown. For example, it should still contain the correct zone regions that are free and used. Leaving serialisation and deserialisation up to higher functions allows determining for the level of reliability needed. For the best reliability, it is advisable to serialise and store the data on each change.\\
\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/LN_reset.svg}
\end{minipage}%
\caption{ LN design: Deleting SSTables from storage }
\label{fig:lndeletion}
\end{figure}
\textbf{LN zone allocation schemes}\\
One zone allocation scheme has been implemented and tested for TropoDB. A second allocation scheme that addresses most shortcomings of the implemented allocation scheme has been designed, but is not implemented. Its design will be described, but its implementation is left to future work. The implemented allocation scheme is similar to a traditional heap implementation of memory. The design is visible in \autoref{fig:zoneallocmerging} and we will describe the states in this figure step by step. This implementation internally starts out as a linked list with only one entry \textbf{(1)}. This entry consists of one large zone region, containing all zones present in LN and is marked as \textit{free}. When a number of zones is requested, a new entry is added to the linked list. This entry takes a few of the zones in the zone region of \textit{free} entry and creates a new entry specifying a new zone region, containing only the zones it needs. This entry is marked as \textit{claimed} \textbf{(2)}. The number of zones present in the zone region of the old free entry decrements. The next time space is needed, this process is repeated. However, as the linked list now also contains claimed entries, it can not split all entries. Instead, the linked list is traversed to find free entries. When more zones are needed than are present in an entry, the entire entry is transitioned to the marked state and the searching continues for extra free entries. In this case, multiple entries are returned and each entry corresponds to a zone region. Eventually, the fragmented log will contain various zone regions that are either claimed or free \textbf{(3)}. On a free, multiple things are done. First, the entry in question is transitioned to the free state again. If the neighbour of this region is also free, they are merged together to form a bigger zone region. This is also visible in step 4 of \autoref{fig:zoneallocmerging}. In this example, the first zone regions are erased and merged back together as one bigger free zone region. Eventually, when all entries are freed, we should be back to the original situation and there will be one big entry again. This implementation only serialises each entry in the linked list and as this data can be very compact, this can require minimal metadata. Even, when there are many zones. Nevertheless, a big disadvantage of this zone allocation scheme that was discovered during testing is that it does not properly spread its heat. As can be seen in the evaluation at \autoref{sec:heatdist}, data tends to stick to the beginning or end of LN as that is where all splits begin or continue. This is thus not the best implementation to reduce wear levelling. For the ZNS device used, the metadata effect of maintaining metadata for each zone is also negligible as the total zone count is less than 10000 and using merging and splitting to reduce cost, will have little effect on performance, both in space as in speed (traversing the small free list).

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/LN_merging_iterator.svg}
\end{minipage}%
\caption{ Zone allocator design: Merging allocator }
\label{fig:zoneallocmerging}
\end{figure}

The other allocation scheme that has only been designed is more simplistic, but is better at spreading the heat, at the cost of needing more metadata. In fact, the amount of needed metadata is fixed and scales linearly with the number of regions. It contains one entry for each zone, specifying whether it is occupied or not and one general pointer, that points to the last claimed zone. This design is visible in \autoref{fig:zoneallocsimple}. At the beginning, all zones start in the \textit{free} state and the pointer points to the first zone \textbf{(1)}. When zones need to be claimed, it first checks if enough zones are still available. If there are not, an error is returned. Otherwise, a loop is started. In this loop, the pointer increments and claims all zones it needs one by one \textbf{(2)}. If the zone passes the last zone, it loops back to the first zone. If any of these zones are adjacent, they are returned to the client as just one zone region. It limits itself to one entire loop. If no space was found, it is considered full \textbf{(3)}. On a deletion, it only resets the zone regions of the SSTables to the free state(3). No data is merged or split as was done for the merging allocation scheme. This solution leads to a better spread of heat. It has no bias towards any part of its zones as it always continues where it left off and walks among all of its zones. A major disadvantage of this design is that it can lead to a reasonable amount of metadata if there are a lot of zones. It is not known if this is common for ZNS.\\
\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/LN_simple_iterator.svg}
\end{minipage}%
\caption{ Zone allocator design: Simple allocator }
\label{fig:zoneallocsimple}
\end{figure}
\textbf{LN SSTable and allocator metadata}\\
TropoDB adds some restrictions to the usage of the fragmented log and its allocator. This is required because of the format in which TropoDB stores its SSTables. For example, it only allows using a maximum of ``n" (configurable) zone regions for each SSTable. Limiting the maximum size of SSTables to a maximum of n zones. These ``n" regions are stored in the metadata of LN SSTables. These tables then store up to n records that contain where each region begins and how many zones this region contains. This is used instead of a starting location as is done for the circular log of L0. Further on, to guarantee persistence TropoDB needs to store the metadata of the zone allocator after each compaction or deletion of LN data somewhere. On a restart, this metadata needs to be used to get the zone allocator in a consistent state. This data is stored along with the rest of the metadata of the index structure, but separate from the SSTables themselves. Theoretically, the metadata could also be stored in other metadata regions, this would have the advantage of separating concerns. For example, the metadata log of the index structure does not need to know about the freelist of LN, but in the current design it does. Nevertheless, the current design is beneficial as the data in the metadata log already needs to be updated after compaction and deletion. This allows issuing fewer appends in total and storing all the metadata updates to one location only.\\\\
% and at the same time inserted into a memory table. During the session only the memory table will be used for reads. The WAL remains write-only while the database remain life. Yet the full operation is only complete when the data is written to both memory storage.  We say that each change has to be backed storage. This is necessary to guarantee persistency. When the system shuts down, either intended or unintended, no data that has been stored by clients should be lost. This is not possible to do, when it is not stored on storage.
% When the database is restarted, the function of the WAL reverses. At this moment it is read-only. All data from the WAL is read in its entirety and reapplied in order to the memory table. A proper analogy would be to think of the WAL as  We should then be in a state close to where the database was shutdown.
% They log N changes 
% The WAL is written to most frequently. Each put writes a key-value pair It is the hottest structure
\textbf{Addressing the design requirements}\\
The fragmented log design addresses the design requirements (and RQ2) in the following way:
\begin{itemize}
    \item DR1: The fragmented log makes exclusive use of appends
    \item DR2: The LN design never stores multiple SSTables in zones. This has the benefit that a zone never contains both dead and live data. In turn, this ensures that when an SSTable needs to be deleted, it can always be reset on storage at no additional cost. No data needs to be moved out of the zones. The LN storage thus adds no additional resets, overwrites or additional write amplification on top of what is issued by the key-value store compactions itself. The only added amplification is caused by the extra metadata needed in the index structure, which is relatively small with the proposed zone allocators, such as the merging zone allocator. However, garbage collection of invalid SSTables will not cause extra overwrites. Resets do have one problem and that is that they can happen in zones that are not completely filled. The relative cost of resets can thus be higher with the proposed design.
    \item DR3: The fragmented log does not necessarily use of extra concurrency by itself, but it does allow for efficient parallelism as we will see and explain in more detail in \autoref{sec:desback}. Multiple concurrent threads can make use of the fragmented log as each SSTable gets its own unique zones and each write or read to an SSTable can, therefore, complete concurrently. It only requires a bit of synchronisation for the zone allocator, as there need to be consensus on what zones to assign to each SSTable. 
    \item DR4: By using an eager reset policy that resets old SSTables whenever it has the chance, we prevent cases where a large number of zones need to be reset. Resetting a large number of zones could lead to latency spikes. Spreading the load by ensuring that the number of zones that need resetting remains low, can spread the latency of all of these resets. Further on, LN is given a small number of active zones to use and is not designed to issue a large number of I/O in parallel. This is done on purpose. This leaves more bandwidth and storage for more important operations such as WAL updates or flushes. 
\end{itemize}

\textbf{Disadvantages of the proposed design}\\
The proposed design for LN comes with a number of disadvantages. The first disadvantage is space amplification. An SSTable is unlikely to be exactly equal to a round number of zones. The LN design only allows storing one (partial) SSTable in each zone and finishes the last zone on completion. The result is that the last zone of an SSTable in LN will contain unused pages. Each SSTable has this issue, which can result in a large number of pages on the SSD that will never be used and limits the size the LSM-tree can have on storage. This could have been solved, by allowing multiple SSTables to be stored in this last zone with solutions like \textit{tail packing} from file systems, but this would store unrelated data together. Storing multiple SSTables together is problematic as it can lead to two cases. Either a zone can only be deleted if all its SSTables are dead or on a delete live SSTables are moved to other zones. The first approach is not a good approach as it is possible for SSTables of the highest level to be stored with the lowest LN level. This can lead to situations in which no zones can be reset and there is a need for space. Essentially, the system will be locked because it can not safely delete any data. The second approach will lead to write amplification and a more involved garbage collector (DR2). The goal of TropoDB is to reduce such effects and this approach is, therefore, not taken. The LN design thus trades write amplification for space amplification (DR2 solution).

The second problem lies in the default zone allocator, the merging allocator. The merging allocator is not able to properly spread its heat. It does not explicitly rotate its zones. If a zone is reset, the zone is free to use again and the merging allocator is likely to pick this zone again. The result is that zones that are invalidated, will be used again, which can lead to peaks of resets. An effect we have also discussed for WALs in \autoref{sec:wals}. The solution is to use a different zone allocator. That is able to spread its load better.

The third problem is a concurrency problem (an issue with DR3). There is only one zone allocator for LN and this is shared by all compaction threads. The zone allocator will need consensus on what to do, which will decrement performance. This means that no matter how many compaction threads are used, there will always need to be some form of synchronisation to the zone allocator beforehand. The zone allocator of TropoDB uses locking to achieve this, which can decrement performance. This can be solved by making the zone allocator more concurrency friendly, but this has not been designed/implemented.

The last problem lies in crash tolerance and reliability. Similar to L0, all I/O operations done in L0 are conducted without first logging that they have been issued, have started or have finished. The LN design does not make use of journalling of any kind. This leads to partial/non-atomic changes if the process crashes during the change. This can happen for appends and deletes. For appends, the zone allocator will have no knowledge that one of its zones contains garbage data and will still give away its zone. That is because the zone allocator makes use of its metadata to check if a zone is full or not, and since the append that was issued during the crash has not been completed, the metadata was not updated. Therefore, the metadata will still state that the zones used by the last append are free. The result is that the appends to this zone in LN will fail and this compaction will fail. This zone is lost for good. For deletes, the problem is less. A delete is only completed if the zone is reset and the zone is readded to the free list. If the delete did not complete, either because the physical delete was not finished or the metadata was not updated, this will not matter. The reset operation is namely idempotent and can be done again to the specified zones. Similar to L0, the specified issues can be solved by making a log containing the changes before they are applied. Then on database recovery, the log can be scanned and undone/reapplied.

\subsection{Summary}
SSTables are always RLE-encoded before they are persisted to storage. SSTables of different LSM-tree levels are persisted differently to storage in TropoDB. All SSTables in the first level, L0, are stored in a \textit{circular log}. The circular log supports SSTables at the granularity of pages. Multiple SSTables can be persisted to the same zone, SSTables can be stored in multiple zones and SSTables can wrap from the last zone of the log back to the first zone of the log. This supports flushing small SSTables with little space-inefficiency. Since L0 is a circular log, SSTables can only be added to the head of the log and SSTables can only be physically removed from the tail. L0 is thus implemented as a waiting queue. Further on, SSTables are only truly removed when an entire zone of SSTables becomes invalid. To support deletes of SSTables that do not live on the tail, L0 supports pseudo-deletes. SSTables that are marked as deleted, but that are only  physically deleted once the tail passes them.

All SSTables from the second LSM-tree level onward are stored in a structure we refer to as a \textit{fragmented log}. This fragmented log is shared between level L1, all the way up to LN. The fragmented log only supports reserving entire zones. This means that each SSTable in the fragmented log, has exclusive access to a round number of zones. These zones are allocated with the help of a \textit{zone allocator}. A zone allocator decides what zones a SSTable will receive and can decide for itself what access pattern it wants to use. The zone allocator we used in TropoDB is the \textit{merging allocator}. An allocator that tries to keep a minimal amount of metadata, but does not focus on spreading heat across its zones, which can lead to improper heat distribution. Resetting SSTables in LN is trivial as an SSTable has an exlusive set of zones. On a delete of an LN SSTable, the zones are reset and the zones are given back to the zone allocator. To guarantee persistence, the metadata of the allocator needs to be stored. This metadata is stored to along with the metadata of the index structure (see \autoref{sec:index}) and is stored after each change in LN. 

\section{Background Operations}
\label{sec:desback}
An LSM-tree makes use of various operations to move data from one layer to the next. How these operations are performed drastically alters how the LSM-tree functions. For example, they have an effect on the size of each component, the latency, and throughput of the database and how and when garbage collection is performed. Further on, most of these operations are disjoint and can run in parallel. They can interfere, which requires proper coordination for optimal results, depending on what is required. In this section, we will take a look at the individual operations and how they coordinate. They will be compared to existing solutions in RocksDB and LevelDB.

\subsection{Flush Operations}
Flushing operations occur when an immutable memtable is ready to be written to L0. This is done by a separate thread. LevelDB, RocksDB and TropoDB all schedule a flush operation, which can be picked up by a background thread. This flush operation reads the memtable pair by pair and writes the data to storage. LevelDB writes all of this data to one new file. TropoDB has a maximum size for SSTables on L0, different from LN. This is implemented because LN in TropoDB only supports bigger I/O at the scale of zones. This is not necessary for L0 and allows more fine-grained control. Therefore, in TropoDB multiple SSTables can be generated with one flush, by splitting the memtable, each with a different range of key-value pairs. Once the writes are complete, the changes are added to the index structure and then the new index structure is written to storage as well. This is the same for both LevelDB and TropoDB. After this operation, the immutable memtable is removed and the flush is completed. However, TropoDB adds one additional operation. As the memtable is inherently tied to one WAL, the WAL needs to be erased as well. Therefore, the flush thread also issues a reset of the entire WAL in question after the flush has been completed.

We mentioned that a flush can lead to multiple SSTables. This is because a flush can contain quite a few key-value pairs. Especially, when a large memtable size is picked. Therefore, a maximum L0 SSTable size is defined. If the size of a flush exceeds this size, it is split. Each split gets a guaranteed to be a disjoint set of key-value pairs. This prevents creating excessively large SSTables that need to be merged into L1 later on. The memtable split does not divide the memtable in equal parts. Instead, it cuts the SSTable right at the moment that it becomes too big. Therefore, it is possible for the last SSTable to only be a few key-value pairs in size, even if the memtable itself is large. As the data will not remain in L0 and the circular log has support for SSTables the size of pages, the effect is considered to be negligible. 

\subsection{Compaction Operations}
\label{sec:compact}
Compaction can happen from any level between L0 and LN, but the data is always sent to L1 and up. The data structure for LN is designed to match garbage collection and garbage collection in TropoDB is designed to match LN. Garbage collection requires some data to determine what task it should do first. It at least requires a policy to determine if a compaction is necessary, what compaction should be picked first and what SSTables should be picked from each level for this compaction. It also needs to determine how big the resulting SSTables will be. Commonly, the level that is compacted \textit{from} determines what SSTables it wants to compact for the next level and the level that is compacted \textit{to} picks all overlapping SSTables. What SSTables are picked by the level that is compacted from can have a large influence. For example, on write amplification of the LSM-tree. Doing many small compactions instead of one large compaction can lead to extra write amplification, but large compactions can lead to expensive operations stalling clients (DR4). There exist various studies that examine compaction strategies in more detail~\cite{balmau2019silk, balmau2017triad, pan2017dcompaction, mei2018sifrdb}, but that is out of scope for this research. Instead, a strategy is picked that is similar to what LevelDB uses. LevelDB uses round-robin scheduling for picking SSTables, TropoDB does the same. This is possible by maintaining a \textit{compaction pointer} pointing to the last picked SSTable. The next compaction then continues from this compaction pointer onward. This pointer is maintained in the index structure as well and requires one for each level. TropoDB does make one exception for L0. On L0, compaction always starts from the tail. This is necessary for the circular log of L0 to make progress. The SSTables that overlap in the next level are then picked as well. The picked SSTables are then used for the compaction.\\
\textbf{Limiting SSTable size}\\
For ordinary merges, it is possible for many SSTables to be involved. It is not advisable, to store the result of this merge in just one big SSTable. This big SSTable will be expensive to read by later reads and will both be an expensive target for later merges and will be merged into more frequently (it is likely to have overlap). It is considered better to restrict the size of an LN SSTable and to write multiple LN SSTables to storage after a compaction if necessary. This approach is taken by both LevelDB, RocksDB and TropoDB. A problem that this splitting can cause, is suboptimal space utilisation. Earlier, we stated that SSTables for LN reserve a round number of zones. If the maximum number of zones for SSTables is configured incorrectly this can lead to holes. Further on, it is possible that the last few LBAs of a compaction will be stored in a separate SSTable. This SSTable will claim quite a bit of space and will be hard to grow in size as it contains a small range of keys. It is important that this is prevented as much as possible. Possibly by increasing the maximum size dynamically for a few edge cases. This has not been investigated further as space utilisation is not the prime objective of this research and is left to future research. In the tests run for this research the maximum size was set to a little bit less than 2 zones. Not using a tiny bit of space at the end of zone 2 is done to avoid creating a very small SSTable at the end. The next SSTable that is written during the compaction needs to be at least the size of this tiny gap (else it would not have been added to a new SSTable in the first place).\\
\textbf{The compaction procedure}\\
Compactions consist of a few operations. The SSTables are read into memory, merged into new SSTables in memory, the new SSTables are then persisted to storage and lastly, the changes are added to a new version in the index structure. Nevertheless, not all of these operations need to happen in order or synchronous. TropoDB continuous on the compaction operation of LevelDB. LevelDB uses \textit{lazy reading} (we call it lazy reading at least) to read its SSTables and uses a variant of merge-sort to sequentially create new tables. The lazy reading is done by creating an iterator existing out of two smaller iterators. One iterator is for L0 and one iterator is for LN. Data can be merged from these iterators, by reading the smallest key-value pair from both iterators iteratively and incrementing the iterator whose pair was picked. One iterator is for tables from L0 (not present with all merges) and one for LN. As all tables from L0 can overlap, they are immediately read and stored in memory from the start. The first iterator can be used to iterate over these SSTables in order (pick the smallest key from all L0 SSTables). The LN iterator makes use of the fact that no SSTable can overlap in LN. This requires only one SSTable from this level to be present in memory at any point in time. Therefore, only one LN SSTable is read at the start of the merge. Once the LN iterator finishes, a new SSTable is then loaded from LN. This prevents needing to read the entirety of LN into memory and allows for very large compactions. Once the result of the merge reaches a certain size, it is sent to storage and a new SSTable is allocated. Only once the write is done (synchronous or asynchronous), will the merge continue. Only once all data is merged, the compaction is considered done. At this point, the metadata of the new SSTables is added to the index structure and the old SSTables are marked as ready to delete.

TropoDB modifies a few parts of the compaction procedure to make it more optimal. These optimisations rely on the fact that not all operations of the merge need to be done sequentially and that there is full control over the storage with TropoDBs approach. Firstly, TropoDB \textit{prefetches} a few LN SSTables from a separate thread (DR3 optimisation). As stated before, only one SSTable from LN is read and present in memory by default. This can cause multiple waits during the merge, where the compaction thread has to wait for a new SSTable to be loaded into memory, instead of doing merge operations. To alleviate this problem, it is possible to prefetch SSTables during the merge in memory. In this case fewer waits need to happen as the table might already be present in memory. Therefore, TropoDB preloads a few SSTables lazily with a separate \textit{reader thread} to reduce the number of merge stalls. Prefetching can happen from any level, just like compactions. So, L0 and LN SSTables can both be prefetched. These reads happen while the compaction thread is merging. This also makes sure that the storage is still used, even during a merge, more efficiently using the available hardware. The reader thread internally limits the amount of SSTables that are prefetched to make sure that not too much memory is used and once a prefetch is no longer needed, it is unloaded.

The other optimisation TropoDB makes use of is deferring the writes of SSTables to a separate \textit{writer thread} (DR3 optimisation). In LevelDB, the compaction thread has to wait before the new SSTable is written to the underlying file system. During this wait, more data could have been merged in memory as writing SSTables and merging are independent jobs. This can be especially problematic when SSTables are large and no buffering is used by the underlying storage system, which is the case for direct writes with file systems or always the case for fragmented logs in TropoDB. TropoDB solves this issue by sending the SSTable to the aforementioned writer thread instead. The task then still needs to be done, but it will not be done by the compaction thread itself. This \textit{writer thread} can hold a few SSTables in a memory buffer and writes the SSTables this buffer contains to storage in order. This allows doing both writes and merges in parallel (task parallelism). However, once the buffer of the writer thread is full, the compaction thread still has to wait for space to become available. Similarly, once all merges are complete, the compaction thread has to wait for the writing thread to finish. This ensures that the compaction itself is still synchronous.\\
\textbf{Lazy compaction by only updating metadata}\\
It is possible to have compaction operations in which only 1 SSTable is involved. This can happen when 1 SSTable moves to the next level, but has no overlap. In this case no merge is necessary and the SSTable can simply be written to the next layer as a copy. The compaction operation is thus copying the exact same data to the next level and altering the metadata to represent the next level. This only requires reading one SSTable, writing one SSTable and altering the index structure. However, SSTables on LN live in the same region. It is thus not necessary to physically rewrite the data. Therefore, as an optimisation, when an SSTable is copied in any layer above L0, the table itself is not even physically copied (Not DR2, but similar). In this case no SSTable reads or writes are issued. Instead, only the metadata is updated. Such an approach can reduce write amplification effects of compactions to a limited degree.

\subsection{Background Operation Coordination}
A common optimisation to reduce the effects of compaction is to do multiple compactions in parallel. For example, compactions from L1 to L2 do not depend on data from L3. Therefore, it is possible to do compactions on both L1 to L2 and L3 to L4 at the same time. This can prevent long stalls (DR4), where many levels need compactions and need to wait on one another. If this queue becomes large enough, it is possible that eventually there is no space left to flush. At this point, there is essentially a traffic jam of compactions and client operations will be stalled. Doing multiple compactions in parallel can break this jam (DR3). However, doing multiple compactions at the same time requires more CPU cores to perform and will ask more from the storage. Every additional concurrent compaction will increase the time each of the other compactions takes as they contest over the same resources. It is a trade-off. Another optimisation that is orthogonal is making use of preemptions. For example, if during a compaction another important background operation is required that is more urgent, the current task is put on hold, and the thread moves to the other operation. This can be seen as an \textit{in-software context switch}. It can prevent having to wait on a flush that can not continue because of another operation, but it does cause stalling issues further down the LSM-tree. 

LevelDB uses just one garbage collection thread in total~\cite{zhang2017flashkv}. This thread is shared between both flushes and compactions. To alleviate this problem, LevelDB does allow for the preemption of tasks. Nevertheless, with such a design only one task can be busy at most, even with preemption. RocksDB takes a more advanced approach and allows running multiple background threads in parallel~\cite{kannan2018redesigning, balmau2019silk}. These background threads can each do either a compaction or a flush and a maximum number of both flush and compaction threads can be specified. This allows configuring how many threads make sense to use for flushes and/or compactions. Similar to LevelDB, some degree of preemption is available. This design can solve many congestion problems and is also beneficial for SSDs as SSDs have the capability to do many operations in parallel. Only using one background operation would not allow this.

TropoDB also allows doing multiple operations in parallel (DR3). The thread model TropoDB uses is visible in \autoref{fig:tropothreads}. TropoDB uses multiple operations in parallel to match the concurrency of the SSD and lower the effects of stalling, as latency stability is one of its goals. TropoDB also has some congestion issues in L0. This is because the maximum size of L0 is limited to its pre-allocated zones. Once it is filled, there is no more space left for L0. This makes it important that data will move to at least L1 at a regular interval, to prevent stalling. However, TropoDB originally inherited its compaction process from LevelDB, which is not trivial to copy to RocksDBs design. Therefore, a design that is in between LevelDB and RocksDB is used instead. TropoDB uses a set number of background threads, each with their own predefined role. Essentially, preemption and background threads are moved into one concept. TropoDB always uses exactly 1 flush thread and 2 compaction threads. The flush thread is only responsible for appending SSTables to L0. In the case no more space is available, it simply waits till space becomes available. One of the compaction threads is known as \textit{L0Compaction} and is responsible for moving data from L0 to L1. It also in charge of cleaning the old data in L0. This thread maintains the bridge between the circular log region of L0 and the fragmented log region of LN. This compaction in particular gets its own thread because of the aforementioned problem with the size limit of L0.  The other compaction thread is responsible for compactions from L1 and up and cleaning up the fragmented logs from L1 and up.\\
\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/TropoDB_threads.svg}
\end{minipage}%
\caption{ TropoDB design: Background threads }
\label{fig:tropothreads}
\end{figure}
\textbf{Stalling client issued puts}
\label{sec:stalling}
A technique that is used by both LevelDB and RocksDB that is important to mention is stalling writes when the key-value store is \textit{busy}. This is important as it has a big influence on latency experiments we discuss later on in \autoref{sec:experiments} This delay is caused by giving \textit{put} operations a small extra delay when the database is busy. For example, by making the thread \textit{sleep}. If such a stall is not done and put operations keep on writing, eventually there might be no space left in the memory table as a flush still has to complete. A few client-issued puts would then have to wait for long periods of time, but most clients would have a lower latency. This creates a skewed latency graph and is detrimental to tail latency. Stalling writes can make latency stable for more clients. Instead of making a few clients wait long, many clients will have to wait a bit longer, provided the flush can finish on time. It can thus be used as a \textit{latency stabiliser} (DR4 related, but not for storage). TropoDB also uses a delay to control the latency and uses a similar approach to LevelDB. When L0 reaches a certain size, every put operation has to wait for a short while. This size is set to be significantly larger than the trigger that causes compaction from L0, indicating a large queue of compactions is awaiting in L0 and a traffic jam is likely to occur soon.\\\\
\textbf{Addressing the design requirements}\\
The background operation design addresses the design requirements (and RQ2) in the following way:
\begin{itemize}
    \item DR1: Background operations do not do I/O themselves.
    \item DR2: A few design decisions are made to reduce the overwrite issues of background threads. LSM-trees basically overwrite data by rewriting the data to the next level, which causes write amplification effects. Lazy compaction in LN ensures that if tables are merely copied in LN, they are not rewritten, reducing write amplification effects. Further on, tweaking the size of both L0 and LN SSTables can allow making trade-offs in sizes. If the size of SSTables in L0 is big, it is likely to overlap with more SSTables, which can lead to bigger merges in L1. This can lead to more expensive compactions in latency, overwrites and resets, but using a smaller size will require more merges in the end. Each flush can lead to multiple smaller tables that each require a merge later on. The different compaction threads also indirectly influence write amplification. If the L0 to LN is more aggressive than the LN compaction threads, it can lead to L1 flooding and the LN compaction thread being able to keep up. This in turn will lead to large write amplification in L1 because of frequent merges. Resets are assigned uniquely to threads in TropoDB. The flush threads erase WALs, the L0 to L1 compaction thread erases L0 data and the LN compaction thread erases LN data. This design allows for a low amount of locking and proper separation of responsibilities, but it does inherently lead to more resets. However, as all background threads make use of eager resets, it can lead to more resets than alternative designs. All threads make sure that all of the invalid data is erased. Other designs could have postponed said erasures. 
    \item DR3: Multiple background operations try to make effective use of the available parallelism. Flushes, L0 compactions and LN compactions can all be done in parallel. Each of these operations uses a different selection of active zones on the SSD and can completely proceed in paralllel. Additionally, compactions make use of multiple concurrent reader threads to prefetch SSTables and allow for deferring writes. These optimisations allow for task parallelism in the key-value store and make effective use of both the storage capabilities and the host capabilities.
    \item DR4: Many design decisions have been made to reduce latency issues. The default LevelDB design only has one background thread. This can lead to long waits for background operations to finish. Especially when it is possible to do multiple operations in parallel. By using multiple background threads, clients have to wait shorter for flushes or L0 to L1 compactions to complete, which can reduce tail latency. Properly adding a bit of delay once the L0 log starts to fill up can also aid in tail latency, because it prevents flooding the L0 log and ending up in a situation where clients have to wait long for background threads to complete. Delaying prematurely can allow more data to move out of L0. 
\end{itemize}

\textbf{Disadvantages of the proposed design}\\
The proposed design is not an ideal design and leaves room for optimisation. Initially, most logic was built on top of LevelDBs background thread model, which did not scale as well as was required for TropoDB in the tests conducted. Above all, it could not achieve proper tail latency (DR4). Therefore, the design had to be altered, but moving to RocksDBs background thread model was incompatible with the components that were already built. As a result, a design was picked that was halfway between LevelDB and RocksDB. This design is made to do more in parallel (DR3), but still less so than RocksDB. 

The biggest issue lies in the communication between the L0 compaction thread and the LN compaction thread (DR3). This happens because both can have compactions in L1. The L0 compaction thread always compacts its data to L1 and the LN compaction thread can compact from L1. If both a L0 compaction and an LN compaction are issued at the same time and their tables overlap, synchronisation is needed and compactions can be stalled. The result is that it is not always possible to do concurrent compactions and that in the worst case the design approximates a design with one compaction thread. This can be solved by prematurely ensuring that the compactions will not use SSTables that overlap or adding logic that can support overlapping compactions, at the cost of some extra write amplification (dead tables are written again).

The next issue is that there is only support for one L0 to L1 compaction thread and one LN compaction thread. This limits scalability (DR3 and DR4) and is not able to use the full potential that some SSDs can give. RocksDB can use an arbitrary number of compaction threads. It would have been better if TropoDB used a similar design, but it does not. The reason is that it is not trivial to move TropoDBs design to a model with an arbitrary number of threads. The compaction procedures contain various contention points that are not thread-safe. For example, the erasures in L0 and LN. These operations are atomic and replace all current deletes for their level. In order to support an arbitrary number of threads, these components need to be significantly altered to support this functionality. We recommend future designs do this from the start. The current design in which each compaction thread is responsible for different tasks (task parallelism) is a mitigation strategy in which concurrency issues do not occur because there is only one thread addressing them. 

\subsection{Summary}
TropoDB makes use of a unique background operation model. TropoDB makes use of a fixed number of concurrent background threads (DR3), each with a different purpose. There is a \textit{flush thread}, a \textit{L0 compaction thread} and a \textit{LN compaction thread}. The flush thread is responsible for flushing immutable memtables to L0 and resetting old WALs. WALs are reset eagerly, which means that after a flush all WALs are removed that are no longer used. The L0 compaction thread is responsible for compactions from L0 to L1. It reads SSTables from the circular log of L0 and merges them with SSTables in the fragmented log of L1. It is also in charge of deletions in L0. Deletions in L0 are done eagerly, which means that after each compaction all SSTables that can safely be deleted are deleted. Further on, SSTables in L0 are removed when there no longer is any space left available and SSTables can be safely removed. The LN compaction thread is in charge of compaction from each level after L1 onward. It decides what LSM-tree level needs compaction the most and then executes this compaction without interruption. It is also in charge of all SSTable deletes from level L1 up to LN. These resets are also done eagerly and are done immediately after each compaction. Deletes in LN are also triggered when space runs out.  To guarantee that the L0 compaction thread and the LN compaction thread do not interfere, they have the ability to coordinate (DR3 and DR4 issues). 

\section{Increasing Concurrency}
\label{sec:desconc}
The default LSM-tree design is serialised and has problems with a few bottlenecks. These bottlenecks cause other operations to stall and hurt latency stability (DR4). For example, the need to wait for a flush to complete when an immutable memtable already exists and a new memtable is filled. With modern hardware, it is not necessary to limit the database to use a serialised approach (DR3). Modern hardware such as SSDs support doing multiple operations in parallel, so serialising is not ideal. Therefore, it is not necessary to limit database design to only one mutable/immutable memtable pair and one ``concurrent" flush. Instead of waiting it should be possible to do multiple flushes concurrently, which is supported by the parallelism capabilities of SSDs. Such designs are known to have merit and have, for example, already been tried out for \textit{Open-channel SSDs} (OCSSDs, a precursor to ZNS). For OCSSDs it is possible to control individual channels of the SSD and do as many operations in parallel as there are channels. Some key-value stores make use of this characteristic to increase parallelism. For example, LOCS and FlashKV.
LOCS uses a memtable for each channel of OCSSDs~\cite{wang2014efficient}. In this case flushes can use the full bandwidth available of the SSD and do not idle because of database design. ZNS SSDs do not come with full control over channels, but they do allow using multiple zones concurrently. This concurrency is limited by the number of active zones (\autoref{sec:znsworkings}). Each of the active zones can be used for different I/O operations. This allows various operations to happen at the same time and can be used similarly to LOCS. Therefore, it was decided to investigate if and how the extra active zones can be used to get the same effect. TropoDB by default only requires 5 active zones: 1 for metadata, 1 for WALs, 1 for L0 and 2 for LN. If more active zones are available than 5 on the SSD used, such an approach makes sense. Otherwise, it does not. Similarly, it might be that increasing the number of parallel structures has a detrimental effect on performance. We already know from decades of research that increasing the number of threads for a problem can also decrease performance or hamper latency stability. For example, io\_uring needs a lot of multithreading in some cases and if these are not available, it will lead to performance degradation~\cite{didona2022understanding}. Therefore, increasing concurrency should always be optional and configurable. 

TropoDB needs to reserve zones beforehand and can not make it fully optional. Instead, this is only optional for database creation. On database creation, the level of concurrency should be specified, which is then used by the inner structures to determine the layout. The approach TropoDB takes to match the number of active zones and increase parallelism is similar to LOCS. TropoDB allows increasing the number of memtables dynamically. Each memtable will then be linked to a unique WAL, a separate flush thread and its own circular L0 log. The level of concurrency can be safely set to anywhere between 1 and $floor(\frac{maximum\_number\_of\_activezones - 3}{2})$. The number ``3'' in this formula is based on the fact that in TropoDBs design there will always be two compaction threads in LN that can lead to one active zone each and that the metadata log can also have an active zone. The division by ``2'' is based on the fact that for each additional level of concurrency (from 1) 2 additional zones can be opened: one for WALs and one for L0. With this design, most active zones can theoretically be used by TropoDB. To support this feature, the database components need to be altered, we will explain the changes applied to each component in this section. The resulting design can be seen in \autoref{fig:tropodbexpanded}. Notice, that the number of circular L0 logs, WALs and memtables has increased.
 
\begin{figure}[h]
\centering
\begin{minipage}{0.85\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/TropoDB_expanded_design.svg}
\end{minipage}%
\caption{ TropoDB design: Dividing an SSD among LSM-tree components with added concurrency capabilities }
\label{fig:tropodbexpanded}
\end{figure}
 
 \subsection{Concurrent Memtables and WALs}
TropoDB has limited support for \textit{concurrent WALs}, which are multiple WALs that are live concurrently  (DR3). Each WAL is linked to its own unique WAL manager, memtable and immutable memtable (if any) in a 1:1:1:1 mapping. The linking is exclusive, which means that each component can only be linked to one of each kind. So for each concurrent memtable, there is one unique WAL manager and one unique live WAL. In addition, each WAL manager has its own distinct selection of zones. The individual WAL managers have no knowledge of each other and do not need knowledge about each other. They all have their own circulation scheme to spread the temperature of their own zones. On database creation, the zones that would ordinarily be given to one WAL manager, are divided evenly among the WAL managers in contiguous regions of zones. We thus now also have a log of WAL managers. 

On database startup, each memtable is linked to a WAL manager. This WAL manager thenn gives the memtable a WAL from its WAL log to use. When the memtable needs to be refreshed (flush operations), the memtable requests a new WAL from its WAL manager. All changes applied to this memtable are only applied to this WAL. The WAL is not aware that there are other memtables or WALs, therefore, nothing is inherently different in the WAL design. The same holds true for the design of individual WAL manager and memtables. In fact, the only changes needed lie in the implementation of the \textit{put} and \textit{get} operations themselves and some logic on top of the memtables. 

Put operations need to decide what memtable/wal combination to use and get operations will need to fetch the most recent change among all memtables. We will explain how both are done in TropoDB and we will explain some alternatives as well. To guarantee that the most recent change is picked, a simple solution is to look for the change in all live (immutable) memtables and pick the most recent change, which is possible as each change comes with a sequence number, that is still synchronised globally. This approach adds a little more read amplification, but only for the in-memory parts of the database. This is the approach TropoDB uses. Put operations are less trivial to design and can be done with various strategies. A challenge is that there can be fewer or more memtables/WALs than there are concurrent put threads. It is thus not as simple as linking a memtable/WAL to a separate put thread. Such a mapping can be done, but is not investigated in this research. During the put request itself, it is decided what memtable/WAL will be the target. Examples of such approaches are picking the memtable that contains a similar range of key-value pairs as the data present in the put operation, using round robin scheduling or using a hashing algorithm. In TropoDB a simple design is picked as the focus of the research lies on the storage aspects, the updates are applied in a round-robin fashion. Each memtable/WAL gets a write in order. This design is picked because it evenly distributes writes. Further on, in such a design a WAL is unlikely to get two writes in a row, decreasing the load one WAL has to endure (queue depth of appends). However, the approach taken by TropoDB does have a number of issues. For example, because round-robin is used, all memtables are likely to flush at similar times. Additionally, it is not a true concurrent memtable design (DR3). That is because all updates still arrive at the same put operation that contains some locking to determine what memtable to pick. Further research can look for a more optimal solution or an alternative strategy. There exist various concurrent WAL implementations that are known to scale~\cite{conway2020splinterdb, chen2021spandb}, such approaches might achieve more success. 
 
The main advantage of the concurrent WAL/memtable design is that multiple updates can be done concurrently to different zones as each WAL has its own zone head. This approach can, therefore, improve performance and better match the maximum performance that the SSD can give. However, the exact effect has only been measured for the entire concurrent implementation, which is shown in \autoref{sec:evalconc}.

\subsection{Concurrent L0 Logs and Concurrent Flush Operations}
A problem with the circular log design of L0 is that circular logs are limited to one write head only. This necessitates serialising multiple flushes and limits concurrency capabilities. In the previous section, we mentioned that in the concurrent design there can be multiple memtables and immutable memtables. Each of these immutable memtables can be flushed. These memtables are disjoint, but are still serialised to storage when one circular log is used. This misses an opportunity to do multiple storage operations in parallel. When multiple L0 logs are used, this effect is removed. As each concurrent flush can happen to a different log. In TropoDB one circular log is used for each additional concurrent immutable memtable. This ensures that each immutable memtable can always be written to one circular log and does not have to wait for other flushes from other memtables to finish. Using concurrent flushes in turn can help with reducing performance penalties when clients are waiting for flushes to complete. Concurrent L0 logs are supported by TropoDB and are possible because each circular log used by L0 gets its own distinct set of zones. This makes it possible to add multiple L0 circular logs, each with its own set of zones, at the cost of one extra active zone. Each flush, can then write to any of the L0 logs. Further on, as reads on L0 are issued to a physical block address and not to a distinct log, no change is needed for reads to support this feature as well. Resets do require some workarounds. Resets are hindered by the tail of each log and L0 logs are only allowed to do resets on their own selection of zones. Therefore, when concurrent logs are used some metadata is added to L0 SSTables, which indicates the \textit{log number}. On a reset, each L0 log should only get reset requests that correspond with its own log number. 
 
As stated before, there is an equal amount of live memtables/immutable memtables and circular L0 logs. Each memtable is, therefore, also linked to a unique circular L0 log and each L0 log gets its own flush thread. The updated model of background threads can be seen in \autoref{fig:bgthreadsexpanded}. Once one of the memtables is full and converted to its immutable memtable, it notifies its personal flush thread. This flush thread functions exactly the same as ordinary flush threads, but only appends to its assigned L0 log. Compaction threads do not require knowledge of the L0 logs for reads. They read from any L0 log and merge the data into SSTables for L1. However, they do require knowledge about the logs for deletion. On a deletion, they only send the deletes to the logs corresponding with the metadata of the SSTable to delete. This can be done because we store the log numbers along with the metadata of SSTables in L0.

This design works, but has one issue that makes such a design inoperable: ordering. Concurrent flushes can happen in this design and are in fact its purpose, but concurrent flushes also mean that the ordering of appends to L0 is no longer correct. On a read, only the most recent change needs to be read. In the case of memtables, this can be solved trivially. Always read all live memtables and pick the most recent change at the cost of read amplification. However, at L0 this is unfeasible as that requires reading many SSTables from storage. To circumvent this issue, an additional versioning number is added on top of the versioning numbers used for individual SSTables (see \autoref{sec:sstable}). This versioning number is also stored along with each L0 SSTable. Each update to L0 gets its own version numbering, which is incremented on each L0 update. All SSTables added by one flush, get the same L0 version number. This allows reading L0 in order of freshness and solves the earlier stated challenge. \\\\
\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/TropoDB_threads_expanded.svg}
\end{minipage}%
\caption{ TropoDB design: Background threads with added concurrency capabilities }
\label{fig:bgthreadsexpanded}
\end{figure}
\textbf{Addressing the design requirements}\\
The concurrency design addresses the design requirements (and RQ2) in the following way:
\begin{itemize}
    \item DR1: All datastructures still make use of appends.
    \item DR2: We will discuss overwrites only in terms of write amplification for now. As the number of overwrites is not inherently different for the underlying structure. Using multiple concurrent structures can lead to either more write amplification and resets or less write amplification and resets. It depends on the individual components and the split. The WALs and L0 are split, but are still internally the same structures. Each write still only happens to one of the respective components, so there is no data duplication. The amount of data written to storage should still be inherently the same. For example, a write happens to one of the WALs and there is still the same amount of WAL regions in total.  For L0 the situation is a bit different. The size of L0 circular logs can have an effect on write amplification. This is a side-effect of the concurrent design, but not a goal. The proposed design does not explicitly focus on reducing/increasing write amplification, but its effects are properly investigated to allow picking a level of concurrency in which the effects on the number of resets are positive.
    \item DR3: The concurrent design focuses on this functional requirement. It adds extra concurrent WALs and circular L0 logs. Both can be addresses concurrently, which can allow to use parallelism of the ZNS SSD more effectively.
    \item DR4: Using multiple concurrent structures can more properly utilise the storage device. This can lead to more concurrent I/O operations and prevent postponing expensive operations. If instead multiple smaller I/O operations are issued concurrently, the total time of the operations can be reduced. This can in turn result in clients needing to wait shorter periods of time if flushes or compactions are needed before puts or gets can proceed. However, using more I/O for the background can result in less I/O being available for the client. So it can also have the opposite effect by increasing the average latency of client operations or causing latency peaks during heavy background operations. Using multiple concurrent logs can also result in concurrent logs being filled sooner and require client operations to be stalled earlier. Using multiple objects concurrently can have a large number of effects. The effects are explicitly tested. This can aid in coming up with a design that fits the concurrency available, but does allow for stable latency.
\end{itemize}

\textbf{Disadvantages of the proposed design}\\
The proposed design has various disadvantages. It was mainly designed and implemented to test the effect of adding extra concurrency on top. It is thus more meant for research and less for actual use. If benefits were measured with this design, which there were not (see \autoref{sec:evalconc}), it would have led to a more refined design than this proof of concept. The result is that design is inherently less refined than for the individual components and misses various optimisations. A few of these need to be explained in more detail.

Concurrent WALs are not really concurrent (FR issues). Writes are striped, independent of the thread used. Each write is sent to a different memtable and WAL. However, the design still needs some synchronisation before each put to determine what memtable and WAL to pick. Additionally, there are striped to a \textit{group log} as group logging is used(see \autoref{sec:desgrouplog}). The result is that there is a bit of overhead on top of each call. A truly concurrent design should not require this, by for example assigning WALs to threads as done in SplinterDB~\cite{conway2020splinterdb}. Using concurrent WALs in the used design has minimal performance benefit for memtables and only makes sense if the WALs can not keep up with the updates it receives. If one WAL is busy, the update can then be seny to another WAL. This happens with synchronous I/O or if the asynchronous WAL queues fill up. A side-effect is that the workload send to the SSD increases significantly depending on the number of WALs; each WAL added, leaves less bandwidth for other structures.  
Striping is also problematic for other reasons. All WALs fill up at equal speed, which means that it is likely for all flushes to occur at the same time. This can lead to a burst of flushes and it would have been better to spread this workload by doing the flushes at different intervals. Further on, striping is irrespective of the data. This means that data can be in any of the memtables and no sorting is done. The result is that all WALs can contain very similar data and eventually all flushes will lead to overlapping SSTables. This could have been reduced by sending different data to different memtables/WALs. This would have led to less overlap and fewer compaction problems later on.

Multiple concurrent L0 logs lead to various issues. The original design was not designed to support concurrent L0 tables. There is still only one L0 to L1 compaction thread. The circular logs can only clean up their data if SSTables are removed on their tail. This was done by forcing compaction to at least pick one SSTable on the tail, but this is not a good idea to do with concurrent logs. The tables might not overlap, which can lead to non-related SSTables being used for more expensive merges in L1 (more overlap in L1). TropoDB tries to solve this by rotating what circular log is guaranteed to have its SSTable used for the compaction, but this still means that not all logs will make progress. Similarly, it is possible for the merge to only pick SSTables from one of the logs. The result is that the number of live and dead SSTables can remain the same for a few logs and that if a flush waits for this log to become clean, it has to wait. If in turn a memtable is filled, the client will have to wait for the flush to finish and a compaction to clean the circular log to finish. This can lead to unpredictable latency. This problem can be reduced by picking a different compaction strategy or using multiple L0 to LN compaction threads. 

Since there are multiple flush threads and only one L0 to L1 compaction thread, we also have a congestion issue. There are more threads that are able to quickly fill up L0 with flushes, but fewer threads to move data out of L0. This can lead to better performance in the short run, but once L0 becomes full, it will remain congested. All flush threads will regularly have to wait for data/garbage to move out of L0 and the effects of concurrency will be minimised. We will end up in a state where the same amount of work is done as in the non-concurrent implementation, but more threads are idling and consuming CPU resources (DR4). This should (perhaps contradictory) be solvable by using more threads from L0 to L1, to alleviate the congestion point. However, then the problem will move to the compactions in LN. Essentially, the problem will move further down the tree.

The last challenge lies in the get implementation. Using multiple concurrent structures will result in more structures that need to be read. It requires a read from multiple memtables and multiple L0 logs (L0 logs can contain different data of different ages), which will lead to read amplification. This is only justifiable to use in cases where increasing write throughput and reducing write latency is more prudent than reducing read amplification and when it is known that the concurrent design can in fact increase write performance.

\subsection{Summary}
TropoDB supports multiple concurrency levels. By default TropoDB uses 5 active zones at most, which does not use all resources available on some ZNS SSDs (DR3). Extra concurrency is added by increasing the number of concurrent memtables, concurrent WALs, circular L0 logs and flushs threads. Each extra level of concurrency adds one of each, which are al linked together in a chain.  So, each memtable uses a different WAL, flush thread and L0 log. All put operations are striped across the concurrent memtables/WALs. Once a memtable or WAL is filled it is flushed with its personal flush thread to its L0 log. The L0 compaction thread can read data from any L0 log and is not aware that there is more than one. To guarantee correctness and ordering, more metadata is added to each SSTable. On a read, all live memtables are read and the most recent change is returned to the client. If the data was not available in any memtable, the most recent L0 SSTable is read, which is determined with the help of a sequence number in the SSTable. If no L0 SSTable contains the data, the read proceeds to LN as usual.

\section{Summary}
TropoDB uses a novel design that is optimised for ZNS Storage. Its design answers RQ2 and tries to use unique optimisation opportunities given by ZNS. Such ZNS opportunities include append operations, limiting number of overwrites and resets, implicit concurrency capabilities and control over the garbage collection process, which can aid in latency issues. TropoDB splits the ZNS storage in distinct zone regions for each component. Each individual component gets a data structure, number of zones and a garbage collection scheme, optimised for its use-case. Further on, all data structures only make use of appends, no writes are used. Metadata of the index structure is stored in a circular log in the first few zones of the SSD. WAL data is stored in multiple once logs, uses a novel circulation scheme to spread heat and makes effective use of appends to increase concurrency capabilities. SSTables in L0 are stored to a circular log and SSTables in higher levels are stored to fragmented logs. All background operations make use of eager garbage collection procedures. TropoDB uses three background threads by default. One background thread is in charge of flushes, one background thread is in charge of compactions from L0 to LN and one background thread is in charge of compactions in higher levels. To support higher concurrency, TropoDB supports adding multiple concurrent memtables, WALs, flush thread and L0 logs.

\chapter{Background and Related Work on KV-Stores on Modern Storage \mbox{Devices}}
\label{sec:background}
In order to explain the design of our key-value store, we first have to explain what a key-value store is and what kind of technologies the key-value store will be interacting with. We need to understand their individual characteristics. We will discuss flash storage, the ZNS interface, key-value stores and LSM-trees, as these are essential to explain how the design of TropoDB came to be.

\section{How do Flash-based SSDs work?}
\textit{Solid State Drives} (SSD) are storage devices that do not make use of moving parts. They internally consist of an integrated circuit and a storage medium. While storage media can differ between SSDs, in this research, we are only interested in the storage medium used most commonly within SSDs, \textit{flash storage}. Whenever we refer to an SSD in this paper, we are talking about flash-based SSDs, unless explicitly stated. In this section, we will highlight the properties of SSDs that are most interesting for this research.

\subsection{Why Flash-based SSDs are used}
Flash storage is not a new technology, as it was invented in 1980 by Toshiba~\cite{klein2016history}. Nevertheless, flash only saw increased usage in the past two decades. In these two decades flash storage became cheaper and saw an increased adoption rate by data centers amongst others~\cite{lee2008case, andersen2010rethinking}. Flash storage is considered promising because of a number of reasons, which led to its large rate of adoption. Most of these reasons relate to SSDs being (in most cases) superior to the most common alternative, \textit{Hard Disk Drives} (HDD). SSDs can achieve lower latencies and higher throughput than conventional HDDs and come with high concurrency capabilities~\cite{athanassoulis2010flash, rizvi2010flash, ajwani2008characterizing}. Further on, unlike HDDs, SSD performance is affected less by random I/O.  This makes them more suited for I/O heavy use-cases such as persistent databases. Yet at the same time, they are not exponentially more expensive than HDDs and are widely available~\cite{grupp2012bleak}. This can make moving to SSDs an affordable transition to increase application performance. Another advantage of SSDs is that they are considered more green \textit{to use}, because SSDs consume less electrical power than HDDs, especially when idle~\cite{andersen2010rethinking, dayarathna2015data}. Nevertheless, the exact difference in power usage depends on the SSD model and how it is used~\cite{dayarathna2015data}. Further on, the cost of manufacturing SSDs is not necessarily greener than the cost of manufacturing HDDs~\cite{tannu2022dirty}. One reason for this cost difference is that SSDs are not just flash storage and each part of the SSD can be made by a different manufacturer. 


\subsection{What is Flash Storage?}
Flash storage is a non-volatile storage medium where data is stored as an electronic charge on a floating-gate between a control-gate and the channel of a CMOS transistor, in what is also known as a \textit{memory cell}. This cell can then be programmed, read, and erased, but only a finite number of times. A cell can be written to a finite number of times because of a process known as \textit{wear levelling}. Wear levelling means that after each write or erasure, the cell slowly degrades. This results in decreased performance and eventually leads to a dead cell. Further on, cell degradation is not just limited to writes and erasures, reads can also degrade the performance of cells, albeit to a lesser degree. This is known as \textit{read disturbance}~\cite{liu2015read}. It is, therefore, considered harmful to do excessive IO on flash storage~\cite{min2012sfs}. 

The amount of bits a cell holds depends on the cell technology used. The cell technology can be defined by several \textit{levels}, with higher levels holding more bits. For example, \textit{single-level cells} (SLC) can hold just 1 bit and \textit{multi-level cells} hold 2. At the moment, there exist in order of density: SLC, MLC, TLC, QLC and PLC~\cite{ma2020low}. Higher levels allow us to store more data in each cell, which allows for denser storage. Yet at the same time, with such designs, it is only possible to write or read at the granularity of the cell technology. That is because multiple bits occupy the same cell. This can decrease throughput and increase wear levelling~\cite{alsalibi2018survey}. 

Cells are then packed together in a hierarchical configuration. Hierarchical configurations are for example \textit{NAND flash} and \textit{NOR flash}. In the case of SSDs, NAND flash is used. So we will only discuss NAND. NAND packs cells together densely, which is good for mass storage, but only allows addressing I/O at the level of \textit{pages}~\cite{cornwell2012anatomy}. For example, a common page size is 4KB. This means that it is not possible to write to just 1 cell, but you always have to write to a multitude instead. Further on, it is not possible to overwrite a page. Once a page is written, it is written. Instead, a page must first be \textit{erased}, before it can be overwritten. However, such erasures can only be done at the next hierarchical level, the level of \textit{blocks}; for example, a block of 2MB. So, every time data needs to be erased, it might be necessary to move around multiple pages to a new block before the old block can be deleted without data loss. This makes it very beneficial to group data based on their lifetime, as this can significantly reduce the need to move around cold pages on erasures. This need to write more data is also known as \textit{write amplification} (WA), writing more physical data than was logically requested. For example, a logical write of 1 page, might be rewritten physically multiple times because of erasures.  To continue the hierarchical layout: blocks are then packed together in \textit{planes}, which are packed together in \textit{dies}, which are packed together in a \textit{package}. Packages can be processed in parallel, increasing performance. Lastly, flash storage also comes with multiple \textit{channels} that can be shared by multiple packages. Those channels allow managing the I/O operations. 

Another important detail of flash is that the cost of operations is asymmetric. The latency of a read operation is lower than the latency of a write operation and in turn the latency of a write operation is lower than the latency of an erasure operation. Further on, flash storage does not make use of moving parts. This allows the device to get the performance of random I/O close to the performance of sequential I/O. For example, Xu et al. benchmarked a flash-based SSD and measured a difference in average device latency between 85$\mu$s for sequential reads and 90$\mu$s for random reads~\cite{xu2015performance}. Do note, that these results are mainly chosen to get an idea of how close random I/O performance is to sequential I/O performance. Results are highly dependent on the device and workload used. In this research, we will also benchmarks random and sequential I/O ourselves (see \autoref{sec:read}). 

\subsection{What is a Flash-based SSD?}
A flash-based SSD is an SSD that contains flash storage. However, an SSD does not just contain a persistent storage medium such as flash. It typically also comes with a small processor complex, which allows managing the storage, and some RAM to buffer I/O operations. How such a design looks is visible in \autoref{fig:flashinternals}. Therefore, to make SSDs truly persistent, it is necessary to force the SSD to flush its buffers. Further on, SSDs are also not just hardware. They typically also come with firmware installed, which defines how to interface with the storage and how to deal with storage errors.

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/flashinternals.svg}
\end{minipage}%
\caption{ Flash-based SSD: Organisation }
\label{fig:flashinternals}
\end{figure}

An SSD on itself does not do much, so it is generally attached to a more functional machine. There exist multiple methods for attaching SSDs. For example by making use of \textit{SAS}, \textit{SATA}, \textit{FC} or \textit{PCIe}~\cite{wong2013ssd}. Each of these has different performance characteristics, requiring rethinking how to optimally use the attached storage. In addition, they can be connected with different interface protocols such as \textit{AHCI} and \textit{NVMe}~\cite{landsman2013ahci}. NVMe directly interfaces with the storage and allows achieving the maximum parallelisation that an SSD can achieve. AHCI on the other hand, does not. NVMe with PCIe is currently considered the state of the art and is also the attachment method is used in this research.

\subsection{Flash Translation Layer}
SSDs are typically interfaced with an interface known as the \textit{Flash translation layer} (FTL)~\cite{kwon2011ftl, chung2009survey}. This can either be implemented directly as firmware on the device itself or implemented on the host~\cite{jhin2018optimizing}. The FTL allows interacting with the SSD as if it is a conventional block device. In other words, it can be interfaced like other block devices such as HDDs. The device can then be used as a flat array of virtual addresses. This can also be seen in \autoref{fig:l2p}. When using an FTL, only writes and reads have to be issued by the client and there is no need to think about how to perform overwrites or erasures anymore. For example, the client can write to any LBA presented in \autoref{fig:l2p}. There is no need to force sequential writes on the host. Instead, this is left entirely up to the FTL. The FTL is in charge of translating virtual addresses to physical addresses and managing erasures in the background with a \textit{logical to physical} table (L2P). This leads to situations as can be seen in \autoref{fig:l2p}. The first LBA points to the second \textit{physical block address} (PBA) and once pointed to the first PBA. When a logical address updates, the old physical address becomes invalid and the logical address is assigned a new physical address. It is up to the FTL to clean up the old addresses on the background. 

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/l2p.svg}
\end{minipage}%
\caption{ FTL: L2P table }
\label{fig:l2p}
\end{figure}

Background operation are performed by a \textit{garbage collector} (GC) present in the FTL. The GC is in charge of erasing old data and moving data to more \textit{convenient} locations. Typically this means that when a host application issues an overwrite, the virtual address is mapped to a different physical address and the old physical address is marked dirty and at some point in time erased by the GC. This erasure can be done immediately or postponed and is left up to the erasure policy of the GC.

Using an FTL makes it trivial to add support for SSDs to an application that uses a standard block interface. It also makes it easier for software engineers that are not familiar with flash specifics to use such devices. Yet, a flash translation layer is not a free lunch. It comes with some significant disadvantages. Later on, we will discuss a different interface, ZNS, that will mitigate most of these disadvantages. The first disadvantage is that there is no control over the physical layout of data on flash~\cite{bjorling2021zns}. A virtual address can point to any erasure block, no matter what else is stored in this erasure block. This prevents hot and cold separation techniques, which is problematic as hot data can be stored together with cold data. In turn, this means that cold data might be moved/erased more often than it needs to be, leading to expensive GC. The second problem is that there is no control over this GC. The GC can be scheduled at sub-optimal times, for example, when a lot of writes are issued by the client. This can cause I/O to stall and the latency of client-issued I/O operations on the SSD to become unpredictable. Thirdly, an FTL requires \textit{overprovisioning} (OP). Overprovisioning reserves a part of the flash storage on the SSD for garbage collection. This can for example be used as a sink for data when the garbage collector wants to reset a zone that still contains live data. This data must be stored somewhere. However, overprovisioning wastes storage by reducing the amount of usable space. Lastly, there is no restriction on FTL implementations, meaning that designs can vary between both vendors and SSD models. For example, there exist FTLs with page-level mapping, block-level mapping and even hybrid-level mapping (hybrid-log FTLs)~\cite{chen2009understanding}. Therefore, it becomes a Herculean effort to optimise for SSDs as a different approach needs to be taken for different SSDs.  

\subsection{How do NVMe ZNS SSDs work?}
\label{sec:znsworkings}
\textit{NVMe Zoned Namespace devices} (ZNS) for SSDs is a specification that defines an SSD in which the flash storage within an SSD is exposed to the user as an array of \textit{zones}~\cite{NVMeSpec,bjorling2021zns}. It is a standard declared in the NVMe specification and is the successor of \textit{Open-channel SSDs}~\cite{bjorling2019open}. It is an alternative to standard FTLs and comes with no garbage collector by itself. In ZNS each zone closely matches with the erasure unit (block) of the flash storage within the SSD. Using ZNS mainly revolves around interacting with these individual zones. This design is perpendicular to ordinary FTLs in which the SSD is exposed as one large flat array of addresses. Each individual zone has a state and a set of attributes that are maintained within the SSD, but can be requested by the host as seen fit. For a full explanation of ZNS, we recommend reading the NVMe specification~\cite{NVMeSpec} or the explanation at zonedstorage~\cite{zonedstorage}, we will only look at a subset of the logic. A major advantage of ZNS is that both garbage collection processes and access patterns can be altered to fit specific applications. This allows creating an optimal mapping for each individual use case. For example, optimising for low-latency applications or optimising for applications to reduce wear levelling effects.\\
\textbf{How Does a Zone Work?}:\\
To explain zones and their states we will follow the example given in \autoref{fig:zones}. Each zone is append-only and has exactly one write head, known as the \textit{write pointer} (wp). The beginning and ending of each zone are fixed. Additionally, the wp of each zone initially starts at the zone's beginning. This zone is then in a state known as the \textit{empty} state. In order to write to this zone, the zone must first be opened. This can be done \textit{explicitly} with an \textit{open} operation or \textit{implicitly} by just writing to the zone (step 1 in the figure). It is only possible to write to exactly the wp of a zone and nowhere else. This can be done with either a \textit{write} or an \textit{append} operation. The difference between a write and an append operation is that for a write you need to know the wp as you write explicitly to the wp (need to specify its location). Whereas for an append, you only need to specify the beginning of the zone and do not need to maintain the wp itself on the host. The physical location of the data will be returned to the host on completion instead; essentially inverting the responsibilities. Additionally, ZNS can accept multiple append operations simultaneously to the same zone as appends are \textit{nameless}~\cite{maheshwari2021blocks}. This can increase the performance, as no additional waits are necessary between ``writes", allowing the SSD to determine the optimal order for appends. After each completed write, the wp increases. This is visible in step 1. Three pages are written in this step and the wp has increased three positions as well. The wp can continue up till the end of the zone is reached, at this point the state is transitioned to \textit{full}. When the zone needs to be reused, it can be reset with the \textit{reset} operation. For this it does not matter if the zone is empty, partially filled or full, the zone will be erased regardless. On a reset, the wp will be reset to the beginning of the zone and the zone will transition back to the empty state. An example of a reset is visible in step 3, the wp moves back to the beginning and all pages are erased. Reads can be performed on the zone regardless of the location of the wp. This means that is possible to read at any address of both a full zone, a partially filled or an empty zone.
\begin{figure}[h]
\centering
\begin{minipage}{0.95\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/zones_simplified.svg}
\end{minipage}%
\caption{ ZNS design: Example of ZNS state transitions }
\label{fig:zones}
\end{figure}\\
\textbf{Where Does a Zone End?}:\\
We mentioned that writes can only continue until the end of the zone is reached. This requires some extra explanation. Each zone has a \textit{zone size} (zse) and a \textit{zone cap} (zcap). The zse and zcap can also be seen in \autoref{fig:zones}.  Zone sizes are the same for all zones and are equal to the total size of a zone. Zone capacities are less than or equal to the zone size. However, the zone capacity is the only part of the zone that can be used by the host. The part between the zcap and zse can not be used by either writes or reads by the host and is meant for the device only. That means the zone is full when the zcap is reached and reads after the zcap are considered illegal by the device.\\
\textbf{Size Constraints}:\\
Yet another property that must be dealt with has to do with I/O limits. Reads and writes must always be aligned on the block level. That means that it is not possible to write just one byte or read just one byte. Instead, I/O must always happen in units of \textit{pages}, typically 512 bytes or 4kB. Writes, appends and reads are lastly limited by a maximum size. The maximum amount of bytes that can be written or read in one operation is limited by the \textit{maximum data transfer size} (MDTS) and the maximum amount of bytes that can be appended in one operation is limited by the \textit{zone append size limit} (ZASL). \\
\textbf{Constrains of Using Multiple Zones}:\\
Finally, there is one last idiosyncrasy of ZNS that we must elaborate on. It is possible to write to different zones at the same time. This can increase parallelism and thus performance. It also make it possible to separate hot and cold data (hot and cold can be written to different zones). Yet, the ZNS specification specifies that it is possible to set a limit on the maximum number of \textit{open} and \textit{active} zones. This means that there is a limit on the number of zones that can be written to simultaneously. Open zones are all zones where the wp of the zone is neither at the beginning nor at the ending of a zone. Once the host considers that a zone no longer needs writes for a while, the zone can be closed with the \textit{close} operation. The wp will not be reset in this case, but the zone transitions to the \textit{active} state. It can then be reopened when it needs to be used again. In other words, each open zone is always active, but not each active zone is open. However, as active zones also have a limit, this might not be enough. Therefore, if the host knows that the zone no longer needs any writes at all, the zone can also be \textit{finished}. The finished command immediately sets the zone to the \textit{full} state.
In short, developers that make use of ZNS need to be aware of the number of zones that are open or active concurrently and act accordingly. 

To give a short summary, ZNS is a specification that allows developers to interact with SSD storage on the level of zones. Developers have to consider the state of zones, have to know where zones begin and start, should adhere to size constraints of I/O and should be careful that they limit their usage of multiple concurrent zones.

\section{What is a Key-value Store?}
A key-value store is a type of database that is part of the NoSQL movement~\cite{doekemeijer2022key}. The idea of key-value stores is not new as they were already used in the 20th century~\cite{sharma2012sql}. The requirements to be called a key-value store are low and not set in stone. They are identified as a flat collection of key-value pairs. Data is stored as keys pointing to values. For example, a filename pointing to file contents or a human name pointing to a social media profile. In this research, we will stick to a minimal definition. In this definition a number of operations are defined. These are also shown in \autoref{tab:kvops}. data can be stored with a \textit{put} operation and data can be retrieved with a \textit{get} operation. A put operation requires setting a mapping from a key to a value. A get retrieves the value associated with a key, as long as there is one. Some key-value stores also come with a \textit{delete} operation, but this can also be implemented by setting an \textit{empty} value with a put. Further on, many key-value stores come with a \textit{scan} operation, allowing iteration on a set of keys, but this is again not required.

\begin{table}[h!]
    \centering
    \begin{tabular}{||l l l||}
        \hline
        Operation & Explanation & Required \\
        \hline \hline
            put & Stores a key-value pair & Yes \\
            \hline
            get & Retrieves a key-value pair & Yes \\
            \hline
            delete & Deletes a key-value pair & No \\
            \hline
            scan & Retrieves a range of key-value pairs & No\\
         \hline
    \end{tabular}
    \caption{Key-value store: Available operations}
    \label{tab:kvops}
\end{table}

The persistency level of key-value stores can differ. Some are stored entirely in memory and will be erased on shutdown (completely in-memory key-value stores), others are also stored on storage. In this research, we are only interested in persistent key-value stores, key-value stores that maintain data on storage and will remain persistent, even on server shutdown. The reliability model can differ between persistent key-value stores. They do not have to be \textit{ACID} (see the ACID specification for more information~\cite{rusinkiewicz1995specification}). In fact, using \textit{BASE} or another less strict model is not uncommon. Therefore, picking a key-value store as a storage backend also requires verifying if the picked key-value store has the reliability guarantees needed for the end product.

Persistent key-value stores have become commonplace and are used by a large number of industries~\cite{dong2021evolution, debnath2011skimpystash, risch2015introduction, cao2020characterizing}. Key-value stores are considered flexible, which allows optimising them for various use-cases. This can range from web shops~\cite{risch2015introduction}, to graph data bases~\cite{cao2020characterizing} to game servers~\cite{debnath2011skimpystash}, to backends for relational data bases~\cite{matsunobu2020myrocks}. Not all storage needs the functionalities that traditional relational databases provide or their persistency guarantees. For such storage needs, it might be better to use a lighter database and add extra requirements on top of the database only as they are needed. For example, ZippyDB uses key-value stores in a distributed setting and adds reliability itself on top~\cite{cao2020characterizing}. In this case, it is possible to use a key-value store for each node and to move reliability problems to the distributed setting itself. This is not (efficiently) possible if the database already includes all of this logic. This allows for a clear separation of problems. Many modern key-value stores are optimised for low-latency, large throughput, and terabytes of data and many of them are optimised for SSDs~\cite{doekemeijer2022key}. These types of databases are the target of this research. As they are optimised for SSDs, they are a target for ZNS optimisation, and as they have to deal with large I/O and low latency, they have characteristics interesting to verify the effects that co-optimising GC can give.

Key-value stores can internally be implemented by almost any data structure; any structure that holds, manages and stores and loads key-value data from storage suffices. Some common implementations make use of \textit{hash tables}, \textit{B-trees} and \textit{LSM-trees}~\cite{doekemeijer2022key}. LSM-trees are the target for this research as they are favorable for flash storage, which we will explain in more detail in the next section.

\subsection{How does an LSM-tree work?}
\label{sec:lsmtree}
An LSM-tree is a composite data structure, a data structure that itself consists of various smaller connected data structures.
In the case of an LSM-tree all structures are connected together in a large sequential funnel. Each data structure in this funnel serves as a waiting hub for the next structure, which is always bigger than the previous structure. All changes always begin in the smallest data structure and progress step by step to the largest structure. This design allows amortising expensive I/O operations and creating individual data structures optimised for their underlying storage and size. For example, structures optimised for doing many small writes in memory and structures optimised for holding large, but infrequently updated quantities of data on storage~\cite{o1996log}. In order to properly describe this funnel, we will describe how written data moves through the tree step by step. How this data movement happens can also be seen in \autoref{fig:lsm}, which showcases the design of the LSM-tree design in LevelDB~\cite{ghemawat2011leveldb,LevelDB} and is based on an image by Lu et al. for the Wisckey paper~\cite{lu2017wisckey}. Then we will describe, again step by step, how data can be read from the tree.

\begin{figure}[h]
\centering
\begin{minipage}{0.75\textwidth}
  \centering
  \includesvg[width=1\linewidth]{graphs/leveldb.svg}
\end{minipage}%
\caption{ LSM-tree design of LevelDB }
\label{fig:lsm}
\end{figure}

\subsubsection{LSM-tree Write Operations}
\label{sec:background_lsm_writes}
\textbf{(1,2)} All \textit{put} operations first arrive at the most basic components of the tree, the \textit{memtable} and optionally a \textit{WAL}. A memtable is a completely in-memory structure that is used to store and get data. It can be characterised as a small in-memory key-value store. When a client \textit{puts} data, it is always first applied to the memtable. The memtable is optimised for random-access memory and can thus overwrite, reorder and sort data without major performance problems. There are no constraints on the implementation of memtables, but a common implementation is a \textit{skip list}~\cite{yeon2020jellyfish}. However, using just a memtable does not guarantee any persistence of data. For example, think about what would happen during a power loss or even a clean and correct database shutdown. In this case, all data written to the in-memory structure will be lost.

Therefore, most LSM-trees come with an extra option that allows making the database persistent. When this option is active each \textit{put} is apart from being logged to the memtable \textbf{(2)}, also logged to another data structure, known as a \textit{write-ahead log} (WAL)\textbf{(1)}. Unlike the memory table, this structure resides entirely in storage. The function of the WAL is to back the memory table. WALs are implemented as append-only logs and are similar in functionality to \textit{redo logs} or \textit{journals} used in traditional relational databases. WALs only record changes applied to the memtable and are thus essentially logs of deltas. Whenever a key-value store restarts, all changes can be read from the WAL and reapplied to a memtable. This recreates the state of the memtable to what it was before database shutdown. The combination of memtable and WAL is thus persistent. 

\textbf{(3,4)} Once the memtable or the WAL becomes too large, the data of memtable should be moved to the next part of the funnel, which will now be completely written to storage. The memtable and WAL are in this case flagged as dirty and the flagged memtable will become \textit{immutable} \textbf{(3)}, or in other words \textit{read-only}. This is necessary as the immutable memtable will be written to a bigger data structure and should, therefore, no longer be altered. Writing the immutable memtable to storage is known as \textit{flushing} and is done by a separate background thread \textbf{(4)}. During this flush the immutable memtable is converted to a \textit{Sorted String Table} (SSTable), which is a collection of key-value pairs sorted on keys. This SSTable is then appended to the top-level of the tree, known as \textit{L0}. Important to remember is that while all SSTables are stored on storage, a bit of the metadata such as the range of each SSTable are retained within memory. This reduces the cost of later read operations as it can already be seen beforehand if a key is present within the range of an SSTable. If it is not, no read will be necessary. Lastly, in order to not stall any updates issued by the client during the flush, a common optimisation is to create a new memtable and WAL so that the client can continue to issue puts. The client will work on these new structures, while the flush can safely work on the old structures.

\textbf{(5)} Eventually, L0 is considered to be too big as well. In this case, a few L0 SSTables move to the next level of the tree, which is known as \textit{L1}, with a process known as \textit{compaction}. A big difference between L0 and L1 is that L0 can have SSTables with overlapping key-ranges, but L1 does not; instead, all SSTables must contain a unique key range. Therefore, the chosen SSTables of L0 are merge sorted with all overlapping SSTables of L1. The tables that result from the merge are appended to L1 and the old SSTables (both the chosen L0 SSTables and the overlapping L1 SSTables) can be removed.

An LSM-tree can have more than two levels on storage. The number of levels is typically configurable. All levels after L1 are similar to L1; they all contain non-overlapping SSTables and they can all be moved to the next level with the compaction procedure. The only difference is that each level is bigger than the previous. The last level where the data might end up is called \textit{LN}.

\subsubsection{LSM-tree Read Operations}
On a \textit{get} operation, we might have to look into multiple data structures as the data can reside in any of the layers of the tree. We should always ensure that only the most recent data is read, as the client does not want to read old data. This can easily be guaranteed if the writes work correctly and they indeed proceed in a funnel. That is because each next part in the funnel contains older data, so on a read we should move through the funnel from beginning to the end. It is important that a snapshot is used, to prevent a client from missing a data movement between levels. It needs to read L0 up to LN as they are at the moment of the get. Whenever an entry is found that matches the key, the value is immediately returned and we no longer have to look further in the funnel as we know it is the most recent entry. The following structures might need to be read in order: memtable, immutable memtable, L0, L1, L..., LN. Note that each of these structures might itself need multiple reads. Both of the memtables reside entirely in memory and only require 1 read each. L0 might require reading all of its SSTables from storage as the ranges can overlap. Lastly, L1 up to LN only requires 1 read from storage as the tables can no longer overlap. 

\subsubsection{LSM-tree Garbage Collection}
Write operations on the LSM-tree are not only issued by clients. Clients only put data into the memtable and the WAL. Separate background processes, usually implemented with threads, are in charge of operations 2,3 and 4 of the write process. These operations are also referred to as \textit{garbage collection} operations. They move the data to new locations and remove the old data. For example, on a compaction, multiple tables are merged into a new table. The old tables need to be removed afterwards. This can lead to write amplification, as data will be rewritten for each layer and can also be rewritten to the same layer on a merge (data is copied). Similarly, it causes quite a few deletions of old data.

\subsubsection{Why a Funnel is used in the LSM-tree}
 The general idea of composite data structures is that each structure has properties that better fit the size, usage and storage medium of the data that it contains. For example, a large part of the data is generally only read, making it beneficial to store that part of the data in a structure that works well for reads only. The LSM-tree is no different in this regard. Only a small part remains in memory as memory is both limited in size and not persistent. This part is optimised for this use case with the use of memtables. Each higher level of the LSM-tree is optimised to handle more data than the previous level and each higher level is generally accessed less frequently. Thus each higher level in the tree has more to gain with efficient storage than with efficient write or update throughput/latency. Another benefit of a funnel design is that it allows delaying and amortising expensive operations. For example, if L0 did not allow overlaps, all flushes would need to merged in L0. This would significantly increase the cost of flushes and it might be better to delay such operations and do them in one go later on; say merging 10 L0 SSTables into L1 instead of merging 10 times into L0 and then one time into L1. This can also reduce write amplification as the number of overwrites is reduced. The same idea holds for levels above L1. Increasing the number of levels can reduce the size of most merges, decreasing write amplification. At the same time, increasing levels, increases the amount of data that might need to be read as more levels need to be read. This is known as \textit{read amplification} (RA), reading more physical data than was logically requested. LSM-trees thus come with a trade-off between WA and RA. 

\subsubsection{Why are LSM-trees Favorable for (ZNS) SSDs?}
An LSM-tree is considered favorable for flash for a number of reasons~\cite{purandareappend, stavrinos2021don}. Part of the explanation will be similar to the reasons why open-channel SSDs are favorable for LSM-trees as well~\cite{zhang2017flashkv}, as the two technologies are very similar. Specifically, we will consider an LSM-tree directly on the storage. Otherwise, it depends on the layers between the LSM-tree and storage whether an LSM-tree is efficient. Firstly, LSM-trees mainly make use of appends. Changes to any structure, add new data, and delete old data. This is true for all major I/O components: updates to WALs, updates to metadata and adding new SSTables are all appends. Using appends fits naturally on SSDs as SSDs do not allow random I/O in the form of overwrites. Instead, adding new data is more convenient. In other words, data is never updated in place, which is the type of access pattern that works best on flash storage. Further on, LSM-trees already come with a clear separation of hot and cold data. Each step in the \textit{funnel} is colder than the previous one. This can help the garbage collector to properly separate data based on temperature. Further on, LSM-trees are optimised to reduce write amplification, not read amplification (in most cases)~\cite{lu2017wisckey, liu2021ptierdb, mei2018sifrdb}. Many LSM-tree designs trade read performance for better write performance. This fits SSDs because of the asymmetric cost of I/O on SSDs, as writes are more expensive than reads in both latency and wear levelling costs. Normally there is an FTL between the SSD and the key-value store. An FTL might translate in-place updates to appends instead; removing benefits that append-only designs might have had before. Similarly, it can not be guaranteed that hot and cold data are stored separately. Lastly, the results can differ between FTL implementations. This makes it hard and not always justifiable to state that LSM-trees are more favorable than other structures. ZNS SSDs give full control over the SSD, which allows guaranteeing that the LSM-tree is used as it is designed.

\subsection{RocksDB and LevelDB}
\label{sec:rocks}
RocksDB is a key-value store that is considered the state-of-the-art~\cite{RocksDB, doekemeijer2022key}. It is used by a large number of users and is a hot topic for research (as of this writing in August 2022). Many key-value store ideas originated from RocksDB or have been applied to RocksDB. For example, applying Wisckey~\cite{lu2017wisckey} on RocksDB with BlobDB~\cite{dong2021evolution}.  RocksDB is built on top of an earlier key-value store, LevelDB~\cite{LevelDB, ghemawat2011leveldb}. LevelDB is a key-value store that originally sparked a large interest in key-value stores and saw a surge in their usage~\cite{doekemeijer2022key}. However, LevelDB left many optimisations open and has, for example, many issues with large I/O and compactions. Further on, it is not explicitly built for Solid State Drives, but Hard Disk Drives. On the other hand, RocksDB is explicitly optimised for flash-based Solid State Drives~\cite{dong2021evolution}. This in combination with its popularity, makes it an interesting target for both optimisations and comparisons. Internally both LevelDB and RocksDB make use of an LSM-tree. The figure, \autoref{fig:lsm}, is also based on the LevelDB design. RocksDB comes with advancements such as multiple compaction threads to make better use of the internal parallelism of flash. TropoDB in turn continues on the work of both RocksDB and LevelDB. Lastly, RocksDB also comes with its own benchmarking tool, db\_bench. A benchmark that is based on the workloads in which RocksDB is used.  

\section{Garbage Collection and the Semantic Gap}
\label{sec:backgroundsemantic}
We have already introduced the notion of a \textit{garbage collection} (GC) process for both LSM-trees and flash storage.  LSM-trees need a GC process to manage flushes and compactions. Flash storage needs a GC process to erase blocks and move live pages (if any) to other blocks when blocks are reset. Both are not entirely disjoint, which can lead to duplicate work and coordination issues. Where possible, the two should coordinate and decide on the block(s) data will be written to and how and when data should be removed.

In the current situation, the GC of an LSM-tree can by default not delete or write data by itself. Instead, it relies on other software to do it, such as an FTL or a file system. In other words, whenever the LSM-trees orders an SSTable to be deleted on a compaction, the deletion itself is handled by other software. Eventually, it will arrive at the GC of the SSD, which then has to decide what to do with the deleted pages. The LSM-tree can not order the data to be physically deleted and can not determine when the data is deleted. This can happen at any time, which means it can also happen at peak activity of the key-value store. Ideally, it should happen at a time that is convenient. For example, when the key-value store is idle. This can only be done if all software layers between the store and storage can communicate about how and when deletion should be done. However, this is generally not possible as it requires all layers between the store and the SSD to support the needed communication features. This coordination issue is not limited to erasures, the same thing happens for writes. For example, when an FTL is used, the LSM-tree has no control over the physical location(s) of its data. It can be stored anywhere. The LSM-tree is designed in such a way that there is a clear hot and cold separation of data: WALs are hot, L0 is colder and LN is the coldest. Using such a hot and cold separation would be very beneficial for garbage collection on the device, as it can prevent storing hot and cold data together in the same blocks. Unfortunately, this again requires all layers in between the database and the SSD to be able to communicate this separation and adhere to this separation (some layers might intentionally ignore the conveyed message). The result is that WAL data and LN data can be stored together in the same erasure blocks. Such problems are known as \textit{semantic gap} issues~\cite{zhang2017flashkv, shen2018didacache, bjorling2021zns, purandareappend}. Each additional layer between the store and the SSD leads to another semantic gap. 

The semantic gap can lead to a number of issues:
\begin{itemize}
    \item No control over the physical locations of data. Hot data can be stored together with cold data, even when an application knows they should be stored separately, which leads to more expensive GC operations in both latency and wear levelling.
    \item No physical control over the scheduling of background operations. Background operations of each layer do not have to be issued at predefined times and each layer can have its own background scheduler logic. This can lead to latency instability as schedulers from different layers can schedule expensive operations at the same time (amplifying effect) or idle at the same time (reducing effect).
    \item No control over data deletions. Additionally, each layer can have a separate definition of what a deletion actually entails; think about deleting entire structures, files or erasing blocks. LSM-trees can delete a structure, but the layers below might only physically erase a few of the data blocks of the structure on storage. The other ``deleted'' blocks of the data can be deleted at a completely different time or not at all.
\end{itemize}

There are multiple solutions to solve the semantic gap. One is to add all semantics that are needed for proper communication between each layer. However, this might be infeasible. For example, it would require an FTL to be aware of all of its possible applications and support them all. An alternative is to simply remove the layers and collapse their features on top of each other. In this paper, we will take a look at this approach and study its effects. ZNS is already an example of such an approach. It leaves the GC up to the host, which allows optimising the GC for one specific application, such as an LSM-tree-based key-value store. Therefore, it can bridge the semantic gap. 

However, there is one problem left. The FTL is not the only layer that can exist between key-value stores and storage. Many key-value stores make use of a \textit{file system} as well~\cite{cao2020characterizing, LevelDB}. Instead of storing the components of an LSM-tree directly on the SSD, they are built on top of files. For example, a WAL can be stored within a file, which is then itself stored on multiple erasure blocks/ZNS zones. This provides a higher abstraction on top of SSDs as key-value stores no longer have to deal with FTL/ZNS specifics. At the same time, it does require the file system to have enough functionalities to communicate the LSM-trees needs to the storage interface. Therefore, hot structures might not be differentiable from cold structures by the file system and be defined as generic files. The result is that they can still be put together in the same zones. Only once all layers between storage and key-value store are truly removed, can the GC of the database and storage be linked together. The main disadvantage is that the key-value store implementation will become storage implementation specific and will require more functionalities than before. It goes against the UNIX philosophy in this regard~\cite{gancarz2003linux} and creates a more complicated design. In the end, it is a trade-off. In this thesis the effect on the GC will be examined when all layers are removed.  

\section{Storage in User-space}
There exist multiple libraries and APIs that can be used to communicate with ZNS storage from an operating system~\cite{yang2017spdk, spdk, tehrany2022understanding, zonedstorage}. As of 2022, the ZNS technology is still relatively new for SSDs, which means that not every storage engine supports all ZNS functionalities for SSDs yet. However, this might change soon. In the end, many ideas and optimisations proposed in this research should be applicable to most other storage engines/operating systems, as long as they adhere to the ZNS specifications as stated in the NVMe ZNS specification~\cite{NVMeSpec}. That is because we focus on ZNS optimisations in this work only. Yet, there can be significant performance and scheduling differences between implementations, which is something to look out for. For example, in GNU/Linux it is possible to set a scheduler for NVMe SSDs. Some schedulers serialise I/O and can, therefore, not make full use of the parallel I/O capabilities of ZNS SSDs such as appends~\cite{tehrany2022understanding}. Further on, some APIs might depend on such a scheduler to be present. Using a serialising scheduler such as \textit{deadline} in GNU/Linux does not allow sending multiple outgoing append commands to the same zone and can, therefore, not reach the full performance benefits of appends. Therefore, in order to get full performance benefits, it is recommended to pick a storage engine solution that allows getting full concurrency capabilities and gives full ZNS control.

In this research, we want to get maximum control over the SSD, scheduling included. As one of the goals is to get latency stability, expensive kernel operations should preferably be minimised as well. Therefore, for TropoDB it was decided to run the SSD within user-space, to get maximum control~\cite{yang2017spdk}. Running it entirely in user-space allows controlling the entire I/O path from beginning to end (minus a few minor steps) and allows having more influence on latency stability. This is possible with the \textit{Storage Performance Development} (SPDK). The SPDK framework unbinds the SSD from the kernel and allows using the device as it is in user-space instead. This removes the kernel from the I/O path, which can reduce more latency issues related to the kernel and removes one additional layer. It also adds more control over the I/O scheduler and its latency. Internally, SPDK only has a basic scheduler that makes use of lightweight threads and revolves around polling from applications themselves. This works well with the latency characteristics of NVMe SSDs and can allow applications to decide for themselves when to poll. It also prevents excessive context switches and their unpredictability. Lastly, SPDK is also considered to be the state of the art and at the moment one of the fastest I/O solutions out there, both for synchronous and asynchronous I/O~\cite{didona2022understanding, kourtis2019reaping}. Lastly, as it is not bound to standard kernel development practices, it is more up-to-date and already has full ZNS support for SSDs. Using SPDK should be a co-optimisation for the key-value store design, but not required to reproduce most of the results presented. It can reduce latency and increase throughput, but it is mainly used to help with stabilising latency and GC effects.

\section{Related Work}
\label{sec:novelty}
Key-value stores for flash storage are a widely researched topic as can be seen in ``Key-Value Stores on Flash Storage Devices: A Survey''~\cite{doekemeijer2022key}; a survey that was conducted as preliminary work for this thesis. Additionally, ZNS is a novel storage technology that attracts the attention of various research communities. It has been stated that ZNS storage is beneficial for key-value stores and that optimising key-value stores for ZNS can be beneficial. There exists concurrent work that looks in the potential of ZNS for key-value stores, but TropoDB is the first key-value store built entirely on ZNS. In this section, we will describe adjacent works.

\subsection{Key-value Stores for ZNS SSDs}
There already exists a full LSM-tree implementation for ZNS devices~\cite{choi2020new}, but this LSM-tree is optimised for garbage collection of the storage itself and not for key-value stores. Purandare et al. come with some advice and design considerations on how to implement LSM-trees more efficiently on ZNS SSDs~\cite{purandareappend}. It also comes with some general research directions for LSM-trees on ZNS SSDs. The proposed ideas can aid with the design of our solution for key-value stores and our implemented solution can in turn also aid the research by Purandare et al. by addressing some of the proposed research directions. For example, they mention the layout of LSM-trees, how to manage different data lifetimes, compaction procedures, indexing, filter and metadata, and how to alter internal data structures of LSM-trees when used on ZNS devices. However, none of these ideas are implemented yet. There also already exists a key-value store that can run on ZNS, but this key-value store itself has no knowledge of ZNS, nor is optimised for it. That is because this implementation runs the key-value store RocksDB~\cite{RocksDB} on a ZNS-friendly file system, known as ZenFS~\cite{bjorling2021zns}. This is possible because RocksDB can use file systems without knowing how they are implemented. Therefore, other ZNS-friendly file systems such as ZoneFS can also be used~\cite{purandareappend,le2020zonefs}.  Stavrinos et al. discuss various design considerations for applications that make use of ZNS devices and some general research directions~\cite{stavrinos2021don}. They also mention that key-value stores could benefit from ZNS devices. There does exist some concurrent work that is looking at how parts of the LSM-tree can be improved for ZNS storage. Jung et al. introduce \textit{lifetime-leveling compaction} (LL-compaction), a compaction tailored for ZNS storage that can reduce space amplification effects and GC effects by storing SSTables with similar lifetimes together~\cite{jung2022lifetime}. TropoDB also stores SSTables with similar lifetimes together, but takes a different approach to do so. Lee et al. propose a different zone allocation algorithm for ZenFS~\cite{lee2022compaction}, \textit{Compaction-Aware Zone Allocation algorithm} (CAZA). This work is more adjacent to ZenFS than it is to TropoDB, but comes with interesting strategies to deal with separating SSTables based on lifetime. It stores SSTables that are likely to be compacted in the near future together instead of storing SSTables based on their level, which is the strategy used by ZenFS and TropoDB to separate on lifetime. It also comes with a number of other strategies for allocations. These strategies and ideas are orthogonal to TropoDB and could have also been used in its design. TropoDB separates SSTables on lifetime similar to ZenFS, the CAZA algorithm could, therefore, have also been used in the design of TropoDB.

None of the aforementioned ideas fully addresses our issue and as far as we know there does not exist any LSM-tree key-value store that is specifically implemented for ZNS NVMe devices yet (as of August 2022). We will be the first to create one. All existing solutions make use of file systems in between the store and the storage or look at only a part of the LSM-tree. 

\subsection{Key-value Stores for Open-channel SSDs}
\label{sec:ocssd}
ZNS can be seen as a successor to \textit{Open-channel} SSDs (OCSSD)~\cite{bjorling2021zns}. Similar to ZNS, OCSSD gives full control over the SSD to the host~\cite{bjorling2017lightnvm}. However, the model it uses to give this control is different. Nevertheless, key-value stores have been created for OCSSDs and many of their design decisions can apply to ZNS as well. A few examples of key-value stores that come with designs that are still partially applicable to ZNS are LOCS~\cite{wang2014efficient}, FlashKV~\cite{zhang2017flashkv} and NoFTL~\cite{vinccon2018noftl}. Those designs come with various solutions on how to map key-value stores to open-channel storage. For example, using as many memtables as there are channels to increase parallelism and match the full bandwidth that flash storage can leverage. Other techniques include scheduling operations based on their cost. For example, LOCS proposes assigning costs to reads, writes and erasures and scheduling these operations based on their cost. Perpendicular to this scheduling technique is scheduling as done by FlashKV, where operations can be prioritised based on the workload. For example, prioritising reads over writes if there is enough space left, but prioritising compactions if forcing compactions becomess critical. Scheduling these types of operations is a bit more problematic for ZNS, as you can not directly do the scheduling for the channels themselves. Nevertheless, it is still possible to schedule operations appropriately and the knowledge gained from this key-value store design is useful. For example, it is still known that scheduling multiple I/O operations at the same time, will lead to interference. Further on, ZNS gives control over all I/O operations issued, resets included. Therefore, it should be possible to schedule resets at appropriate times. For example, the design proposed by FlashKV could still be attempted by giving the compaction procedure more zones to use if compactions become critical or to allow resets only when they are critical. 


\subsection{Key-value Store LSM-tree Garbage Collection}
A goal of TropoDB is to improve the garbage collection process of an LSM-tree-based key-value store. Garbage collection of LSM-trees is a known problem. Especially compaction procedures are problematic. TropoDB is definitely not the first to address this issue and many earlier concepts can be reused to aid in the design of key-value stores for ZNS SSDs as well. Various ideas have been proposed to reduce the effect of garbage collection procedures that are not necessarily limited to approaches that relate to the storage medium. We will look at a few interesting solutions. Only the first solution, will be tied to the storage medium and is most interesting for TropoDB.

The first solution that we like to discuss is improving the scheduling of compaction procedures. This idea does not aim to remove compactions, but to do compactions at the right time. It should be scheduled in such a manner that read and write operations receive as little hinder as possible from this background operation. For example, if compactions are performed too late, flushes have to wait before L0 is properly compacted to L1. On the other hand, if they are performed
too early, compactions might not fully use the buffering capabilities available and compact duplicate data (hot entries that keep reoccurring).
One such approach is proposed by Balmau et al. and is known as SILK~\cite{balmau2019silk}. SILK properly links the
compaction operations to the internal flash parallelism. It always prioritises compactions on L0 because these compactions are most relevant for client operations and it allows preempting compactions. This means that for example a compaction on L2 can be stopped, to perform a compaction on L0 instead. Compactions also only get a part of all available flash bandwidth, allowing clients to always make some progress, even when multiple compactions happen at the same time. This idea is shown to stabilise the latency of the key-value store and shows no latency peaks like would happen without such a scheduling procedure. The scheduling procedures described in \autoref{sec:ocssd} are orthogonal and such scheduling designs can also be used for ZNS optimised designs.

A common approach to reduce garbage collection cost is to move away from the standard levelled LSM-tree design, towards a \textit{tiered} design, also known as a \textit{LSM-forest} design~\cite{liu2021ptierdb, mei2018sifrdb, raju2017pebblesdb}. With a forest design, each level of the LSM-tree can contain overlapping SSTables or multiple logical LSM-trees. With such designs, it is not necessary to do full compactions and partial compactions are allowed.  Partial compactions can reduce the effects of congestion in the LSM-tree, reducing stalls for the client. This might, therefore, also have been interesting to use for TropoDB as it can reduce the garbage collection effects. In the end, such solutions can reduce the write amplification effects of compactions, at the cost of increasing read amplification. Forest designs also suffer from other problems. For example, such designs can lead to more expensive garbage collection procedures later on. Additionally there exist partitioned forest designs~\cite{mei2018sifrdb}, which come with yet again other trade-offs. The main point to make here is that LSM-trees do not have to be limited to levelled designs. The design can be altered if that makes sense for the use-case. There is a lot of research in this area. Such designs are not necessarily storage specific, but can aid in reducing the effects of garbage collection processes as a co-optimisation. This approach can use a garbage collection optimised tree design with a garbage collection optimised storage design. This has not been tried with TropoDB, but it might be worthwhile to also support LSM-forests on ZNS. Similar to how TropoDB has been optimised for a levelled LSM-tree, so can a different database be optimised for a LSM-forest or a different tree design altogether. There might be a tree design that fits ZNS more naturally (a zone tree?) that works better for reducing the effects of garbage collection.

Another common approach to deal with compactions is to delay compactions altogether. This is again not storage specific. A problem with delaying compactions is that while it might improve performance (latency, throughput, write amplification) in the short run, it is likely to get worse results in the long run. It can lead to significant spikes in long-tail latency~\cite{balmau2019silk, chai2019ldc}, which is the opposite of the goal of TropoDB. However, if done correctly delaying compactions can still be beneficial. TRIAD delays compactions based on heuristics~\cite{balmau2017triad}. TRIAD only compacts from L0 to L1 if the overlap of tables in L0 is big enough, which can reduce write amplification effects of LSM-trees. However, it does generally lead to higher read amplification and can lead to a bigger more expensive compaction in the end. A larger more expensive compaction can cause tail latency to increase. Additionally, it comes with a novel strategy to reuse part of the WAL on a flush. Instead of creating an SSTable from the immutable memtable, it writes references to data in the WAL and keeps the old WAL around. Such a design reduces write amplification, but leads to read amplifications and fragmented data, as the WAL holds both live data and dead data. Such an approach will also make it non-trivial to do garbage collection on ZNS SSDs as it will eventually require compacting the WALs.

\subsection{Key-value Stores in User-space}
TropoDB runs its key-value store in user-space as a co-optimisation. It uses SPDK to achieve this. Removing the kernel from the I/O path can reduce interference from the kernel. TropoDB is not the first to do this and it is valuable to look at how other key-value stores implemented such an approach. Kourtis et al. proposed uDepot~\cite{kourtis2019reaping}. This key-value store allows using SPDK to run in user-space as well. They note that it is able to achieve higher performance in both throughput and latency to run the key-value store in user-space. They also note that it is not always possible to have full control over the device, which makes user-space key-value stores unusable. They come with a custom framework that allows switching between I/O backends, some of which do not require running in user-space. This is not a target of TropoDB, but is in general advisable to do. Such an approach could also be used for TropoDB and is orthogonal.

\subsection{ZNS SSD Research}
Research on ZNS storage is not limited to key-value stores. There is also research on other applications for ZNS storage and research into the ZNS interface itself. Ideas proposed for other applications might be transferable to key-value stores and the other way around. Research into ZNS has implications for key-value store designs. If it turns out that a certain strategy for ZNS achieves low latency for reads, that should be used for key-value stores as well. Additionally, new changes to the ZNS interface can lead to new options becoming available for key-value stores as well. In this section, we will look at such ideas.  
There exist a few works that investigate the raw performance of ZNS. These works can be used together with our ZNS SPDK performance benchmarks to get a more accurate picture of ZNS performance. For example, Tehrany et al. come with some initial performance investigations for ZNS and file systems for ZNS~\cite{tehrany2022understanding}. They benchmark with the libaio/psync storage engines and investigate the performance of various I/O schedulers as well. Whereas, for benchmarking ZNS we solely focus on SPDK benchmarks with SPDKs default I/O scheduler. We also do not benchmark raw file system performance, but solely benchmark file systems when used in conjunction with RocksDB. The benchmarks are, therefore, complementary. Nick et al. also state that larger I/O sizes are required to saturate the device bandwidth~\cite{tehrany2022understanding}, which is important to know for key-value store design. Shin et al. also come to this conclusion~\cite{shin2020exploring}. Additionally, Shin et al. come with other ZNS performance characteristics as well~\cite{shin2020exploring}. For example, that performance of I/O operations differs based on the LBA, something that is not accounted for in the key-value store or measured for that matter. LSM-trees can address this characteristic by using LBAs that are able to achieve higher performance for more urgent matters. Bj{\o}rling et al. also come with various benchmarks for ZNS, RocksDB and file systems~\cite{bjorling2021zns}. They benchmarked with db\_bench and measured among others throughput, average read latency, write amplification and workload runtime (time till completion of a large workload). Further on, they used the same workloads as were used in this research. Their research confirms the results shown in our work.

Key-value stores are not the sole target of ZNS research. For example, Bergman et al. investigate a swap subsystem optimised for ZNS, known as ZNSwap~\cite{bergman2022znswap}. This work comes with a garbage collector optimised for the use-case, that is able to achieve both better throughput and tail latency. Lessons learned from this research can be carried over to key-value store research. For example, ZNSwap allows for swapping between the custom data placement and zone reclamation
policies (garbage collection) and comes with an API to add new policies. A similar approach can be taken for LSM-tree designs. For example, allowing the garbage collection policies to be altered dependent on the use-case of the key-value store.

There are also works that investigate modifications to the ZNS interface. For example, Han et al. propose ZNS+, an expanded ZNS interface~\cite{han2021zns}. ZNS+ adds various modifications on top of ZNS, that make it more optimised for \textit{log-structured file systems} (LFS). LFS and LSM-tree key-value stores have a lot in common, which means that ZNS+ also has advantages for key-value stores. In ZNS+ it is possible to offload data copy operations to the device and it is possible to overwrite data with sparse sequential writes. They state that this allows for an alternative reclamation strategy, known as \textit{threaded logging-based block reclamation}. In the end, both optimisations allow for less communication overhead between host and device and both can also be used in LSM-trees. Therefore, it is valuable to also investigate building an LSM-tree on top of ZNS+. Maheshwari comes with an extension on top of ZNS that allows for variable-size pieces of data, known as \textit{rocks}~\cite{maheshwari2021blocks}. Rocks can both be implemented on the host-level and the device-level. Rocks support an I/O granularity smaller than 512 bytes (a block size less than 512 bytes is not supported by NVMe) such as 16 bytes, which can aid LSM-tree design in a number of ways. For example, allowing small key-value pairs or small WAL updates to be written to ZNS with minimal padding. At the moment, designs like TropoDB are limited by size constraints, which hinder small I/O. Rocks prevent such issues. Lastly, Purandare et al. propose \textit{group appends}, which also allow appends smaller or bigger than the page size~\cite{purandareappend}. This leads to similar possible optimisations for LSM-trees as Rocks.

\subsection{Summary}
In this chapter we have looked at various related works. We have looked at key-value store designs for ZNS SSDs. While there are various partial implementations of LSM-trees on ZNS and designs for LSM-trees on ZNS, no full LSM-tree has been implemented on ZNS yet. TropoDB is the first one. We have also looked at key-value stores for Open-channel SSDs, and discussed various design decisions that are also applicable for ZNS. 

We continued by looking at general work in garbage collection for LSM-trees. There, we described techniques to reduce garbage collection effects. We described techniques to schedule garbage collection operations, postpone garbage collections and reduce the cost of garbage collection. Such techniques are orthogonal and are also applicable to TropoDB. 

We also shortly investigated key-value stores for user-space. The works presented here are orthogonal and can be applied to the design of TropoDB as well.

Lastly, we took a look at general research for ZNS SSDs. We have seen research in performance benchmarks, which showcase performance characteristics for ZNS, that have not yet been investigated for key-value stores. We have also seen designs for applications different than key-value stores. We finished by looking at alterations in the ZNS interface. If any of these alterations become part of the ZNS specification, they will have profound effects on LSM-tree key-value store design.

\section{Summary}
In short, flash-based SSDs are an affordable, fast and available storage medium, that are widely used. However, traditional SSDs come with an interface known as an FTL. An FTL is typically installed on the device and comes with a garbage collection process. The garbage collection process causes the QoS of the device to suffer and leads to latency and lifetime issues of the device. Additionally, it requires overprovisioning of storage. ZNS is an extension to the NVMe specification that addresses the issues of the garbage collection process and overprovisioning, by transferring control to the host software. This allows applications to decide for themselves, how to do garbage collection. 

Key-value stores are a common type of database, that is also used in low-latency use-cases and use-cases that require latency stability. A common implementation makes use of LSM-trees. Internally, it also conducts garbage collection to move data around. A design that is said to fit SSD characteristics, but was not able to achieve optimal performance because of earlier-stated issues with the FTL and garbage collector. ZNS gives new opportunities and allows optimising a garbage collector specifically for both an LSM-tree and ZNS storage, effectively co-optimising both garbage collection processes.

Co-optimising garbage collection processes can be taken a step further by reducing additional layers in between the key-value store and storage. This can be done by reducing the kernel from the I/O path as well and moving the key-value store into user-space I/O. This effectively gives the key-value store full control over the storage.

We have also looked at various related works that have aided in the design in implementation TropoDB. There exist initial designs for LSM-trees on ZNS, but no implementations yet. These designs lay a foundation for TropoDBs design. While there are no LSM-tree key-value stores on ZNS, there are a few file systems for ZNS and general design guidelines for building applications on ZNS. Further on, there do exist key-value store implementations for Open-channel SSDs, the successor of ZNS storage. Additionally, there exist general research for ZNS to improve the interface and benchmark performance characteristics of ZNS. Lastly, there are various related works that have looked at reducing garbage collection effects of LSM-trees and building key-value stores in user-space.